<!DOCTYPE html>
<html>
  <head>
    <title>Cluster Evaluation</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Evaluating Clustering

Based on slides by Andreas Müller
???
Last time we talked about clustering, and an obvious question is:
Which algorithm do we use, and how do we set the parameters?
That's a bit more of an art than a science, unfortunately.
But I want to give you some tools to do better than just guessing.

---
class: center, middle

# Evaluation Measures
???
The fist thing I want to talk about is evaluation measures.
How good is a particular clustering? 
There's basically two types:
Supervised, where you have a ground-truth clustering,
and unsupervised, where you don't.

Usually supervised methods are much more helpful. Then again, if you
have labels, why do clustering? Just use classification and you will
generally get much better results! Typically supervised is used to
compare various clustering algorithms, esp. when coming up with new
clustering algorithms. In general comparing clustering results.

Let's start with the supervised methods.
Accuracy doesn't make sense. Q: Why not?
A: Cluster labels don't matter.

---
class: spacious
# Supervised evaluation

- Compare clustering against "Ground Truth".

- Unclear what the assumptions are.

- Impossible to do in practice.

???
- Usually classification datasets.
- Often used for papers on clustering algorithms.
- Compare partitions, not labels!

Here you assume that you are provided some "true" cluster labels that
you want to compare against.  What people often do is just use a
classification dataset, and basically treat it as an unsupervised
clustering problem.  This is particularly done by researchers in
clustering that want to show that their algorithm performs better than
some other algorithm.  Doing this, you are measuring how well a
completely unsupervised algorithm can identify the classes in the
dataset as clusters.  The problem here is that it's very unclear what
kind of assumptions about the clusters you are making with this
labeling of the data.  There might be many valid ways to cluster a
dataset, and it's unclear why the class labels should somehow
correspond to the "best" clustering, or in what sense this clustering
is "best".  It's very unlikely that there's a coherent definition of
cluster so that the classes correspond to clusters for any particular
dataset - unless the classes are really well separated.

The final downside is that this is actually impossible in practice,
and therefore basically completely useless. Why would you use a
clustering algorithm if you have ground truth? You'd always be better
using a classification algorithm. Even if you have very little labeled
data, using this in some way is probably still much better than trying
to run a completely unsupervised clustering algorithm.  So basically
if you're running a supervised evaluation method, why are you doing
clustering at all?  This makes sense to do if you're studying
clustering algorithms, but not really in any practical application.  I
still want to talk you through how these methods work.

One thing to keep in mind for all the cluster evaluation metrics is that
they compare partitions, not labellings. While clustering outcomes in 
scikit-learn (and other libraries) are always given as integers, you
don't want to compare the actual integers, as you do in supervised learning.
You want to compare the partitions given by the cluster labels.

---
class:spacious
# Thinking in Partitions

- Partition: set expressed as a union of disjoint sets.

- Y=[0, 0, 0,  1, 1, 1] → C={{0, 1, 2}, {3, 4, 5}}

- Y=[0, 1, 0, 1, 2, 2] → C={{0, 2}, {1, 3}, {4, 5}}

- Partition given by [1, 0, 1, 0] and [0, 1, 0, 1] are the same!

???
Here's a small example.
Partitioning the data means expressing it as a union of disjoint sets.
So a partitioning is a sett of sets.
In this example, the labelling 0,0,0,1,1,1 means that the first three
data points are in the same cluster, and the remaining datapoints are in
a different cluster. And I wrote that down as a partition here,
using the sample indices.
Here's another example of 0,1,0,1,2,2, which means the zeroth and 2nd example
are in one set, and the first and third sample are in a different set, and the
fourth and fifth are in a different one again.

So the labels really have no semantics, and the partitions given by 0,1,0,1
and 1,0,1,0 are the same, so if you compare these two clusterings,
they should be regarded as exactly the same clustering.

---
# Contingency Matrix
For clusterings `$X_i$` and `$Y_j$`, `$n_{ij}=|X_i \cap Y_j|$`

`$$
\begin{array}{c|cccc|c}
    {{} \atop X}\!\diagdown\!^Y &
    Y_1&
    Y_2&
    \ldots&
    Y_s&
    \text{Sums}
    \\
    \hline
    X_1&
    n_{11}&
    n_{12}&
    \ldots&
    n_{1s}&
    a_1
    \\
    X_2&
    n_{21}&
    n_{22}&
    \ldots&
    n_{2s}&
    a_2
    \\
    \vdots&
    \vdots&
    \vdots&
    \ddots&
    \vdots&
    \vdots
    \\
    X_r&
    n_{r1}&
    n_{r2}&
    \ldots&
    n_{rs}&
    a_r
    \\
    \hline
    \text{Sums}&
    b_1&
    b_2&
    \ldots&
    b_s&
    \end{array}$$`
    


???
Similar to confusion matrix, but for clustering.

The most basic way to compare two different clustering results, say
yours and the ground truth, or two different algorithms, is the
contingency matrix.  You might know contingency matrices from
statistics, where they are used to relate discrete probability
distributions.

The Contingency matrix is quite similar to a confusion matrix, in that
it counts how often a datapoint was given label i in one clustering
and label j in the other clustering. So rows correspond to one
clustering, and columns correspond to the
other.  

However, because labels don't mean anything, the diagonal has no
meaning, and both of these contingency matrices here mean that the two
clusterings produce the same partition, i.e. they are identical. (next
slide)

Also, contingency matrices do not need to be square. If you run clustering
on a dataset, you don't usually know the number of clusters so you want
to compare different clusterings with different numbers of clusters.
 
Now that we have this basic representation of cluster overlap, we can
define derived metrics 

---
# Contingency Matrix
- Both of these mean identical partitions:

.center[
![:scale 20%](images/contingency_matrix_1.png) ![:scale 20%](images/contingency_matrix_2.png)
]
???
- They just assigned different labels
- Like confusion matrices, a detailed summary of what's happening
- but often we want a single number description

---
# Mutual Information

`$$\Large MI(U,V)=\sum\limits_{i=1}^{|U|}\sum\limits_{j=1}^{|V|}P(i,j)\log\frac{P(i,j)}{P(i)P'(j)}$$`

`$$\Large =\sum\limits_{i=1}^{|U|}\sum\limits_{j=1}^{|V|}\frac{|U_i \cap V_j|}{N}
\log\frac{N|U_i \cap V_j|}{|U_i||V_j|}$$`


???

A very common metric is mutual information, which is the mutual
information of the discrete distributions given by the partitions (or
the contingency table). i.e., how much two distributions depend on
each other (or how correlated are they). Comes from information
theory. High = very similar.

Mutual information is a standard information theoretic measure for relating
two distributions, providing a measure of how interdependent they are.
There we just create distributions corresponding to the partitions and compare those.

We can rewrite this uing the contingency matrix, the probability P(i, j) is
just the the entry n_ij divided by the number of samples, and P(i) and P(j) are the
row and column sums divided by the number of samples.
- P(i) = how likely is it that a random point ends up in partition i 
- P(i,j) = prob. that random point ends up in U_i and V_j 

Mutual information is also always between zero and one, with one being
perfect information. 

Prob: Depends on # clusters and size of clusters. If ground truth has
particular cluster sizes, may not be possible for different clustering
to get MI of 0 or 1. Want to normalize. If someone just randomly
partitions, you want this to have an MI of 0 with the ground truth.

However, if one partition is a sub-partitioning of the other
partition, the mutual information is still one. So if you over/under
partition data, this doesn't penalize that.

- Between 0 and 1 but not normalized or adjusted. If V is subpartition
  of U still = 1

---
# NMI and AMI

`$$ NMI(U,V) = \frac{MI(U,V)}{\sqrt{H(U)H(V)}}$$`

`$$ AMI = \frac{MI - E[MI]}{\max(H(U),H(V))-E[MI]} $$`

???

- Similar to the rand index, the mutual information depends on the number
of samples and size of the clusters, and it's common to adjust it for chance,
to give it a more universal scale.

- Two common normalizations are the normalized mutual information NMI, which divides the mutual information
by the squareroot of the products of the entropy of the marginal distributions,
- and the adjusted mutual information AMI, which has an adjustment for chance.
- (Entropy is a measure of uncertainty, can think of chaos)
- Both use the entropy to penalize overpartitioning, but only AMI is
adjusted for chance, so zero on expectation.
- These two usually behave pretty similarly. There is other variants
of NMI, for example dividing by the maximum, as I did here for AMI.
- Summary:
- Penalizes overpartitioning (via entropy)
- NMI always between 0 and 1
- AMI slightly easier to interpret, max of 1, but can be smaller than 0
- Adjusts for chance – any two random partitions have expected AMI of 0
- Expectation is over all possible labels of the data
- in general 0 means no info, 1 means perfect information
---

# Rand Index

- Computed of partitions U, V over the same elements (say {0, …, n-1})

`$$ R(U, V) = \frac{r+s}{\binom{n}{2}}  $$`

- $r$ :  Number of pairs of points in the same set in U and in V

- $s$ : Number of pairs of points in the different sets in U and in V

- $\binom{n}{2}$ : Number of possible pairs

- Between 0 and 1

???
- Another One of the most useful one is the Rand Index. But slightly hard to understand.

- For two partitions U and V, it computes the number of pairs of
points that are in the same set in U and in the same set in V, called
"r" here, 
- and the number of pairs of points that are in different sets in both
C1 and C2, called "s" here.
- In other words, this sum is which pairs of points the partitions
  "agreed on" -- either in same cluster or not.
- This is divided by n choose 2, so the number of all possible pairs
  of points.
- This is always between zero and one, though the actual possible
minimum and maximum depend on the number of clusters and the number of
points.
- A little more complicated, but seems intuitive to me.

- Alright, so the rand index is about matching pairs of points, which is slightly
tricky to do in your head, but not that complicated really.

---
# Rand Index Example

`$$R([0,0,1,1],[1,1,0,0]) = R\left(\{\{0,1\},\{2,3\}\},\{\{0,1\},\{2,3\}\} \right)$$`
`$$ = \frac{2+4}{6} = 1 $$`

`$$R([0,1,0,1],[0,0,1,2]) = R(\{\{0,2\},\{1,3\}\},\{\{0,1\},\{2\},\{3\}\})$$`
`$$ = \frac{0+3}{6} = \frac{1}{2} $$`

???

- Let's do an example. We're comparing the labeling 0011 and the labelling 1100.
- They are the same, so hopefully the rand index is going to be 1.
- First, I write these as partitions here. Then I check which pairs of
points are in the same set in both partitions. The pair 0,1 is in the
same set in both partitions, so we count that. The pair 2,3 is also in
the same set in both partitions, so we count that as well. No other
pairs are in the same set in either partition, so we're done counting
for "r".
- Now we count the pairs of points that are in different sets. Zero
and 2 are in different sets in partition one, and also in different
sets in partition 2.  Same for zero and 3, and 1 and 2, and 1
and 3. So that's four pairs in total.  So the nummerator is 2 + 4
= 6. The denominator is 4 choose 2, which is also six, so the rand
index is 1, as expected.

- For a second example, let's look at these two partitions, 0,1,0,1
and 1,2,0,0.  Here there's no pair of points that is in the same set
in both of them.  There are three pairs of points that are in
different sets in both of them, (1,2), (2,3) and (0,3). So the rand
index is 3/6, so 1/2.

---
# Adjusted Rand Index (ARI)

`$$ AdjustedIndex = \frac{\text{Index}-\text{ExpectedIndex}}{\text{MaxIndex}-\text{ExpectedIndex}}$$`

`$$ARI = \frac{\sum_{ij}\binom{n_{ij}}{2} - \frac{\sum_{i}\binom{a_{i}}{2}\sum_{j}\binom{b_{j}}{2}}{\binom{n}{2}}}
{\frac{1}{2}\left(\sum_{i}\binom{a_{i}}{2} + \sum_{j}\binom{b_{j}}{2}\right) -\frac{\sum_{i}\binom{a_{i}}{2} + \sum_{j}\binom{b_{j}}{2}}{\binom{n}{2}}}$$`


???

- Because the range of the rand index depends on the data, in practice
people prefer to use the adjusted rand index, which is adjusted for
chance.
- So we take the index we just computed, subtract the expected index
over this dataset, and divide it by the maximum index on this dataset
minus the expected index.  
- This means the expectation over all cluster assignments is zero, and
the maximum over cluster assignments is 1.
- However, it means the adjusted rand index can become negative,
basically if the partitions are less similar than they would be on
expectation, or by chance.  
- What this works out to is this nice formula here. The n_ij is the
entries of the continency table we saw before, and the a_i and b_i are
the row and columns sums of that
contingency table.  
- Just keep in mind that it measures exactly the same thing as the
rand index, only adjusting for chance.  
- You can see all of the
normalization terms only depend on the row and column sums, so the
number of samples in each of the clusters. Only this first part here
depends on the contingency table.  
- The most common use of this is to compare a particular clustering to
a ground-truth clustering.

---
# KMeans on digits dataset
.center[![:scale 70%](images/ari_nmi_ami_digits.png)]

???
- How do these behave in practice
- Common application: tuning (figuring out best # clusters)
- ARI, AMI and NMI for k-Means on “digits” dataset. 
- ARI and AMI penalize too many clusters.
- (MI not very helpful in this case)
- Interesting that it's not best at 10...
- Why do you think that? (just hard to cluster, no guarantee 10 is
  best. Maybe need to have two clusters of 7, e.g.)
- Note that NMI does not penalize for higher number of clusters as
  much...

---

class:center,middle

# Unsupervised Evaluation

???
- Often a little more vague since we don't have labels
- BUT we can actually use these in practice
---

# Silhouette Score

- For each sample:
`$$ s = \frac{b-a}{\max(a,b)} $$`

- a is the mean distance to samples in same cluster

- b is the mean distance to samples in nearest cluster

- For whole clustering: average over all samples

- Prefers compact clusters (like k-means) 
???
- Nearest, but not same, cluster
- in other words, prefers kmeans over dbscan (as one consequence)
---

.center[
![:scale 60%](images/ari_silhoutte.png)]
???
- ARI is highest for best clustering, but Silhouette does not see this
  b/c clusters are not compact

---
.center[
![:scale 80%](images/number_clusters.png)]

- What's the right number of clusters?
???
- Q: ? A: Probably 4, though this isn't so clear. (drawn from 4 actually)
- Can look at individual silhouette scores
---

# Silhouette Plots

.center[
![:scale 70%](images/silhouette_plots.png)
]
- Plot sorted silhouette score for each sample in each cluster

- http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html

???
- Silhouette found "right" 3 clusters
- Plot silhouette score for each point in each cluster
- Each cluster "should" have many points with similar silhouette
  score, gradual drop off
- Sharp drop off = bad, either two clusters in one or overpartition
- Requires interpretation, but can be good for exploring
---
.center[
![:scale 60%](images/number_clusters_gaussians.png)
]



???
- Q: What’s the right number of clusters?
- A: Generated as mixture of 10 Gaussians, but not clear at all. So what! No real right answer!

---
.center[
![:scale 60%](images/more_silhouette_plots.png)
]

???
- Look at silhouette scores for this.
- What looks good?
---
# Digits Dataset

.center[
![:scale 80%](images/silhouette_digits.png)
]

???
- Found 15 clusters as best!
- The value of these diagrams is not clear to be; they are hard to
  interpet (Andreas seems to agree)
- But many seem to like them.
 
---
# Cluster Stability

- Try multiple random samplings / perturbations
- Measure consistency

- Clustering stability: an overview 
https://arxiv.org/abs/1007.1075

???
- For comparing clustering algorithms / parameters
(with different number of clusters for example)
- Try multiple random samplings / perturbations
- Q: How might you get different random samplings? A: bagging!
- (Or actually add some noise)
- The configuration that yields the most consistent result among
perturbations is best.
- Nice overview paper

---

# Stability Intuition

.center[
![:scale 80%](images/stability_intuition.png)
]

???
- k=2 doesn't work well, pretty unstable
- Similar for k=5
- But k=4 would probably be pretty stable.

---
.smaller[
```python
def cluster_stability(X, est, n_iter=20, random_state=None):
    labels = []
    indices = []
    for i in range(n_iter):
        # draw bootstrap samples, store indices
        sample_indices = rng.randint(0, X.shape[0], X.shape[0])
        indices.append(sample_indices)
        est = clone(est)
        if hasattr(est, "random_state"):
            # randomize estimator if possible
            est.random_state = rng.randint(1e5)
        X_bootstrap = X[sample_indices]
        est.fit(X_bootstrap)
        # store clustering outcome using original indices
        relabel = -np.ones(X.shape[0], dtype=np.int)
        relabel[sample_indices] = est.labels_
        labels.append(relabel)
    scores = []
    for l, i in zip(labels, indices):
        for k, j in zip(labels, indices):
            # we also compute the diagonal which is a bit silly
            in_both = np.intersect1d(i, j)
            scores.append(adjusted_rand_score(l[in_both], k[in_both]))
    return np.mean(scores)
```]

???
- Some code for this, a little complicated.
- First run estimator several times, saving labels
- -1 for not in bag, otherwise get label
- Then for each pair, look at some score between them
- Then compute mean over all these
- So mean should be higher if clusterings are consistent
---

.center[![:scale 70%](images/cluster_stability.png)]
- Outcome depends critically on initialization and n_iter.
- This is kmeans++ with n_iter=10

???
- Looking at kmeans with various k, showing stability
- Above 4, clearly not stable
- Below that, we have stable clusterings b/c the clusters are far enough apart

---
.center[![:scale 80%](images/cluster_stability_2.png)]
???
- More realistic dataset here
- 2 seems most stable here
- Don't really know what answer is here
- but this is real life! Important to see how these behave

---

# Digits Dataset

.center[
![:scale 70%](images/digits_dataset_ncluster_scan.png)
]
???
- On digits dataset
- Stability does down quickly
- And thinks 8 is the best!
---
# Stability on Digits Dataset
.center[
![:scale 70%](images/cluster_stability_different_algos.png
)
]
???
- Looking at stability of various algorithms
- Q: What are DBSCAN params?
- DBSCAN doesn't have # clusters param, instead has epsilon and min. points
- So here you can see the curve as epsilon varies
- High ep, just 1 cluster and rest noise, decrease -> more clusters, less noise
- really small ep -> mostly noise points
- Overall probably like kmeans the best, but actually there is
randomness so maybe agg is equivalent

---
# Adult Dataset
.center[
![:scale 70%](images/adult_dataset_ncluster_scan.png)
]
???
- People make more/less 50K
- Pretty noisy here
- But stability gives nice smooth results
---
# Labeled Faces in the Wild
.center[![:scale 70%](images/labfaces_ncluster_scan.png)]
- With PCA, 100 components. Without whitening: no stable clusters!

???
- Too high Dimensional, using PCA
- Basically must use whitening here...
- Stability behaves similar to ARI (supervised)
- Whereas silhouette behaves differently, so stability captures
something important

--- 
class:spacious
# Qualitative Evaluation(aka eyeballing)

- Look at low-dim visualization
- Look at individual points
- Look at cluster centers (if available)

???
- Scores can sometimes be helpful
- But usually what you're doing is exploratory analysis
- Important question is: do these clusters MEAN anything

---
# Digits

.center[
![:scale 100%](images/digits_eyeball.png)
]
???
- Using PCA or t-SNE to reduce dimensionality, then 3 clustering algorithms
- DBSCAN doesn't seem to do well here
- Agg and kmeans do well
- various places where Agg and kmeans make mistakes
- Can then look at the various clusters and try to figure out what's
  with the points in the mistake clustera

---

# K-Means cluster centers

.center[
![:scale 70%](images/kmeans_cluster_centers_digits.png)
]

.smaller[
```python
fig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},
                        figsize=(10, 5))
km = KMeans(n_clusters=10).fit(digits.data)
for ax, center in zip(axes.ravel(), km.cluster_centers_):
    ax.imshow(center.reshape(8, 8), cmap='gray_r')
fig.suptitle("K-Means cluster centers")
```]
???
- For things that are easy to visualize (digits, face images), can
  visualize the cluster centers (advantage of kmeans)
- Visualization of digits
- Sometimes it's clear what they mean
- What's is upper right? Maybe combination of 1 and 8?
- In some sense asking the computer what a "canonical" element of each
  cluster looks like

---

# Agglomerative Clusters
.center[
![:scale 90%](images/agglomerative_clusters.png)
]
???
- If you don't have cluster centers, can't do that
- But can pick random points from each cluster
- Each column corresponds to one of 10 clusters, “first” 5 samples are shown.

---
# DBSCAN Clusters
.center[
![:scale 90%](images/dbscan_clusters.png)
]

???
- Number on top gives clusters size
- Gives some info about which clusters are meaningful and which probably not
---

# For feature extraction

.smaller[
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

km = KMeans(n_init=1, init="random")
pipe = make_pipeline(km, LogisticRegression())

param_grid = {'kmeans__n_clusters': [10, 50, 100, 200, 500]}
grid = GridSearchCV(pipe, param_grid, cv=5, verbose=True)
grid.fit(X, y_people)
```]

.center[
![:scale 40%](images/param_kmeans_nclusters.png)
]

???
- Pretty easy to use for feature extraction (on faces dataset)
- So do clustering, then use this as a feature
- Q: Am I doing supervised or unsupervised here?
---

class: spacious
# So what is n_clusters?
- As preprocessing
  – The one that makes the whole pipeline work best.
  – Larger is often better.
- For exploratory analysis:
  – The one that tells you the most about the data.
- For most datasets, there is no “correct” number of clusters.

???
- For exploratory: what is best for YOU to help you think about the data
- For interpretability, you probably want stability. Otherwise (ifyour
  results constantly change), what do you results mean?

---

class:center,middle

If you want semantics from clustering, you need manual confirmation.

Clustering might not pick up on what you thought it would.

    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="https://amueller.github.io/COMS4995-s18/slides/aml-17-032818-clustering-evaluation/' + url +  '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
