<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="/Users/rob/src/reveal/css/reveal.css"/>

<link rel="stylesheet" href="/Users/rob/src/reveal/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="/Users/rob/src/reveal/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = '/Users/rob/src/reveal/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)
<section>
<section id="slide-orgc20fecf">
<h2 id="orgc20fecf">Class Imbalance</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas MÃ¼ller
</p>
<aside class="notes">
<p>
But first, we need to finish metrics for regression&#x2026;.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org245d74c">
<h2 id="org245d74c">Metrics for Regression Models</h2>
<aside class="notes">
<ul>
<li>Now some metrics for regression</li>

</ul>

</aside>
</section>
<section id="slide-org1305b31">
<h3 id="org1305b31">Built-in Standard Metrics</h3>
<ul>
<li>\(R^2\): easy to understand scale</li>
<li>MSE: easy to relate to input</li>
<li>Mean/Median absolute error: more robust</li>
<li>When using "scoring" use <code>neg_mean_squared_error</code> etc.</li>

</ul>
<aside class="notes">
<ul>
<li>\(R^2\), between 0 and 1 (actually not, but usually)</li>
<li>MSE: same units as output</li>

</ul>

</aside>
</section>
<section id="slide-org0139317">
<h3 id="org0139317">Prediction Plots</h3>
<div class="column" style="float:left; width: 75%">
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import Ridge
from sklearn.datasets import load_boston
boston = load_boston()
X_train, X_test, y_train, y_test = \
    train_test_split(boston.data, boston.target)
ridge = Ridge(normalize=True)
ridge.fit(X_train, y_train)
pred = ridge.predict(X_test)
plt.plot(pred, y_test, 'o')
</code></pre>
</div>
</div>
<div class="column" style="float:left; width: 25%">

<div class="figure">
<p><img src="./assets/regression_boston.png" alt="regression_boston.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Prediction plots are great debugging tools</li>
<li>Plot predicted versus true, so you want to get points on the diagonal</li>
<li>So for some of the points we are underpredicting pretty bad (esp. for high values)</li>
<li>And overpredicting for low values</li>

</ul>

</aside>
</section>
<section id="slide-org56e9d0d">
<h3 id="org56e9d0d">Residual Plots</h3>
<div class="column" style="float:left; width: 50%">


<div class="figure">
<p><img src="./assets/regression_boston.png" alt="regression_boston.png" height="250px" />
</p>
</div>


<div class="figure">
<p><img src="./assets/regression_boston_2.png" alt="regression_boston_2.png" height="250px" />
</p>
</div>

</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/regression_boston_3.png" alt="regression_boston_3.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Different way is residual plot, which is basically just rotated</li>
<li>Can also look at histogram of residuals, which I think is pretty intuitive</li>
<li>Note: no matter how many features (dimensions) you have, you can
visualize these in 2 dimensions</li>

</ul>

</aside>
</section>
<section id="slide-orgb412af9">
<h3 id="orgb412af9">Target vs Feature</h3>

<div class="figure">
<p><img src="./assets/target_vs_feature.png" alt="target_vs_feature.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>For more detail, look at per feature plots</li>
<li>We have seen this before, looking at feature vs. target</li>

</ul>

</aside>
</section>
<section id="slide-org84343c5">
<h3 id="org84343c5">Residual vs Feature</h3>

<div class="figure">
<p><img src="./assets/residual_vs_feature.png" alt="residual_vs_feature.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Can also look at feature vs. residual</li>
<li>i.e., plotting feature value to residual (<code>y_test - pred</code>)</li>
<li>For each sample with a particular value for this feature, plot the
difference between prediction and actual value</li>
<li>This can tell you a lot, b/c it can tell you that for specific
values of a specific feature, we are doing really poorly</li>
<li>i.e., there are trends we might be missing</li>
<li>Ideally you want these all clustered around zero, meaning the error
the model makes is independent of each feature</li>
<li>Otherwise, it means there's some pattern in the feature value that we didn't capture</li>

</ul>

</aside>
</section>
<section id="slide-org6a1e322">
<h3 id="org6a1e322">Absolute vs Relative: MAPE</h3>
<p>
Mean Absolute Percentage Error
</p>

<p>
\[ \text{MAPE} = \frac{100}{n} \sum_{i=1}^{n}\left|\frac{y-\hat{y}}{y}\right|\]
</p>


<div class="figure">
<p><img src="./assets/mape.png" alt="mape.png" height="350px" />
</p>
</div>

<aside class="notes">
<ul>
<li>Another metric: Mean Absolute Percentage Error</li>
<li>Captures the percentage error</li>
<li>It's got a residual, clearly taking a mean, and taking absolute
value b/c that's all we care about</li>
<li>Interesting thing is the division of the residual by the true value (e.g., housing price)</li>
<li>If a data point is large, we don't care as much about prediction error, since the value is large anyway</li>
<li>For small values we care more</li>
<li>Which is intuitive: if a house costs 25 million, it doesn't matter that much if you predict a million off</li>
<li>But if a house is 50,000, you better not be off by a million!</li>
<li>Notice the two pairs on the plot, the absolute error between them vs. MAPE</li>
<li>Often used in forecasting (predicting future, like stock prices)</li>
<li>Careful, it might not be intuitive</li>
<li>If anything is zero, well it's not defined</li>
<li>If something is <b>close</b> to zero, you basically have to be perfect or
else the percentage error will be very large</li>

</ul>

</aside>
</section>
<section id="slide-org73ad023">
<h3 id="org73ad023">Over vs under</h3>
<ul>
<li>Overprediction and underprediction can have different cost.</li>
<li>Try to create cost-matrix: how much does overprediction and
underprediction cost?</li>
<li>Is it linear?</li>

</ul>
<aside class="notes">
<ul>
<li>First question: Should you look at absolute error or relative errors?</li>
<li>Can assign costs to under/over prediction</li>
<li>e.g., how much does it cost if I make a prediction and get a
particular residual</li>
<li>This builds an optimization problem, which you can solve</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org6171c44">
<h2 id="org6171c44">Class Imbalance</h2>
<div class="outline-text-2" id="text-org6171c44">
</div>
</section>
<section id="slide-org79c6ff0">
<h3 id="org79c6ff0">Two sources of imbalance</h3>
<ul>
<li>Asymmetric cost</li>
<li>Asymmetric data</li>

</ul>
<aside class="notes">
<ul>
<li>We've already talked about different costs associated with the classes.</li>
<li>e.g., predict cancer vs. non-cancer</li>

</ul>

</aside>
</section>
<section id="slide-orge0ab105">
<h3 id="orge0ab105">Why do we care?</h3>
<ul>
<li>Why should cost be symmetric?</li>
<li>All data is imbalanced</li>
<li>Detect rare events</li>

</ul>
<aside class="notes">
<ul>
<li>No reason why FP and FN should have same cost, e.g., ad-prediction clickthrough rates</li>
<li>Often data is very imbalanced, e.g., rare disease diagnosis, ad clickthrouth rates (0.01% or so)</li>
<li>Balanced data is actually pretty rare</li>

</ul>

</aside>
</section>
<section id="slide-org412b61f">
<h3 id="org412b61f">Changing Thresholds</h3>
<font size=6>
<div class="org-src-container">

<pre><code class="python" >data = load_breast_cancer()
lr = LogisticRegression(solver='lbfgs', max_iter=10000)
X_train, X_test, y_train, y_test = \
    train_test_split(data.data, data.target,
                     stratify=data.target, random_state=0)

lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)

print(classification_report(y_test, y_pred))
</code></pre>
</div>

<pre class="example">
precision    recall  f1-score   support

           0       0.91      0.92      0.92        53
           1       0.96      0.94      0.95        90

   micro avg       0.94      0.94      0.94       143
   macro avg       0.93      0.93      0.93       143
weighted avg       0.94      0.94      0.94       143

</pre>

<div class="org-src-container">

<pre><code class="python" >y_pred = lr.predict_proba(X_test)[:, 1] > .85

print(classification_report(y_test, y_pred))
</code></pre>
</div>

<pre class="example">
precision    recall  f1-score   support

           0       0.84      1.00      0.91        53
           1       1.00      0.89      0.94        90

   micro avg       0.93      0.93      0.93       143
   macro avg       0.92      0.94      0.93       143
weighted avg       0.94      0.93      0.93       143

</pre>
</font>
<aside class="notes">
<ul>
<li>Talked about this last time</li>
<li>Change threshold to shift precision vs. recall</li>

</ul>

</aside>
</section>
<section id="slide-org775f002">
<h3 id="org775f002">roc curve</h3>

<div class="figure">
<p><img src="./assets/roc_svc_rf_curve.png" alt="roc_svc_rf_curve.png" />
</p>
</div>
<div class="notes">
<ul>
<li>Visualizes basically at all possible thresholds</li>

</ul>

</div>
</section>
<section id="slide-org394ef07">
<h3 id="org394ef07">remedies for the model</h3>
<div class="notes">
<ul>
<li>Today let's look at how to change the training/building of the model to take this imbalance into account</li>

</ul>

</div>
</section>
<section id="slide-org1c773bc">
<h3 id="org1c773bc">Mammography Data</h3>
<div class="column" style="float:left; width: 50%">
<font size=6>
<div class="org-src-container">

<pre><code class="python" >import openml
# mammography dataset
# https://www.openml.org/d/310
data = openml.datasets.get_dataset(310)
X, y = data.get_data(target='class')
print(X.shape)
</code></pre>
</div>

<pre class="example">
(11183, 6)

</pre>

</font>

<div class="org-src-container">

<pre><code class="python" >print(np.bincount(y))
</code></pre>
</div>

<pre class="example">
[10923   260]

</pre>

</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/mammography_data.png" alt="mammography_data.png" />
</p>
</div>
</div>
<aside class="notes">
<p>
Note: Split one slide to two due to size constraints?
</p>
<ul>
<li>Example of very imbalanced data sets</li>
<li>Don't need to use this package, can download from web</li>
<li>Whether or not there are calcium deposits in the tissue</li>
<li>Notice the very skewed distributions</li>

</ul>

</aside>
</section>
<section id="slide-orgd2aedbe">
<h3 id="orgd2aedbe">Mammography Data</h3>
<div class="org-src-container">

<pre><code class="python" >X_train, X_test, y_train, y_test = \
    train_test_split(X, y, random_state=0)
lr = LogisticRegression(solver='lbfgs')
scores = cross_validate(lr, X_train, y_train, cv=10,
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
</code></pre>
</div>

<pre class="example">
0.919, 0.636

</pre>

<div class="org-src-container">

<pre><code class="python" >rf = RandomForestClassifier(n_estimators=100)
scores = cross_validate(rf, X_train, y_train, cv=10,
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
</code></pre>
</div>

<pre class="example">
0.940, 0.740

</pre>

<aside class="notes">
<ul>
<li>Baseline models here</li>
<li>This <code>cross_validate</code> lets you specify multiple scoring methods</li>
<li>Returns dictionary of results</li>
<li>Notice difference&#x2026;</li>

</ul>

</aside>
</section>
<section id="slide-org28d100c">
<h3 id="org28d100c">Mammography Data</h3>

<div class="figure">
<p><img src="./assets/mammography_features23.png" alt="mammography_features23.png" />
</p>
</div>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-org73643cd">
<h3 id="org73643cd">Basic Approaches</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/basic_approaches.png" alt="basic_approaches.png" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">
<p>
Change the training procedure
</p>
</div>
<aside class="notes">
<ul>
<li>Want to better adapt to imbalance</li>
<li>Can change data or training procedure</li>
<li>Changing data easier</li>
<li>Can add or remove samples, or both</li>
<li>Lots of strategies for each</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org18b246c">
<h2 id="org18b246c">Change the Data: Sampling</h2>
<div class="outline-text-2" id="text-org18b246c">
</div>
</section>
<section id="slide-orgba900b8">
<h3 id="orgba900b8">Scikit-learn vs. resampling</h3>

<div class="figure">
<p><img src="./assets/pipeline.png" alt="pipeline.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Unfortunately hard to add to sklearn (will be added eventually)</li>
<li>The transform method only transforms X</li>
<li>Pipelines work by chaining transforms</li>
<li>To resample the data, we need to also change y</li>

</ul>

</aside>
</section>
<section id="slide-orgc072b6f">
<h3 id="orgc072b6f">Imbalance-learn</h3>
<p>
<a href="http://imbalanced-learn.org">http://imbalanced-learn.org</a>
</p>
<div class="org-src-container">

<pre><code class="python" >!pip install imbalanced-learn
# or conda install ...
</code></pre>
</div>
<p>
Extends <code>sklearn</code> API
</p>
<aside class="notes">
<ul>
<li>Allows us to resample with a special pipeline</li>

</ul>

</aside>
</section>
<section id="slide-orgb4dd5d8">
<h3 id="orgb4dd5d8">Sampler</h3>
<ul>
<li>To resample data sets, each sampler implements</li>

</ul>
<div class="org-src-container">

<pre><code class="python" >data_resampled, targets_resampled = obj.sample(data, targets)
</code></pre>
</div>
<ul>
<li>Fitting and sampling in one step:</li>

</ul>
<div class="org-src-container">

<pre><code class="python" >data_resampled, targets_resampled = \
	obj.fit_sample(data, targets)
</code></pre>
</div>
<ul>
<li>In pipelines: sampling only done in fit!</li>

</ul>
<aside class="notes">
<ul>
<li>Imbalance-learn extends scikit-learn interface with a "sample" method.</li>
<li>Basically implements a bunch of sampler objects</li>
<li>Which can be fit and then sampled</li>
<li>Maybe you fit to estimate or get some info about the population,
like learning how much you need to sample the majority/minority</li>
<li>Imbalance-learn has a custom pipeline that allows resampling.</li>
<li>We have special pipelines, sampling only done during "fit", when fitting the model</li>
<li>Warning: not everything in imbalance-learn is multiclass!</li>

</ul>

</aside>
</section>
<section id="slide-org32d5a12">
<h3 id="org32d5a12">Random Undersampling</h3>
<div class="org-src-container">

<pre><code class="python" >from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(replacement=False)
X_train_subsample, y_train_subsample = \
    rus.fit_sample(X_train, y_train)
print(X_train.shape)
print(X_train_subsample.shape)
print(np.bincount(y_train_subsample))
</code></pre>
</div>

<pre class="example">
(8387, 6)
(392, 6)
[196 196]

</pre>

<aside class="notes">
<ul>
<li>Drop data from the majority class randomly</li>
<li>Often until balanced</li>
<li>Very fast training (data shrinks to 2x minority)</li>
<li>Loses data!</li>

</ul>

</aside>
</section>
<section id="slide-org0745a96">
<h3 id="org0745a96">Random Undersampling</h3>
<font size=6>
<div class="org-src-container">

<pre><code class="python" >from imblearn.pipeline import make_pipeline as make_imb_pipeline
undersample_pipe = make_imb_pipeline(RandomUnderSampler(), lr)
scores = cross_validate(undersample_pipe,
                        X_train, y_train, cv=10,
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
# was 0.918, 0.631 without
</code></pre>
</div>

<pre class="example">
0.920, 0.533

</pre>

<div class="org-src-container">

<pre><code class="python" >undersample_pipe_rf = \
    make_imb_pipeline(RandomUnderSampler(),
                      RandomForestClassifier(n_estimators=100))
scores = cross_validate(undersample_pipe_rf,
                        X_train, y_train, cv=10, 
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
# was 0.943, 0.738 without
</code></pre>
</div>

<pre class="example">
0.946, 0.624

</pre>

</font>
<aside class="notes">
<ul>
<li>imblearn pipelines</li>
<li>Amazing that roc auc actually improved with a fraction of samples!</li>
<li>As accurate with fraction of samples!</li>
<li>Really good for large datasets</li>
<li>Opposite is oversampling</li>

</ul>

</aside>
</section>
<section id="slide-orga953b8e">
<h3 id="orga953b8e">Random Oversampling</h3>
<div class="org-src-container">

<pre><code class="python" >from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler()
X_train_oversample, y_train_oversample = \
    ros.fit_sample(X_train, y_train)
print(X_train.shape)
print(X_train_oversample.shape)
print(np.bincount(y_train_oversample))
</code></pre>
</div>

<pre class="example">
(8387, 6)
(16382, 6)
[8191 8191]

</pre>

<aside class="notes">
<ul>
<li>Repeat samples from the minority class randomly (with replacement)</li>
<li>Often until balanced</li>
<li>Mostly repeats&#x2026;</li>
<li>Kind of weird&#x2026;</li>
<li>Much slower (dataset grows to 2x majority)</li>

</ul>

</aside>
</section>
<section id="slide-org95fbd1b">
<h3 id="org95fbd1b">Random Oversampling</h3>
<font size=6>
<div class="org-src-container">

<pre><code class="python" >oversample_pipe = make_imb_pipeline(RandomOverSampler(), lr)

scores = cross_validate(oversample_pipe,
                        X_train, y_train, cv=10,
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
# was 0.918, 0.631 without
</code></pre>
</div>

<pre class="example">
0.924, 0.561

</pre>

<div class="org-src-container">

<pre><code class="python" >oversample_pipe_rf = \
    make_imb_pipeline(RandomOverSampler(),
                      RandomForestClassifier(n_estimators=100))
scores = cross_validate(oversample_pipe_rf,
                        X_train, y_train, cv=10,
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
# was 0.943, 0.738 without
</code></pre>
</div>

<pre class="example">
0.918, 0.701

</pre>
</font>
<aside class="notes">
<ul>
<li>Logreg the same, Random Forest much worse than undersampling (about same as doing nothing)</li>

</ul>

</aside>
</section>
<section id="slide-org4773826">
<h3 id="org4773826">Curves for LogReg</h3>

<div class="figure">
<p><img src="./assets/curves_logreg.png" alt="curves_logreg.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Left: Logreg ROC for orig, over, under</li>
<li>Right: precision-recall curve</li>

</ul>

</aside>
</section>
<section id="slide-orgec84a5f">
<h3 id="orgec84a5f">Curves for Random Forest</h3>

<div class="figure">
<p><img src="./assets/curves_rf.png" alt="curves_rf.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Left: RF ROC for orig, over, under</li>
<li>Right: precision-recall curve</li>
<li>Left: undersample seems best on left, but worst on right!</li>
<li>Opposite for over!</li>
<li>These are actually telling you similar things b/c TPR = recall.</li>
<li>Though FPR is very different than precision</li>
<li>But they are rotated and stretched in very different ways&#x2026;</li>

</ul>

</aside>
</section>
<section id="slide-orgdf3c735">
<h3 id="orgdf3c735">ROC or PR?</h3>
<p>
FPR or Precision?
\[ \large\text{FPR} = \frac{\text{FP}}{\text{FP}+\text{TN}}\]
\[ \large\text{Precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}\]
</p>
<aside class="notes">
<ul>
<li>For imbalanced problems, FPR is not that interesting</li>
<li>Typically TN will be much bigger (positives are rare), so FPR will be close to 0</li>
<li>So Precision-Recall curve may be more informative</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-orgfe57e3b">
<h2 id="orgfe57e3b">Change the Training</h2>
<div class="outline-text-2" id="text-orgfe57e3b">
</div>
</section>
<section id="slide-org2b860df">
<h3 id="org2b860df">Class-weights</h3>
<ul>
<li>Instead of repeating samples, re-weight the loss function.</li>
<li>Works for most models!</li>
<li>Same effect as over-sampling (though not random), but not as
expensive (dataset size the same).</li>

</ul>
<aside class="notes">
<p>

</p>

</aside>
</section>
<section id="slide-orgaad823b">
<h3 id="orgaad823b">Class-weights in linear models</h3>
<p>
\[\min_{w \in â^{p}}-C \sum_{i=1}^n\log(\exp(-y_iw^T \textbf{x}_i) + 1) + ||w||_2^2\]
\[ \min_{w \in \mathbb{R}^p} -C \sum_{i=1}^n c_{y_i} \log(\exp(-y_i w^T \mathbf{x}_i) + 1) + ||w||^2_2 \]
Similar for linear and non-linear SVM
</p>
<aside class="notes">
<ul>
<li>Give a weight to each class</li>
<li>They usually sum to 1, weights based on relative fraction of class in samples</li>
<li>Intuition: think of repeating each sample c_y_i times</li>

</ul>

</aside>
</section>
<section id="slide-org5dba418">
<h3 id="org5dba418">Class weights in trees</h3>
<p>
Gini Index: 
\[H_\text{gini}(X_m) = \sum_{k\in\mathcal{Y}} p_{mk} (1 - p_{mk})\]
\[H_\text{gini}(X_m) = \sum_{k\in\mathcal{Y}} c_k p_{mk} (1 - p_{mk})\]
Prediction:
Weighted vote
</p>
<aside class="notes">
<ul>
<li>Change the tree-building procedure</li>
<li>Similar to impurity</li>
<li>Replacing the data points c_k many times</li>

</ul>

</aside>
</section>
<section id="slide-org2f0bed7">
<h3 id="org2f0bed7">Using Class Weights</h3>
<div class="org-src-container">

<pre><code class="python" >lr = LogisticRegression(solver='lbfgs',
                        class_weight='balanced')
scores = cross_validate(lr, X_train, y_train, cv=10,
                        scoring=('roc_auc',
                                 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
</code></pre>
</div>

<pre class="example">
0.924, 0.559

</pre>

<div class="org-src-container">

<pre><code class="python" >rf = RandomForestClassifier(n_estimators=100,
                            class_weight='balanced')
scores = cross_validate(rf, X_train, y_train, cv=10,
                        scoring=('roc_auc',
                                 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
</code></pre>
</div>

<pre class="example">
0.917, 0.702

</pre>

<aside class="notes">
<ul>
<li>Can do this in sklearn</li>
<li><code>balanced</code> makes populations have same size for all classes (intuitively)</li>

</ul>

</aside>
</section>
<section id="slide-org9538b77">
<h3 id="org9538b77">Ensemble Resampling</h3>
<ul>
<li>Random resampling separate for each instance in an ensemble!</li>
<li>Paper: "Exploratory Undersampling for Class Imbalance Learning"</li>
<li>Not in sklearn (yet)</li>
<li>Easy with imblearn</li>

</ul>
<aside class="notes">
<ul>
<li>I don't think in sklearn, but always check the latest documentation to be sure</li>
<li>Idea: build ensemble, like bagging</li>
<li>only you random undersampling for each classifier in your ensemble</li>

</ul>

</aside>
</section>
<section id="slide-org67e6dda">
<h3 id="org67e6dda">Easy Ensemble with imblearn</h3>
<div class="org-src-container">

<pre><code class="python" >tree = DecisionTreeClassifier(max_features='auto')
resampled_rf = \
    BalancedBaggingClassifier(base_estimator=tree,
                              n_estimators=100, random_state=0)
scores = cross_validate(resampled_rf, X_train, y_train,
                        cv=10,
                        scoring=('roc_auc', 'average_precision'))
roc = scores['test_roc_auc'].mean()
avep = scores['test_average_precision'].mean()
print(f"{roc:.3f}, {avep:.3f}")
</code></pre>
</div>

<pre class="example">
0.947, 0.661

</pre>

<aside class="notes">
<ul>
<li>As cheap as undersampling, but much better results than anything else!</li>
<li>Each classifier built with random undersample of dataset</li>
<li>Ensembles help again, b/c undersampling throws away a lot of data&#x2026;</li>
<li>&#x2026;BUT we're doing the sampling over and over, so actually use a lot of the data</li>

</ul>

</aside>
</section>
<section id="slide-org9875664">
<h3 id="org9875664"></h3>

<div class="figure">
<p><img src="./assets/roc_vs_pr.png" alt="roc_vs_pr.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Comparison of curves again</li>
<li>Better in some areas, at least better than understampling with low recall</li>
<li>Plus much cheaper than oversampling</li>
<li>Review: undersampling does sampling ONCE, then uses that to build models</li>
<li>Easy Ensemble does the undersampling separately for each model that it builds</li>

</ul>

</aside>
</section>
</section>
</div>
</div>
<script src="/Users/rob/src/reveal/lib/js/head.min.js"></script>
<script src="/Users/rob/src/reveal/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: '/Users/rob/src/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: '/Users/rob/src/reveal/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: '/Users/rob/src/reveal/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: '/Users/rob/src/reveal/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: '/Users/rob/src/reveal/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: '/Users/rob/src/reveal/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
