<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">

<section>
<section id="slide-orgb79b3c1">
<h2 id="orgb79b3c1">Supervised Learning</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
</section>
<section id="slide-org4370d59">
<h3 id="org4370d59">Supervised Learning</h3>
<p>
\((x_i,y_i) \propto p(x,y)\) i.i.d
</p>

<p>
\(x_i \in \mathbb{R}^p\)
</p>

<p>
\(y_i \in \mathbb{R}\)
</p>

<p>
\(f(x_i) \approx y_i\)
</p>

<p>
\(f(x) \approx y\)
</p>
<aside class="notes">
<p>
As a reminder, in supervised learning, the dataset we learn form is
input-output pairs (xi,yi), where xi is some n-dimensional input, or
feature vector, and yi is the desired output we want to learn to
predict. We assume these samples are drawn from some unknown joint
distribution p(x,y). You can think of this as there being some (not
necessarily deterministic) process that goes from xi to yi, but that
we don’t know. We try to find a function that approximates the output
yi for each known input xi. But we also demand that for new inputs x ,
and unobserved y, f(x) is approximately y.
</p>

<p>
We're going to talk about two simple supervised algorithms today and
in the process talk about evaluating model performance.
</p>

</aside>
</section>
<section id="slide-org7358d07">
<h3 id="org7358d07">Classification vs. Regression</h3>
<p>
<b>Classification</b>: Predict discrete values (class labels)
</p>

<p>
<b>Regression</b>: Predict continuous values
</p>
<aside class="notes">
<p>
The way we'll use regression&#x2026;though actually it is more general
</p>

</aside>
</section>
<section id="slide-org3886e8c">
<h3 id="org3886e8c">k Nearest Neighbors (Review)</h3>

<div class="figure">
<p><img src="./assets/knn_boundary_test_points.png" alt="knn_boundary_test_points.png" height="400px" />
</p>
</div>


<aside class="notes">
<p>
Let’s say we have this two-class classification dataset here, with two features, one on the x axis and one on the y axis. And we have three new points as marked by the stars here. If I make a prediction using a one nearest neighbor classifier, what will it predict? It will predict the label of the closest data point in the training set. That is basically the simplest machine learning algorithm I can come up with.
</p>

</aside>
</section>
<section id="slide-orgaa4423a">
<h3 id="orgaa4423a">k Nearest Neighbors (Review)</h3>

<div class="figure">
<p><img src="./assets/knn_boundary_k1.png" alt="knn_boundary_k1.png" height="400px" />
</p>
</div>

<p>
\(f(x) = y_i, i = \text{argmin}_j ||x_j - x||\)
</p>
<aside class="notes">
<p>
Here’s the formula: the prediction for a new x is the y<sub>i</sub> so that x<sub>i</sub> is the closest point in the training set. 
So how to evaluate this model?
</p>

</aside>
</section>
<section id="slide-org9eccbb7">
<h3 id="org9eccbb7">Accuracy</h3>
<ul>
<li>Split data into training and testing</li>
<li>Evaluate <b>accuracy</b>: # correctly classified / # data points</li>

</ul>
<aside class="notes">
<p>
We talked before about training and testing split.  So we do the split
and then evaluate <b>accuracy</b>, which we define thus for
classification. Just wanted to give the formal definition.
</p>

</aside>
</section>
<section id="slide-org6ea4b82">
<h3 id="org6ea4b82">kNN code with scikit-learn</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = \
	train_test_split(data, target, random_state=0)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)
print("accuracy: {:.2f}".format(knn.score(X_test, y_test)))
# prints "accuracy: 0.77"
</code></pre>
</div>
<aside class="notes">
<p>
You've already seen how to do this, here's a review.
<b>Now let's take a minute to actually implement it!</b>
</p>

</aside>
</section>
<section id="slide-org34f9fe2">
<h3 id="org34f9fe2">kNN Implementation (k=1)</h3>
<div class="org-src-container">

<pre><code class="python" >import math
# fit doesn't need to do anything. Assume we have data and target as np
# arrays Note: this actually only returns the nearest neighbor. You'd
# also need to map this to an actual class based on target.
def predict(X):
	predictions = []
	for x in X:
		best = None
		mn = math.inf
		for ngh in data:
			dist = euclidean_distance(d, ngh)
			if dist < mn:
				mn = dist
				best = ngh
		predictions.append(best)
	return np.array(predictions)
</code></pre>
</div>
<aside class="notes">
<p>
Basic, 1-neighbor implementation (Save k neighbors for hw2?)
</p>

</aside>

</section>
</section>
<section>
<section id="slide-org466be12">
<h2 id="org466be12">Model Hyperparameters</h2>
<div class="outline-text-2" id="text-org466be12">
</div>
</section>
<section id="slide-org77f7a7e">
<h3 id="org77f7a7e">Tuning Parameter: # neighbors</h3>

<div class="figure">
<p><img src="./assets/knn_boundary_k2.png" alt="knn_boundary_k2.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
So this was the predictions as made by one-nearest neighbor. But we can also consider more neighbors, for example three. Here is the three nearest neighbors for each of the points and the corresponding labels. We can then make a prediction by considering the majority among these three neighbors. And as you can see, in this case all the points changed their labels! Clearly the number of neighbors that we consider matters a lot. But what is the right number? 
</p>

<p>
BTW, even k, break ties randomly. Usually just don't use even k though.
</p>

</aside>
</section>
<section id="slide-org2c181d5">
<h3 id="org2c181d5">kNN with more neighbors</h3>

<div class="figure">
<p><img src="./assets/knn_boundary_k3.png" alt="knn_boundary_k3.png" height="400px" />
</p>
</div>

</section>
<section id="slide-org766e9b5">
<h3 id="org766e9b5">Hyperparameters</h3>
<ul>
<li><b>Parameters</b>: values our model learns from data and uses for prediction</li>
<li><b>Hyperparameters</b>: parameters for <i>how</i> our model learns
<ul>
<li>No perfect way to automatically find best</li>

</ul></li>

</ul>
<aside class="notes">
<ul>
<li>In ML we use the data to learn the parameters</li>
<li>In some sense it's like learning which "if" statements to write</li>
<li>But we have these meta-parameters that also determine how we do our learning</li>
<li>knobs, or tuning parameters</li>
<li>These are hyperparameters and we have to find the right away to set them</li>

</ul>

<p>
The is a problem you’ll encounter a lot in machine learning, the problem of tuning parameters of the model, also called hyper-parameters, which can not be learned directly from the data.
</p>

</aside>
</section>
<section id="slide-orga0297c1">
<h3 id="orga0297c1">Influence of Neighbors</h3>

<div class="figure">
<p><img src="./assets/knn_boundary_varying_k.png" alt="knn_boundary_varying_k.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Here’s an overview of how the classification changes if we consider
different numbers of neighbors. You can see as red and blue circles
the training data. And the background is colored according to which
class a datapoint would be assigned to for each location. For one
neighbor, you can see that each point in the training set has a little
area around it that would be classified according to it’s label. This
means all the training points would be classified correctly, but it
leads to a very complex shape of the decision boundary. If we increase
the number of neighbors, the boundary between red and blue simplifies,
and with 40 neighbors we mostly end up with a line. This also means
that now many of the training data points would be labeled
incorrectly.
</p>

<p>
Model Complexity vs. Flexibility
</p>

</aside>
</section>
<section id="slide-org18059be">
<h3 id="org18059be">Model Complexity vs. Accuracy</h3>

<div class="figure">
<p><img src="./assets/knn_model_complexity.png" alt="knn_model_complexity.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
We can look at this in more detail by comparing training and test set scores for the different numbers of neighbors. Here, I did a random 75%/25% split again. This is a very noisy plot as the dataset is very small and I only did a random split, but you can see a trend here. You can see that for a single neighbor, the training score is 1 so perfect accuracy, but the test score is only 70%. If we increase the number of neighbors we consider, the training score goes down, but the test score goes up, with an optimum at 19 and 21, but then both go down again.
</p>

<p>
This is a very typical behavior, that I sketched in a schematic for you.
</p>

<p>
Q: Which is more "complex": many neighbors or few? A: Few!
</p>

</aside>
</section>
<section id="slide-orga7b3e03">
<h3 id="orga7b3e03">Model Complexity vs. Error</h3>

<div class="figure">
<p><img src="./assets/overfitting_underfitting_cartoon_train.png" alt="overfitting_underfitting_cartoon_train.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
here is a cartoon version of how this chart looks in general, though it's horizontally flipped to the one with saw for knn. This chart has accuracy on the y axis, and the abstract concept of model complexity on the x axis. If we make our machine learning models more complex, we will get better training set accuracy, as the model will be able to capture more of the variations in the data.
</p>

</aside>
</section>
<section id="slide-org3df2b27">
<h3 id="org3df2b27">Overfitting and Underfitting</h3>

<div class="figure">
<p><img src="./assets/overfitting_underfitting_cartoon_full.png" alt="overfitting_underfitting_cartoon_full.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
But if we look at the generalization performance, we get a different story. If the model complexity is too low, the model will not be able to capture the main trends, and a more complex model means better generalization. However, if we make the model too complex, generalization performance drops again, because we basically learn to memorize the dataset.
</p>

<p>
If we use too simple a model, this is often called underfitting, while if we use to complex a model, this is called overfitting. And somewhere in the middle is a sweet spot. Most models have some way to tune model complexity, and we’ll see many of them in the next couple of weeks. So going back to nearest neighbors, what parameters correspond to high model complexity and what to low model complexity? high n<sub>neighbors</sub> = low complexity!
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org24d5721">
<h2 id="org24d5721">Sources of error</h2>
<ol>
<li>Noise</li>
<li>Bias</li>
<li>Variance</li>

</ol>
</section>
<section id="slide-orgd77c756">
<h3 id="orgd77c756">Noise</h3>
<ul>
<li>Data is often inherently noisy</li>
<li>Variables you can't model or predict</li>
<li>Life is messy!</li>

</ul>
<aside class="notes">
<p>
Can try to predict the economy from several key indicators, e.g. stock prices.
But you won't get it exactly correct, there's just too much little detail. You'd have to know everything about the brains of everyone contributing to the economy.
</p>

</aside>
</section>
<section id="slide-org18aca35">
<h3 id="org18aca35">Bias</h3>
<ul>
<li>Your model has biased towards certain functions</li>
<li>Over all possible training sets (of size \(n\)), what's the average
fit?</li>
<li>Ex: fitting a constant function -&gt; high bias</li>
<li>low complexity -&gt; high bias</li>

</ul>
<aside class="notes">
<p>
high bias: not very flexible
</p>

</aside>
</section>
<section id="slide-org9f2c8a4">
<h3 id="org9f2c8a4">Variance</h3>
<ul>
<li>How much do specific fits vary from the expected fits?</li>
<li>i.e. do the specific \(n\) data points you get affect the fit a lot?</li>
<li>high complexity -&gt; high variance</li>

</ul>
<aside class="notes">
<p>
Example is kNN with high K: decision boundaries will be crazy!
high complexity is going to latch on to noise -&gt; high variance
</p>

</aside>
</section>
<section id="slide-orgfb8cec1">
<h3 id="orgfb8cec1">Bias-Variance Tradeoff</h3>

<div class="figure">
<p><img src="./assets/biasvariance.png" alt="biasvariance.png" height="400px" />
</p>
</div>

<p>
Source: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a>
</p>
<aside class="notes">
<p>
We can't actually compute bias and variance, since we don't know the true parameters that generate the data&#x2026;we just get the data and want to make the best predictions
</p>

</aside>
</section>
<section id="slide-org661f5c9">
<h3 id="org661f5c9">Error vs. Dataset Size</h3>

<div class="figure">
<p><img src="./assets/error-vs-data1.png" alt="error-vs-data1.png" height="400px" />
</p>
</div>

<p>
Source: <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf</a>
</p>
<aside class="notes">
<p>
For a fixed model complexity&#x2026;
Is this high complexity or low compared to the next?
This one is low
</p>

</aside>
</section>
<section id="slide-org9483f1f">
<h3 id="org9483f1f">Error vs. Dataset Size</h3>

<div class="figure">
<p><img src="./assets/error-vs-data2.png" alt="error-vs-data2.png" height="400px" />
</p>
</div>

<p>
Source: <a href="http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf">http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture12-nup.pdf</a>
</p>
<aside class="notes">
<p>
This one is high because it <b>really</b> overfits until we get lots of data, then it does better
</p>

<p>
In the limit, the "test" error (which approximates the true error) approaches the training error. True error results from bias + noise in the limit. With just a few points, a fixed complexity model will fit these points well.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.10,
minScale: 0.20,
maxScale: 10.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
