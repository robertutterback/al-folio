<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-orge56994b">
<h2 id="orge56994b">Imputation</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
\(2+2\)
</p>

</aside>
</section>
<section id="slide-orgde3386f">
<h3 id="orgde3386f">Dealing with missing values</h3>
<aside class="notes">
<p>
Pandas uses <code>None</code> for missing values, Python's null type.  If it's
missing at random, that's not informative and that's different.  What
we're talking about is missingness that is informative. Maybe someone
didn't fill out a certain question on a form, and there may be a
reason why they didn't fill it out.
</p>

<p>
Only missing input. Missing outputs is different, where you don't know
the true output/class for every data point.
</p>

</aside>
</section>
<section id="slide-orgb32ae0c">
<h3 id="orgb32ae0c"></h3>

<div class="figure">
<p><img src="./assets/l18_01.png" alt="l18_01.png" />
</p>
</div>
<aside class="notes">
<p>
Data from iris, which is unrealistic but we can see it all
One way to deal with this is just to drop the points with missing values.
Or drop column if there's a column that has lots of missing values.
</p>

<p>
Might be fine but (1) losing info, (2) what if you need to make
predictions on new data that has missing values? Or maybe all the
"hard" data points were the ones with missing values &#x2013; you'll think
your accuracy is great, when in reality it's poor.
</p>

</aside>
</section>
<section id="slide-org78f4651">
<h3 id="org78f4651"></h3>

<div class="figure">
<p><img src="./assets/l18_02.png" alt="l18_02.png" />
</p>
</div>
<aside class="notes">
<p>
Build a model, fill in missing values from data in the other rows/columns.
</p>

<p>
Then keep that model, apply it to anything in your test set (or in
production).
</p>

<p>
If missing all features, what can you do? Well, doesn't help building
your main prediction model. But you need to think about what you might
do in practice. Say you are suggesting items to purchase and you have
a new user &#x2014; you have no feature values to use! Maybe use the
average of all or something.
</p>

</aside>
</section>
<section id="slide-org7e9babd">
<h3 id="org7e9babd">Imputation Methods</h3>
<ul>
<li>Mean/Median</li>
<li>kNN</li>
<li>Regression models</li>
<li>Probabilistic models</li>

</ul>
<aside class="notes">
<p>
Mean/median of column.
</p>

<p>
We'll talk about the first 3, just give some pointers to probabilistic
models. These basically try to build a probabilistic model that has a
high probability of generating the dataset (figuring out where the
data comes from/how it is produced), then use that to fill in missing
data.
</p>

</aside>
</section>
<section id="slide-orgdafc9a7">
<h3 id="orgdafc9a7">Baseline: Dropping Columns</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import LogisticRegressionCV
X_train, X_test, y_train, y_test = \
    train_test_split(X_, y, stratify=y)
nan_columns = np.any(np.isnan(X_train), axis=0)
X_drop_columns = X_train[:, ~nan_columns]
scores = cross_val_score(LogisticRegressionCV(v=5), 
                         X_drop_columns, y_train, cv=10)
np.mean(scores)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.772
</code></pre>
</div>

<aside class="notes">
<p>
Again with Iris, which is a toy dataset. Just dropping all columns
with ANY missing data.
</p>

</aside>
</section>
<section id="slide-org9cc126f">
<h3 id="org9cc126f">Mean and Median</h3>

<div class="figure">
<p><img src="./assets/l18_03.png" alt="l18_03.png" />
</p>
</div>
<aside class="notes">
<p>
Only one implemented in sklearn currently is mean, show here.
Just replace a missing field with the mean of the column.
</p>

<p>
BTW, other transforms (e.g. scalers) will not let you use missing
data, it will throw an error.
</p>

</aside>
</section>
<section id="slide-orgd7ac3ed">
<h3 id="orgd7ac3ed"></h3>

<div class="figure">
<p><img src="./assets/l18_04.png" alt="l18_04.png" />
</p>
</div>
<aside class="notes">
<p>
Two other dimensions w/o missing data, not shown here. Just showing data in the other 2 dimensions.
Original dataset has kind-of three clusters, green in the upper right.
</p>

<p>
Data from iris, but just the two features that had values removed.
Note how the green points have moved because we just set their values to the mean!
</p>

<p>
Can do things like doing mean per class, although this is not done that often.
</p>

</aside>
</section>
<section id="slide-org88b5db9">
<h3 id="org88b5db9"></h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScalar
nan_columns = np.any(np.isnan(X_train), axis = 0)
X_drop_columns = X_train[:,~nan_columns]
logreg = make_pipeline(StandardScalar(),
                       LogisticRegression())
scores = cross_val_score(logreg, X_drop_columns,
                         y_train, cv = 10)
print(np.mean(scores))

mean_pipe = make_pipeline(Imputer(), StandardScalar(),
                          LogisticRegression())
scores = cross_val_score(mean_pipe, X_train, y_train, cv=10)
print(np.mean(scores))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.794
0.729
</code></pre>
</div>

<aside class="notes">
<p>
Comparison of dropping columns vs. mean imputation. Actually in this case it's worse!
If you just have a few missing values it might work okay.
</p>

</aside>
</section>
<section id="slide-org2e532c2">
<h3 id="org2e532c2">kNN Imputation</h3>
<ul>
<li>Find k nearest neighbors that have non-missing values.</li>
<li>Fill in all missing values using the average of the neighbors.</li>
<li>Will be in sklearn soon&#x2026;</li>
<li>Not too hard to use <code>KNeighborsClassifier</code> or <code>KNeighborsRegressor</code> manually</li>

</ul>
<aside class="notes">
<p>
For each data point, find nearest neighbors. Use average of neighbors
to fill in missing values. 
</p>

<p>
But if there's missing values, how do you compute distances?
Q: Ideas?
</p>

</aside>
</section>
<section id="slide-org8477574">
<h3 id="org8477574">kNN Imputation Code</h3>
<div class="org-src-container">

<pre id="smallcode"><code class="python" >distances = np.zeros((X_train.shape[0], X_train.shape[0]))
for i, x1 in enumerate(X_train):
    for j, x2 in enumerate(X_train):
        dist = (x1 - x2) ** 2
        nan_mask = np.isnan(dist)
        distances[i, j] = dist[~nan_mask].mean() * X_train.shape[1]
neighbors = np.argsort(distances, axis=1)[:, 1:]
n_neighbors = 3
X_train_knn = X_train.copy()
for feature in range(X_train.shape[1]):
    has_missing_value = np.isnan(X_train[:, feature])
    for row in np.where(has_missing_value)[0]:
        neighbor_features = X_train[neighbors[row], feature]
        non_nan_neighbors = \
            neighbor_features[~np.isnan(neighbor_features)]
        X_train_knn[row, feature] = \
            non_nan_neighbors[:n_neighbors].mean()
</code></pre>
</div>
<aside class="notes">
<p>
Inefficient didactic implementation
</p>

<p>
compute all pairwise distances; element-wise differences, squared (vector op)
mean of all non-nan vals multiplied by #  features
like mean Euclidean distance weighted by how many features are missing
Kind of a heuristic for putting them on the scale
</p>

<p>
Once the distances are computed, look at every feature and then every
row (data point) that is missing that feature, compute the nearest
neighbors that don't have that feature missing, then average their
value for that feature.
</p>

<p>
Pretty commonly used, but pretty slow (n^2 for distances)
</p>

</aside>
</section>
<section id="slide-org64cdf25">
<h3 id="org64cdf25">kNN Imputation Plot</h3>
<div class="org-src-container">

<pre><code class="python" >scores = cross_val_score(logreg, X_train_knn, y_train, cv=10)
np.mean(scores)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.849
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/l18_09.png" alt="l18_09.png" height="450px" />
</p>
</div>
<aside class="notes">
<p>
Looks better than before, all points on the diagonal.
</p>

</aside>
</section>
<section id="slide-orgc19b448">
<h3 id="orgc19b448">Model-Driven Imputation</h3>
<ul>
<li>Train regression model for missing values</li>
<li>Possibly iterate: retrain after filling in</li>
<li>Very flexible!</li>

</ul>
<aside class="notes">
<ol>
<li>Impute the missing values, say using the mean.</li>
<li>Try to predict missing features using regression model trained on non-missing features.</li>
<li>Repeat until things aren't changing much.</li>

</ol>

</aside>
</section>
<section id="slide-orgcad0b0d">
<h3 id="orgcad0b0d">Model-driven imputation with RF</h3>
<div class="org-src-container">

<pre id="smallcode"><code class="python" >rf = RandomForestRegressor(n_estimators=100)
X_imputed = X_train.copy()
for i in range(10):
    last = X_imputed.copy()
    for feature in range(X_train.shape[1]):
        inds_not_f = np.arange(X_train.shape[1])
        inds_not_f = inds_not_f[inds_not_f != feature]
        f_missing = np.isnan(X_train[:, feature])
        rf.fit(X_imputed[~f_missing][:, inds_not_f],
               X_train[~f_missing, feature])
        X_imputed[f_missing, feature] = rf.predict(
            X_imputed[f_missing][:, inds_not_f])
    if (np.linalg.norm(last - X_imputed)) < .5:
        break
scores = cross_val_score(logreg, X_imputed, y_train, cv=10)
np.mean(scores)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.855
</code></pre>
</div>

<aside class="notes">
<p>
Don't worry too much about what random forests does&#x2026;we'll talk about
it later. Assumes we did imputation on X_Train before, like mean.
</p>

<p>
Note that we're using some predicted values to fit a model for missing
values. Ex: we might have missing values in both feature 7
and 42. When building our model for 7, we'll be using predicted data
for 42, and vice versa when building for 42. So every iteration we
hope our predictions will get better.
</p>

<p>
Works better than NN in this case.
</p>

</aside>
</section>
<section id="slide-org5be75bc">
<h3 id="org5be75bc">Imputation Method Comparison</h3>

<div class="figure">
<p><img src="./assets/mean_knn_rf_comparison.png" alt="mean_knn_rf_comparison.png" />
</p>
</div>
<aside class="notes">
<p>
Not super obvious that kNN is better than regression.
</p>

</aside>
</section>
<section id="slide-org94048ac">
<h3 id="org94048ac">Fancyimpute</h3>
<ul>
<li><code>!pip install fancyimpute</code></li>
<li>MICE (<code>IterativeImputer</code>) is iterative and works well often</li>
<li>Try different things in practice, MICE might be best</li>

</ul>
<aside class="notes">
<p>
The best solution in python right now.
</p>

<p>
Mention <code>!pip</code>, might also install <code>mglearn</code>
</p>

<p>
MICE in this package is known as <code>IterativeImputer</code>. Very recently
changed&#x2026;used to be called MICE and not have fit/transform, now
does. Pretty similar to the regression model but does Bayesion
inference.
</p>

</aside>
</section>
<section id="slide-orgcfda2b0">
<h3 id="orgcfda2b0"></h3>

<div class="figure">
<p><img src="./assets/fancy_impute_comparison.png" alt="fancy_impute_comparison.png" height="600px" />
</p>
</div>
<aside class="notes">
<p>
Methods from fancyimpute. Soft impute does matrix factorization,
similar to what you'd do in recommender systems, which we may or may
not get to this semester.
</p>

</aside>
</section>
<section id="slide-orgf215dec">
<h3 id="orgf215dec">Applying <code>fancyimpute</code></h3>
<div class="org-src-container">

<pre><code class="python" >from fancyimpute import IterativeImputer
imputer = IterativeImputer(n_iter=5)
X_complete = imputer.fit_transform(X_train)

scores = cross_val_score(logreg, X_train_fancy_mice, 
                         y_train, cv=10)
np.mean(scores)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.866
</code></pre>
</div>

<aside class="notes">
<p>
I really don't know anything about how this works&#x2026;
This is cheating with the imputation, we're leaking information&#x2026;
</p>

<p>
Q: What should we do instead? A: add our imputer into the pipeline
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
