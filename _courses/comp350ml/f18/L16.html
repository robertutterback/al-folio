<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-orga9a0cd2">
<h2 id="orga9a0cd2">Preprocessing and Feature Engineering</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
Today we’ll talk about preprocessing and featureengineering. What
we’re talking about today mostly applies to linear models, and not to
tree-based models, but it also applies to neural nets and kernel SVMs.
</p>

<p>
\(2+2\)
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org788c8f2">
<h2 id="org788c8f2">Preprocessing Pipelines</h2>
<aside class="notes">
<p>
I want to talk a bit more about preprocessing and cross-validation
here, and introduce pipelines.
</p>

</aside>
</section>
<section id="slide-org08a24fd">
<h3 id="org08a24fd">Leaking Information</h3>
<div class="column" style="float:left; width: 50%">
<p>
Information leak:
<img src="./assets/cv_info_leak.png" alt="cv_info_leak.png" />
</p>
</div>
<div class="column" style="float:left; width: 50%">
<p>
No information leak:
<img src="./assets/cv_no_info_leak.png" alt="cv_no_info_leak.png" />
</p>
</div>

<ul>
<li>Need to include preprocessing in cross-validation!</li>

</ul>
<aside class="notes">
<p>
What we did was we trained the scaler on the training data, and then applied cross-validation to the scaled data. Tha’s what’s show on the left. The problem is that we use the information of all of the training data for scaling, so in particular the information in the test fold. This is also known as information leakage. If we apply our model to new data, this data will not have been used to do the scaling, so our cross-validation will give us a biased result that might be too optimistic.
</p>

<p>
On the right you can see how we should do it: we should only use the training part of the data to find the mean and standard deviation, even in cross-validation. That means that for each split in the cross-validation, we need to scale the data a bit differently. This basically means the scaling should happen inside the cross-validation loop, not outside.
</p>

<p>
In practice, estimating mean and standard deviation is quite robust and you will not see a big difference between the two methods. But for other preprocessing steps that we’ll see later, this might make a huge difference. So we should get it right from the start.
</p>

</aside>
</section>
<section id="slide-org438def1">
<h3 id="org438def1"></h3>
<div class="org-src-container">

<pre><code class="python" >X, y = boston.data, boston.target
X_train, X_test, y_train, y_test = \
	train_test_split(X, y, random_state=0)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
ridge = Ridge().fit(X_train_scaled, y_train)
X_test_scaled = scaler.transform(X_test)
print2(ridge.score(X_test_scaled, y_test))
</code></pre>
</div>

<pre class="example">
(0.634)

</pre>

<div class="org-src-container">

<pre><code class="python" >from sklearn.pipeline import make_pipeline
pipe = make_pipeline(StandardScaler(), Ridge())
pipe.fit(X_train, y_train)
print2(pipe.score(X_test, y_test))
</code></pre>
</div>

<pre class="example">
(0.634)

</pre>

<aside class="notes">
<p>
Now I want to show you how to do preprocessing and crossvalidation right with scikit-learn.
</p>

<p>
At the top here you see the workflow for scaling the data and then applying ridge again. Fit the scaler on the training set, transform on the training set, fit ridge on the training set, transform the test set, and evaluate the model.
</p>

<p>
Because this is such a common pattern, scikit-learn has a tool to make this easier, the pipeline. The pipeline is an estimator that allows you to chain multiple transformations of the data before you apply a final model.
</p>

<p>
You can build a pipeline using the make_pipeline function. Just provide as parameters all the estimators. All but the last one need to have a transform method. Here we only have two steps: the standard scaler and ridge.
</p>

<p>
make_pipeline returns an estimator that does both steps at once. We can call fit on it to fit first the scaler and then ridge on the scaled data, and when we call score, it transforms the data and then evaluates the model.
</p>

</aside>
</section>
<section id="slide-org08bc44c">
<h3 id="org08bc44c">Pipelines</h3>

<div class="figure">
<p><img src="./assets/pipelines.png" alt="pipelines.png" height="500px" />
</p>
</div>
<aside class="notes">
<p>
Let’s dive a bit more into the pipeline. Here is an illustration of what happens with three steps, T1, T2 and Classifier. Imagine T1 to be a scaler and T2 to be any other transformation of the data.
</p>

<p>
If we call fit on this pipeline, it will first call fit on the first step with the input X. Then it will transform the input X to X1, and use X1 to fit the second step, T2. Then it will use T2 to transform the data from X1 to X2. Then it will fit the classifier on X2.
</p>

<p>
If we call predict on some data X’, say the test set, it will call transform on T1, creating X’1. Then it will use T2 to transform X’1 into X’2, and call the predict method of the classifier on X’2. This sounds a bit complicated, but it’s really just doing “the right thing” to apply multiple transformation steps.
</p>

</aside>
</section>
<section id="slide-org2e09439">
<h3 id="org2e09439">Cross-Validation in Pipelines</h3>
<div class="org-src-container">

<pre><code class="python" >knn_pipe = make_pipeline(StandardScaler(),
                         KNeighborsRegressor())
scores = cross_val_score(knn_pipe,
                         X_train, y_train, cv=10)
print2(np.mean(scores), np.std(scores))
</code></pre>
</div>

<pre class="example">
(0.745, 0.106)

</pre>

<aside class="notes">
<p>
How does that help with the cross-validation problem? Because now all
steps are contain in pipeline, we can simply pass the whole pipeline
to crossvalidation, and all processing will happen inside the
cross-validation loop. That solve the data leakage problem.
</p>

<p>
Here you can see how we can build a pipeline using a standard scaler
and kneighborsregressor and pass it to cross-validation.
</p>

</aside>
</section>
<section id="slide-orgbad013c">
<h3 id="orgbad013c">Naming Steps</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.pipeline import make_pipeline
knn_pipe = make_pipeline(StandardScaler(),
                         KNeighborsRegressor())
pp.pprint(knn_pipe.steps)
</code></pre>
</div>

<pre class="example">
[   (   'standardscaler',
        StandardScaler(copy=True, with_mean=True, with_std=True)),
    (   'kneighborsregressor',
        KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',
          metric_params=None, n_jobs=1, n_neighbors=5, p=2,
          weights='uniform'))]

</pre>

<div class="org-src-container">

<pre><code class="python" >from sklearn.pipeline import Pipeline
pipe = Pipeline((("scaler", StandardScaler()),
                 ("regressor", KNeighborsRegressor)))
</code></pre>
</div>
<aside class="notes">
<p>
But let’s talk a bit more about pipelines, because they are great. The
pipeline has an attribute called steps, which &#x2014; contains its
steps. Steps is a list of tuples, where the first entry is a string
and the second is an estimator (model). The string is the “name” that
is assigned to this step in the pipeline. You can see here that our
first step is called “standardscaler” in all lower case letters, and
the second is called kneighborsregressor, also all lower case letters.
</p>

<p>
By default, step names are just lowercased classnames. You can also name the steps yourself using the Pipeline class directly. Then you can specify the steps as tuples of name and estimator. make_pipeline is just a shortcut to generate the names automatically.
</p>

</aside>
</section>
<section id="slide-orgbac6d2c">
<h3 id="orgbac6d2c">Pipeline and GridSearchCV</h3>
<div class="org-src-container">

<pre><code class="python" >knn_pipe = make_pipeline(StandardScaler(),
                         KNeighborsRegressor())
param_grid = \
    {'kneighborsregressor__n_neighbors': range(1, 10)}
grid = GridSearchCV(knn_pipe, param_grid, cv=10)
grid.fit(X_train, y_train)
print(grid.best_params_)
print2(grid.score(X_test, y_test))
</code></pre>
</div>

<pre class="example">
{'kneighborsregressor__n_neighbors': 7}
(0.600)

</pre>

<aside class="notes">
<p>
These names are important for using pipelines with gridsearch. Recall that for using GridSearchCV you need to specify a parameter grid as a dictionary, where the keys are the parameter names. If you are using a pipeline inside GridSearchCV, you need to specify not only the parameter name, but also the step name – because multiple steps could have a parameter with the same name.
</p>

<p>
The way to do this is to use the stepname, then two underscores, and then the parameter name, as the key for the param_grid dictionary.
</p>

<p>
You can see that the bestparams will have this same format.
</p>

<p>
This way you can tune the parameters of all steps in a pipeline at once!
</p>

<p>
And you don’t have to worry about leaking information, since all transformations are contained in the pipeline.
</p>

<p>
You should always use pipelines for preprocessing. Not only does it make your code shorter, it also makes it less likely that you have bugs.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgc32c951">
<h2 id="orgc32c951">Feature Distributions</h2>
<aside class="notes">
<p>
Now that we discussed scaling and pipelines, let’s talk about some
more preprocessing methods. One important aspect is dealing with
different input distributions.
</p>

</aside>
</section>
<section id="slide-orgcbb2ed7">
<h3 id="orgcbb2ed7">Transformed Features</h3>

<div class="figure">
<p><img src="./assets/boston_box_transformed.png" alt="boston_box_transformed.png" height="500px" />
</p>
</div>
<aside class="notes">
<p>
Here is a box plot of the boston housing data after transforming it
with the standard scaler. Even though the mean and standard deviation
are the same for all features, the distributions are quite
different. You can see very concentrated distributions like Crim and
B, and very skewed distribuations like RAD and Tax (and also crim and
B).
</p>

<p>
Many models, in particular linear models and neural networks, work better if the features are approximately normal distributed.
</p>

<p>
Let’s also check out the histograms of the data to see a bit better what’s going on.
</p>

</aside>
</section>
<section id="slide-org025f234">
<h3 id="org025f234">Transformed Histograms</h3>

<div class="figure">
<p><img src="./assets/boston_hist.png" alt="boston_hist.png" />
</p>
</div>
<aside class="notes">
<p>
Clearly CRIM and ZN and B are very peaked, and LSTAT and DIS and Age
are very asymmetric. Sometimes you can use a hack like applying a
logarithm to the data to get better behaved values. There is slightly
more rigorous technique though.
</p>

<p>
Q: If I wanted to apply log to a vector, how could I do that?
A: np.log(x)
If you want to apply it to many data points, loop over them and apply it
</p>

</aside>
</section>
<section id="slide-orgbdcd3b8">
<h3 id="orgbdcd3b8">Box-Cox Transform</h3>
<div class="column" style="float:left; width: 60%">
<div>
\begin{equation}
bc_\lambda(x) = 
\begin{cases}
\frac{x^\lambda - 1}{\lambda} & \text{ if $\lambda \ne 0$}\\
\log(x) & \text{ if $\lambda =0$}\\
\end{cases}
\end{equation}

</div>

<p>
Only applicable for positive \(x\)!
</p>
</div>
<div class="column" style="float:left; width: 40%">

<div class="figure">
<p><img src="./assets/boxcox.png" alt="boxcox.png" height="300px" />
</p>
</div>
</div>

<div class="org-src-container">

<pre><code class="python" ># sklearn 0.20-dev
from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method='box-cox')
# soon: Yeo-Johnson
pt.fit(X)
</code></pre>
</div>
<aside class="notes">
<p>
The Box-Cox transformation is a family of univariate functions to transform your data, parametrized by a parameter lambda. For lamda=1 the function is the identity, for lambda = 2 it is square, lambda =0 is log and there is many other functions in between. For a given dataset, a separate parameter lambda is determined for each feature, by minimizing the skewdness of the data (making skewdness close to zero, not close to -inf), so it is more “gaussian”. The skewdness of a function is a measure of the asymmetry of a function and is 0 for functions that are symmetric around their mean. Unfortunately the Box-Cox transformation is only applicable to positive features.
</p>

</aside>
</section>
<section id="slide-orgaecd643">
<h3 id="orgaecd643">Box-Cox on Boston</h3>
<div class="column" style="float:left; width: 70%">

<div class="figure">
<p><img src="./assets/boston_hist.png" alt="boston_hist.png" height="200px" />
</p>
</div>

<div class="figure">
<p><img src="./assets/boston_hist_boxcox.png" alt="boston_hist_boxcox.png" height="200px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 30%">
<p>
Before
</p>

<br/>
<br/>
<br/>
<p>
After
</p>
</div>
<aside class="notes">
<p>
Here are the histograms of the original data and the transformed data. The title of each subplot shows the estimated lambda. If the lambda is close to 1, the transformation didn’t change much. If it is away from 1, there was a significant transformation.
</p>

<p>
You can clearly see the effect on “CRIM” which was approximately log-transformed, and lstat and nox which were approximately transformed by sqrt.
</p>

<p>
For the binary CHAS the transformation doesn’t make a lot of sense, though.
</p>

</aside>
</section>
<section id="slide-org9906f86">
<h3 id="org9906f86">Box-Cox Scatter</h3>
<div class="column" style="float:left; width: 70%">

<div class="figure">
<p><img src="./assets/boston_housing_scatter.png" alt="boston_housing_scatter.png" height="200px" />
</p>
</div>

<div class="figure">
<p><img src="./assets/boston_bc_scaled_scatter.png" alt="boston_bc_scaled_scatter.png" height="200px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 30%">
<p>
Before
</p>

<br/>
<br/>
<br/>
<p>
After
</p>
</div>

<aside class="notes">
<p>
Here is a comparison of the feature vs response plot before and after the box-cox transformation.
</p>

<p>
The dis, lstat and crim relationships now look a bit more obvious and linear.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org7d6b66e">
<h2 id="org7d6b66e">Discrete Features</h2>
<div class="outline-text-2" id="text-org7d6b66e">
</div>
</section>
<section id="slide-org0fb075c">
<h3 id="org0fb075c">Categorical Variables</h3>
<p>
\(\{ 'red', 'green', 'blue'\} \subset \mathbb{R}^p\) ?
</p>
<aside class="notes">
<p>
Before we can apply a machine learning algorithm, we first need to think about how we represent our data.
</p>

<p>
Earlier, I said \(x \in R^n\). That’s not how you usually get data. Often data has units, possibly different units for different sensors, it has a mixture of continuous values and discrete values, and different measurements might be on totally different scales.
</p>

<p>
First, let me explain how to deal with discrete input variables, also known as categorical features. They come up in nearly all applications.
</p>

<p>
Let’s say you have three possible values for a given measurement, whether you used setup1 setup2 or setup3. You could try to encode these into a single real number, say 0, 1 and 2, or e, &pi;, &tau;.
</p>

<p>
However, that would be a bad idea for algorithms like linear regression
</p>

</aside>
</section>
<section id="slide-org36fb490">
<h3 id="org36fb490">Why not use integer codes?</h3>
<ul>
<li>Puts an ordering on data&#x2026;even when there is none!</li>
<li>Ex: feature with 4 values, classify A or B
<ul>
<li>1 or 4 imply A, 2 or 3 imply B</li>
<li>Linear classifier will not classify correctly</li>
<li>Not linearly separable</li>

</ul></li>

</ul>
<aside class="notes">
<p>
Why not just use integers, e.g., 0, 1, 2 for categorical variables?
But with one hot encoding I've made a 4 dimensional space and linear can learn it.
Regression would depend a lot on the ordering
</p>

</aside>
</section>
<section id="slide-orgdc03d1d">
<h3 id="orgdc03d1d">One-Hot Encoding</h3>
<p>
\[
\matrix{&\text{red}&\text{green}&\text{blue}\\
& 1 & 0 & 0\\
& 0 & 1 & 0\\
& 0 & 0 & 1\\}
\]
</p>
<aside class="notes">
<p>
If you encode all three values using the same feature, then you are imposing a linear relation between them, and in particular you define an order between the categories. Usually, there is no semantic ordering of the categories, and so we shouldn’t introduce one in our representation of the data.
</p>

<p>
Instead, we add one new feature for each category,
</p>

<p>
And that feature encodes whether a sample belongs to this category or not.
</p>

<p>
That’s called a one-hot encoding, because only one of the three features in this example is active at a time.
</p>

<p>
You could actually get away with n-1 features, but in machine learning
that usually doesn’t matter. If you're worried about colinearities,
you might care. However often it's easier to interpet the model with
the extra 1 in there. Depends on the model sometimes.
</p>

</aside>
</section>
<section id="slide-org65374a8">
<h3 id="org65374a8"></h3>

<div class="figure">
<p><img src="./assets/boros1.png" alt="boros1.png" height="200px" />
</p>
</div>

<div class="figure">
<p><img src="./assets/boros2.png" alt="boros2.png" height="200px" />
</p>
</div>
<aside class="notes">
<p>
Sci-kit makes a lot of things really simple&#x2026; but unfortunately this
is not as easy as it should be yet. One option is to use Pandas.
</p>

<p>
<code>get_dummies</code> leaves integers alone. Applies transformation to all
columns that have data type that is object or categorical (built into
Pandas). Strings are "objects", so it works."
</p>

</aside>
</section>
<section id="slide-org598a17b">
<h3 id="org598a17b"></h3>

<div class="figure">
<p><img src="./assets/boros3.png" alt="boros3.png" height="200px" />
</p>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/boros4.png" alt="boros4.png" height="200px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/boros5.png" alt="boros5.png" height="200px" />
</p>
</div>
</div>
<aside class="notes">
<p>
If someone has already encoded to integers, which you might think is
helpful at first, well then this won't work since it leaves integers
alone. But you can explicitly tell it which columns to look at.
</p>

</aside>
</section>
<section id="slide-org0cf1074">
<h3 id="org0cf1074"></h3>

<div class="figure">
<p><img src="./assets/boros6.png" alt="boros6.png" />
</p>
</div>
<aside class="notes">
<p>
Need to be careful if you data set changes. So in this case I got a
different dataset with different Boros, and it didn't have queens but
it does have staten island. (Or maybe your test set doesn't have
queens but has staten island). If you use <code>get_dummies</code> it will look
like the same shape and you might even combine the datasets without
noticing. Scikit learn completely ignores column names, so it won't
tell you!
</p>

</aside>
</section>
<section id="slide-org391a58d">
<h3 id="org391a58d">Pandas Categorical Columns</h3>
<div class="org-src-container">

<pre id="cattable"><code class="ipython" >import pandas as pd
df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],
                   'boro': ['Manhattan', 'Queens', 'Manhattan',
                            'Brooklyn', 'Brooklyn', 'Bronx']})
df['boro'] = pd.Categorical(
    df.boro,
	categories=['Manhattan', 'Queens', 'Brooklyn', 'Bronx', 'Staten Island'])
pd.get_dummies(df)
</code></pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">&#xa0;</th>
<th scope="col" class="org-right">salary</th>
<th scope="col" class="org-right">boro_Manhattan</th>
<th scope="col" class="org-right">boro_Queens</th>
<th scope="col" class="org-right">boro_Brooklyn</th>
<th scope="col" class="org-right">boro_Bronx</th>
<th scope="col" class="org-right">boro_Staten Island</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">103</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">89</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-right">142</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-right">54</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-right">63</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-right">219</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>
Fix the problem by explicitly creating Pandas categorical types.  Now
I'm explicitly telling pandas what the categories all and
<b>transforming</b> a column of the dataset.  Even though Staten Island
isn't in our dataset a column is created. And by applying this
transformation to both train and test it will make sure the columns
are all there and match up.
</p>

</aside>
</section>
<section id="slide-org10f7871">
<h3 id="org10f7871">OneHotEncoder</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.preprocessing import OneHotEncoder
df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],
                   'boro': [0, 1, 0, 2, 2, 3]})
ohe = OneHotEncoder(categorical_features=[1]).fit(df)
ohe.transform(df).toarray()
</code></pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">103</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">89</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">142</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">54</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">63</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">219</td>
</tr>
</tbody>
</table>

<ul>
<li>Only works for integers right now, not strings.</li>

</ul>
<aside class="notes">
<p>
Here's a way to do it in sklearn, here's the old way (out of 2)
Problem: it only considers values given during training! If it sees a
new category during testing it will either throw it out or throw an
error.  
</p>

<p>
Only ints, not strings. 
</p>

<p>
If you don't tell it the categorical features, it will assume ALL
columns are (I think), and create number of columns equal to the
highest value! E.g., for salary it would create 219 (220?) features!
</p>

</aside>
</section>
<section id="slide-org148030f">
<h3 id="org148030f">Categorical Encoder</h3>
<div class="org-src-container">

<pre><code class="python" >df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],
                   'boro': ['Manhattan', 'Queens', 'Manhattan',
                            'Brooklyn', 'Brooklyn', 'Bronx']})
ce = CategoricalEncoder().fit(df)
ce.transform(df).toarray()
</code></pre>
</div>

<p>
[[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],<br />
[ 0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.],<br />
[ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],<br />
[ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],<br />
[ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],<br />
[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]]<br />
</p>

<p>
Always transforms all columns.
</p>

<aside class="notes">
<p>
The new way, which again is not released yet, is this.
</p>

<p>
This works on strings and integers, always works on all columns. Is a
bit smarter for integers; just counts the number of unique values,
e.g., for salary 6 columns added here.  4 for Boroughs since there's
only 4 unique ones here.
</p>

<p>
Fit-transform paradigm ensures train and test-set categories
correspond.
</p>

</aside>
</section>
<section id="slide-org83b142c">
<h3 id="org83b142c">The Future?</h3>
<p>
<code>CategoricalEncoder</code> + <code>ColumnTransformer</code>
</p>
<div class="org-src-container">

<pre><code class="python" >categorical = df.dtypes == object
preprocess = make_column_transformer(
    (StandardScaler(), ~categorical),
    (CategoricalEncoder(), categorical))
model = make_pipeline(preprocess, LogisticRegression())
</code></pre>
</div>
<aside class="notes">
<p>
Again next version of sklearn.
</p>

<p>
Call StandardScaler on all Non-categorical features and categorical encoder on all categorical.
</p>

</aside>
</section>
<section id="slide-org0a1a249">
<h3 id="org0a1a249">OneHot vs Statisticians</h3>
<ul>
<li>One-hot is redundant (last one is 1 – sum of others)</li>
<li>Can introduce co-linearity</li>
<li>Can drop one</li>
<li>Choice which one matters for penalized models</li>
<li>Keeping all can make the model more interpretable</li>

</ul>
</section>
<section id="slide-org2feae3f">
<h3 id="org2feae3f">Models Supporting Discrete Features</h3>
<ul>
<li>In principle:
<ul>
<li>All tree-based models, naive Bayes</li>

</ul></li>
<li>In scikit-learn:
<ul>
<li>None</li>

</ul></li>
<li>In scikit-learn soon:
<ul>
<li>Decision trees, random forests</li>

</ul></li>

</ul>
<aside class="notes">
<p>
In theory trees and this thing called naive Bayes can <b>directly</b>
handle discrete features (e.g. just pass in strings). But currently
scikit-learn cannot handle that, though it's probably coming.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
