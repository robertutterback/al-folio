<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-org784c048">
<h2 id="org784c048">Preprocessing and Feature Engineering</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
Today we’ll talk about preprocessing and featureengineering. What
we’re talking about today mostly applies to linear models, and not to
tree-based models, but it also applies to neural nets and kernel SVMs.
</p>

<p>
\(2+2\)
</p>

</aside>
</section>
<section id="slide-org95ddd5b">
<h3 id="org95ddd5b"></h3>

<div class="figure">
<p><img src="./assets/boston_housing_scatter.png" alt="boston_housing_scatter.png" />
</p>
</div>
<aside class="notes">
<p>
Let’s go back to the boston housing dataset. The idea was to predict
house prices. Here are the features on the x axis and the response, so
price, on the y axis.
</p>

<p>
What are some thing you can notice? (concentrated distributions,
skewed distibutions, discrete variable, linear and non-linear effects,
different scales)
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgb3d64ca">
<h2 id="orgb3d64ca">Scaling</h2>
<div class="outline-text-2" id="text-orgb3d64ca">
</div>
</section>
<section id="slide-orgbaf9c19">
<h3 id="orgbaf9c19"></h3>
<div class="org-src-container">

<pre><code class="python" >plt.figure()
plt.boxplot(X)
plt.xticks(np.arange(1, X.shape[1] + 1), features,
		   rotation=30, ha="right")
plt.ylabel("MEDV")
</code></pre>
</div>


<div class="figure">
<p><img src="assets/bostonbox.png" alt="bostonbox.png" />
</p>
</div>

<aside class="notes">
<p>
Let’s start with the different scales.
</p>

<p>
Many models want data that is on the same scale. KNearestNeighbors: If
the distance in TAX is between 300 and 400 then the distance
difference in CHAS doesn’t matter!
</p>

<p>
Linear models: the different scales mean different penalty. L2 is the same for all!
</p>

<p>
We can also see non-gaussian distributions here btw!
</p>

<p>
Code: <code>ha</code> is "horizontal alignment", just says which point should be
the anchor when rotating
</p>

</aside>
</section>
<section id="slide-org01da4de">
<h3 id="org01da4de">Ways to Scale Data</h3>

<div class="figure">
<p><img src="./assets/scaling_plots.png" alt="scaling_plots.png" />
</p>
</div>
<aside class="notes">
<p>
StandardScaler: subtract mean, divide by standard deviation.
</p>

<p>
MinMaxScaler: subtract minimum, divide by range. Afterwards between 0 and 1.
</p>

<p>
Robust Scaler: uses median and quantiles, therefore robust to outliers. Similar to StandardScaler.
</p>

<p>
Normalizer: only considers angle, not length. Helpful for histograms, not that often used.
</p>

<p>
StandardScaler is usually good, but doesn’t guarantee particular min and max values
</p>

</aside>
</section>
<section id="slide-org63d51b7">
<h3 id="org63d51b7">Sparse Data</h3>
<ul>
<li>Data with many zeros &#x2014; only store non-zero entries</li>
<li>Subtracting anything will make data "dense" &#x2014; often can't fit in memory</li>
<li>Only scale, don't center (<code>MaxAbsScaler</code>)</li>

</ul>
<aside class="notes">
<p>
You have to be careful if you have sparse data. Sparse data is data
where most entries of the data-matrix X are zero – often only 1% or
less are not zero.
</p>

<p>
You can store this efficiently by only storing the nonzero
elements. Subtracting the mean results in all features becoming
non-zero!
</p>

<p>
So don’t subtract anything, but you can still scale. MaxAbsScaler
scales between -1 and 1 by dividing with the maximum absolute value
for each feature.
</p>

<p>
Part of me thinks this is bad design: the "sparse" data structures
needn't assume 0 as the missing value. You should be able to set
whatever the missing values are, and algorithms can use this. But
there may be some technical details I can't yet see.
</p>

</aside>
</section>
<section id="slide-org50d37b5">
<h3 id="org50d37b5">Standard Scaler Example</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.preprocessing import StandardScaler
X_train, X_test, y_train, y_test = \
	train_test_split(X, y, random_state=0)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

ridge = Ridge().fit(X_train_scaled, y_train)
X_test_scaled = scaler.transform(X_test)
print("{:.2f}".format(ridge.score(X_test_scaled, y_test)))
</code></pre>
</div>

<pre class="example">
0.63

</pre>


<aside class="notes">
<p>
Here’s how you do the scaling with StandardScaler in scikit-learn. Similar interface to models, but “transform” instead of “predict”. “transform” is always used when you want a new representation of the data.
</p>

<p>
Fit on training set, transform training set, fit ridge on scaled data, transform test data, score scaled test data.
</p>

<p>
The fit computes mean and standard deviation on the training set, transform subtracts the mean and the standard deviation.
</p>

<p>
We fit on the training set and apply transform on both the training and the test set. That means the training set mean gets subtracted from the test set, not the test-set mean. That’s quite important.
</p>

</aside>
</section>
<section id="slide-org580f56b">
<h3 id="org580f56b">Importance of Scaling</h3>

<div class="figure">
<p><img src="./assets/scaling_importance.png" alt="scaling_importance.png" />
</p>
</div>
<aside class="notes">
<p>
Here’s an illustration why this is important using the min-max scaler. Left is the original data. Center is what happens when we fit on the training set and then transform the training and test set using this transformer. The data looks exactly the same, but the ticks changed. Now the data has a minimum of zero and a maximum of one on the training set. That’s not true for the test set, though. No particular range is ensured for the test-set. It could even be outside of 0 and 1. But the transformation is consistent with the transformation on the training set, so the data looks the same.
</p>

<p>
On the right you see what happens when you use the test-set minimum and maximum for scaling the test set. That’s what would happen if you’d fit again on the test set. Now the test set also has minimum at 0 and maximum at 1, but the data is totally distorted from what it was before. So don’t do that.
</p>

</aside>
</section>
<section id="slide-org04e9482">
<h3 id="org04e9482">Scikit-Learn API</h3>

<div class="figure">
<p><img src="./assets/sklearn_api.png" alt="sklearn_api.png" />
</p>
</div>

<p>
Shortcuts:
</p>
<ul>
<li><code>est.fit_transform(X) == est.fit(X).transform(X)</code> (mostly)</li>
<li><code>est.fit_predict(X) == est.fit(X).predict(X)</code> (mostly)</li>

</ul>
<aside class="notes">
<p>
Here’s a summary of the scikit-learn methods. All models have a fit method which takes the training data X_train. If the model is supervised, such as our classification and regression models, they also take a y_train parameter. The scalers don’t use a y_train because they don’t use the labels at all – you could say they are unsupervised methods, but arguably they are not really learning methods at all. Models (also known as estimators in scikit-learn) to make a prediction of a target variable, you use the predict method, as in classification and regression. If you want to create a new representation of the data, a new kind of X, then you use the transform method, as we did with scaling. The transform method is also used for preprocessing, feature extraction and feature selection, which we’ll see later. All of these change X into some new form. There’s two important shortcuts. To fit an estimator and immediately transform the training data, you can use fit_transform. That’s often more efficient then using first fit and then transform. The same goes for fit_predict.
</p>

</aside>
</section>
<section id="slide-orgcbef201">
<h3 id="orgcbef201">Scaling and Distances</h3>

<div class="figure">
<p><img src="./assets/knn_scaling.png" alt="knn_scaling.png" />
</p>
</div>
<aside class="notes">
<p>
Here is an example of the importance of scaling using a distance-based algorithm, K nearest neighbors. My favorite toy dataset with two classes in two dimensions. The scatter plots look identical, but on the left hand side, the two axes have very different scales. The x axis has much larger values than the y axis. On the right hand side, I used standard scaler and so both features have zero mean and unit variance. So what do you think will happen if I use k nearest neighbors here? Let's see
</p>

</aside>
</section>
<section id="slide-orgf869e5f">
<h3 id="orgf869e5f">Scaling and Distances</h3>

<div class="figure">
<p><img src="./assets/knn_scaling2.png" alt="knn_scaling2.png" />
</p>
</div>
<aside class="notes">
<p>
As you can see, the difference is quite dramatic. Because the X axis has such a larger magnitude on the left-hand side, only distances along the x axis matter. However, the important feature for this task is the y axis. So the important feature gets entirely ignored because of the different scales. And usually the scales don't have any meaning - it could be a matter of changing meters to kilometers.
</p>

</aside>
</section>
<section id="slide-orgaaddd9e">
<h3 id="orgaaddd9e"></h3>
<div class="org-src-container">

<pre><code class="python" >scores = cross_val_score(RidgeCV(), X_train, y_train, cv=10)
print2(np.mean(scores), np.std(scores))
</code></pre>
</div>

<pre class="example">
(0.717, 0.125)

</pre>

<div class="org-src-container">

<pre><code class="python" >scores = cross_val_score(RidgeCV(), X_train_scaled, y_train, cv=10)
print2(np.mean(scores), np.std(scores))
</code></pre>
</div>

<pre class="example">
(0.718, 0.127)

</pre>

<div class="org-src-container">

<pre><code class="python" >scores = cross_val_score(KNeighborsRegressor(), X_train, y_train, cv=10)
print2(np.mean(scores), np.std(scores))
</code></pre>
</div>

<pre class="example">
(0.499, 0.146)

</pre>

<div class="org-src-container">

<pre><code class="python" >scores = cross_val_score(KNeighborsRegressor(), X_train_scaled, y_train, cv=10)
print2(np.mean(scores), np.std(scores))
</code></pre>
</div>

<pre class="example">
(0.750, 0.106)

</pre>

<aside class="notes">
<p>
Let’s apply the scaler to the Boston housing data.
</p>

<p>
First I used the StandardScaler to scale the training data. Then I applied ten-fold cross-validation to evaluate the Ridge model on the data with and without scaling. I used RidgeCV which automatically picks alpha for me. With and without scaling we get an R^2 of about .72, so no difference. Often there is a difference for Ridge, but not in this case.
</p>

<p>
If we use KneighborsRegressor instead, we see a big difference. Without scaling R^2 is about .5, and with scaling it’s .75. That makes sense since we saw that for distance calculations basically all features are dominated by the TAX feature.
</p>

<p>
However, there is a bit of a problem with the analysis we did here. Can you see it?
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
