<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-orge77597b">
<h2 id="orge77597b">Computational Considerations</h2>
<aside class="notes">
<p>
For all linear models
</p>

</aside>
</section>
<section id="slide-org362b0a0">
<h3 id="org362b0a0">Solver choices</h3>
<ul>
<li>Don't use <code>SVC(kernel='linear')</code>, use <code>LinearSVC</code></li>
<li>When \(P >> n\): Lars (or LassoLars) instead of Lasso</li>
<li>For small \(n\) (\(< 10,000\)), don't worry &#x2014; it will be fast enough</li>
<li><code>LinearSVC, LogisticRegression</code> when \(n >> P\): <code>dual=False</code></li>
<li><code>LogisticRegression(solver="sag")</code> when \(n\) is really big (100,000+)</li>
<li>Stochastic Gradient Descent also good when \(n\) is large</li>

</ul>
<aside class="notes">
<p>
svc uses ovo and hinge loss, LinearSVC uses ova and squared hinged
loss, linearSVC will be faster
</p>

<p>
lars: feature selection much more quickly (for regression)
</p>

<p>
defining "large" is tenous; depends on # features, there's some tradeoffs there
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org84e85ad">
<h2 id="org84e85ad">Efficient Cross-Validation</h2>
<div class="outline-text-2" id="text-org84e85ad">
</div>
</section>
<section id="slide-orgc0d1109">
<h3 id="orgc0d1109">Models with built-in CV</h3>
<ul>
<li><code>LarsCV()</code>, <code>LassoLarsCV()</code>, <code>ElasticNetCV()</code> (use path algorithms to compute full solution path)</li>
<li><code>LogisticRegressionCV()</code> uses warm-starts doesn't support all solvers</li>
<li>All have reasonable built-in parameter grids</li>
<li><code>RidgeCV()</code> does GCV, approximations to LOO (can't pick the cv)</li>

</ul>
<aside class="notes">
<p>
Don't use these for the HW, just use GridSearchCV like in previous slides
</p>

<p>
LARs is a different model for regression: Least Angle Regression
</p>

<p>
For everything but RidgeCV, you can tell it what cross validation to
use, e.g. number of folds, but will use a different algorithm to
compute the best params all at once
</p>

<p>
RidgeCV will independently solve for each param, but has a trick to
make CV go faster. It approximates LOO (Q: What is that?)
</p>

</aside>
</section>
<section id="slide-org88aed8d">
<h3 id="org88aed8d">Using EstimatorCV</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.datasets import load_boston
boston = load_boston()
X_train, X_test, y_train, y_test = train_test_split(
    boston.data, boston.target, random_state=42 )
grid = GridSearchCV(Ridge(),
                    param_grid={'alpha': np.linspace(.1, 1, 10)},
                    cv=10 )
grid.fit(X_train, y_train)
print("Grid-search score: {:.2f}".format(grid.score(X_test,y_test)))
print("grid alpha: {}".format(grid.best_params_['alpha']))

ridge = RidgeCV().fit(X_train, y_train)
print("ridgecv score: {:.2f}".format(ridge.score(X_test, y_test)))
print("ridgecv alpha: {}".format(ridge.alpha_))
</code></pre>
</div>

<pre class="example">
Grid-search score: 0.68
grid alpha: 0.1
ridgecv score: 0.68
ridgecv alpha: 0.1

</pre>

<aside class="notes">
<p>
Both have same score: 0.68, find same alpha: 0.1
But second one is 10 times faster
</p>

<p>
Problem is when you need to do preprocessing steps..Want
preprocessing to be inside cross validation, but you can't really do
it with this built in CV and training.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgbcce3f7">
<h2 id="orgbcce3f7">Stochastic Gradient Descent</h2>
<div class="outline-text-2" id="text-orgbcce3f7">
</div>
</section>
<section id="slide-org870f5ca">
<h3 id="org870f5ca">Big Data</h3>
<ul>
<li>As \(n\) increases, the model variance goes down &#x2013; better fit!</li>
<li>But so does computation time!</li>
<li>Exact solution is too expensive; can't look at all data</li>
<li>Use a worse algorithm that can look at all data</li>

</ul>
</section>
<section id="slide-org51f7a9f">
<h3 id="org51f7a9f">Gradient Descent</h3>
<div class="column" style="float:left; width: 50%">
<p>
Want: \[ \text{arg} \min_w F(w) \]
Initialize \(w_0\) \[ w^{(i+1)} = w^{(i)} - \eta_i \frac{d}{dw} F(w^{(i)}) \]
Converges to local minimum
</p>
</div>
<div class="column" style="float:right; width: 50%">

<div class="figure">
<p><img src="./assets/gradient_descent.png" alt="gradient_descent.png" height="400px" />
</p>
</div>
</div>
</section>
<section id="slide-org41831ce">
<h3 id="org41831ce">Gradient Descent Path</h3>

<div class="figure">
<p><img src="./assets/gd_path.png" alt="gd_path.png" height="400px" />
</p>
</div>

<p>
\[ w^{(i+1)} = w^{(i)} - \eta_i \frac{d}{dw} F(w^{(i)}) \]
</p>
</section>
<section id="slide-org4074a8f">
<h3 id="org4074a8f">Learning Rate</h3>

<div class="figure">
<p><img src="./assets/gc_learning_rate.png" alt="gc_learning_rate.png" />
</p>
</div>

<p>
\[ w^{(i+1)} = w^{(i)} - \eta_i \frac{d}{dw} F(w^{(i)}) \]
</p>
</section>
<section id="slide-orge2aae2c">
<h3 id="orge2aae2c">Stochastic Gradient Descent</h3>
<ul>
<li>Logistic regression: \[ F(w) = C \sumn \log(\exp(-y_i w^T \vec{x}_i) + 1) + \ltwo{w}^2 \]</li>
<li>Pick \(x_i\) randomly, then: \[ \frac{d}{dw} F(w) = \frac{d}{dw} C \log(\exp(-y_i w^T \vec{x}_i) + 1) + \frac1n \ltwo{w}^2 \]</li>
<li>In practice, just iterate over \(i\)</li>

</ul>
</section>
<section id="slide-org3252a4e">
<h3 id="org3252a4e">SGD and partial <code>_</code> fit</h3>
<ul>
<li><code>SGDClassifier()</code>, <code>SGDRegressor()</code> fast on very large datasets</li>
<li>Tuning learning rate and schedule can be tricky</li>
<li><code>partial_fit</code> allows working with out-of-memory data!</li>

</ul>
</section>
<section id="slide-org5bc12b9">
<h3 id="org5bc12b9"></h3>
<div class="org-src-container">

<pre><code class="python" >sgd = SGDClassifier()
for X_batch, y_batch in batches:
	sgd.partial_fit(X_batch, y_batch, classes=[0,1,2])
sgd.score(X_test, y_test)

sgd = SGDClassifier()
for i in range(10):
	for X_batch, y_batch in batches:
		sgd.partial_fit(X_batch, y_batch, classes=[0,1,2])
sgd.score(X_test, y_test)

</code></pre>
</div>
<aside class="notes">
<p>
First has 0.815, second 0.947
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
