 <!DOCTYPE html>
<html>
  <head>
    <title>Word Embeddings</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-container .smaller p, .remark-slide-container .smaller li,
      .remark-slide-container .smaller .remark-code, .remark-slide-container .smaller a{
          font-size: 25px;
      }
      .normal {
          font-size: 30px;
      }
      .remark-slide-container .smallerr p, .remark-slide-container .smallerr li,
      .remark-slide-container .smallerr .remark-code, .remark-slide-container .smallerr a{
          font-size: 20px;
      }
      .remark-slide-container .smallest p, .remark-slide-container .smallest li,
      .remark-slide-container .smallest .remark-code, .remark-slide-container .smallest a{
          font-size: 15px;
      }

      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      
      .left-column30 {
      width: 30%;
      float: left;
		}

	  .right-column70 {
      width: 70%;
      float: right;
      }


      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .column:first-of-type {float:left}
      .column:last-of-type {float:right}
      .tiny-code .remark-code, .remark-inline-code .tiny-code{
        font-size: 15px;
      }
      .split-40 .column:first-of-type {width: 50%}
      .split-40 .column:last-of-type {width: 50%}
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Word Embeddings

Based on slides by Andreas Müller

???
- word embeddings are the logical next step in doing text analysis
after working with topic models.

---

# Beyond Bags of Words

Limitations of bag of words:
- Semantics of words not captured
- Synonymous words not represented
- Very distributed representation of documents

???

- Last time we started talking about limitations of bag of words
- semantics of words are not captured 
- synonymous words are not represented
- if I use two synonymous words, for example film and movie, they are
  represented in completely different ways and there is no connection
  between them.
- and we also have a very distributed representation of documents,
  which might be hard to understand or hard to learn form.
  
---

# Last Time

- Latent Semantic Analysis
- Non-negative Matrix Factorization
- Latent Dirichlet Allocation
- All embed documents into a continuous, corpus-specific space.
- Today: Embed words in a “general” space.

???

- Last time we looked at latent semanic analysis, non-negative matrix
factorization and latent dirichlet allocation.
- Each of them embeds the documents into a continuous, corpus
specific space of a dimension of our choosing, using an unsupervised
objective.
- The first two are matrix factorization, the third is a probabilistic
generative model.
- Today we'll be talking about a more general embedding, 
- tries to embed individual words, not documents.
- The goal is to learn an embedding that's generally useful, and not
necessarily tied to a particular corpus.

---
# Idea

- Unsupervised extraction of semantics using large corpus (wikipedia etc)
- Input: one-hot representation of word (as in BoW).
- Use auxiliary task to learn continuous representation.

???

- We'll talk about several methods, but they'll all use a very large
corpus to create a relatively generic embedding.
- So we might use something like wikipedia or google news or a big
corpus of books.
- It's an unsupervised task, but the way it is phrased is using an
auxiliary supervised task to learn the embedding.
- And what I mean by that will be more clear shortly.
- The starting principle is a one-hot representation of each word, as
in Bag of words, which basically just means each word is identified by
some index.

---
# Skip-Gram models

- Given a word, predict surrounding word
- Supervised task, each document yields many examples
- Not interested in performance for this task, just want to learn representations.

???

- A very commonly used auxiliary task is the skip-gram model.
- Here the task is, given a word, predict the surrounding words. This
is a supervised task, with each document providing many examples,
basically as many as there are words in the document.
- I'm saying this is an auxiliary task, because we're not really
interested in actually solving this problem of predicting surrounding
words, we just want to use this task to learn useful representations.

---
class: left

# Example
```
[“What is my purpose?”, “You pass the butter.”]
```
.center[
![:scale 50%](images/context_window.png)
]

Using context windows of size 1 (in practice 5 or 10):

???
- Here's an example of what this task looks like. Let's say we have a
corpus of two documents, "what is my purpose" and "you pass the
butter".
- We specify a context window size, let's say one. Then, for each word
that has enough context, we generate a training example.
- So for "is", we have a context of "what" and "my", for "my" we have
"is" and "purpose", and so on. So the input would be "is" and the goal
would be to predict that the word before is "what" and the word after
is "my". that assigns a high probability to the outputs "what" and
"my".
- In practice larger context windows are used, often 5-10 words in
either direction. Cearly it's impossible to actually solve this task,
because each word can appear in many different contexts. The goal is
to learn which contexts are likely, and which are not.

---

.center[
![:scale 50%](images/cbow_skipgram_1.png)
]

???

- The architecture we'll be using
- Kind of like a neural network, except linear
- Input is 1 hot encoded, so only 1 input will be hot
- Multiply by a weight matrix gives you hidden layer
- This hidden layer is trying to build a representation of the word!
- From there we have multiple outputs that correspond to the context
  words (again 1-hot encoded)
- V is very large! But N may be much smaller
- each W' is really just a linear classifier (with softmax activation)
- Making this into 2 matrix multiplications creates a bottleneck,
  forces a compressed representation of the input
- Each row of W corresponds to how to embed a particular word into this new space
- So then you give it all the examples and try to find the W, W' that
  minimize the error (e.g., log loss or similar)
- But then all you really care about is this W vector, which provides
  the word embeddings

---

# Softmax Training
.center[
![:scale 70%](images/softmax.png)
]


.smallest[
<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et. al. - Distributed Representations of Words and Phrases and their Compositionality (2013)</a>
]

???
- Probability of context word given input word
- Bottom sum is summing over many things
- Usually is it approximated by just sampling a smaller # words
- So we will solve for v and v', but notice after we're doing we are throwing away v'!

---
# CBOW vs Skip-gram

.center[
![:scale 70%](images/cbow_skipgram.png)
]

.smallest[
Efficient Estimation of Word Representations in Vector Space https://arxiv.org/pdf/1301.3781.pdf
]
???

- This is the skip-gram model, here you can see it on the right
- So we start with the one-hot-encoding of the word, and we have a
matrix multiplication into a latent space. A matrix multiplication
with a one-hot vector just means that each row in that embedding
matrix represents one word.
- From this latent space, we run logistic regression, trying to
predict what the word is for each particular offset, so here in this
picture we're trying to predict the word two to the left, one to the
left, one to the right, and two to the right. So these are basically
four classifiers running on the same input.
- Another approach is shown on the left, the continuous bag of words
model, or CBOW.
- This is something that seems a bit more natural to me, where given
the context, we are trying to predict the word in the center.
- So here we would be given four context words and try to figure out
what's the most likely word to be in between those.
- Should be able to handle synonyms
- But it turns out skim-grams seem to work better.

---
# Implementations

- Gensim
- Word2vec
- Tensorflow
- Don't train yourself

???
- Actually you could train yourself with TensorFlow, since that will use GPU
- Mostly just download a model that someone else made
- We'll look at Gensim
---


# Gensim - topic models for humans

- Multiple Latent Dirichlet Allocation implementations
- Wrappers for Mallet and vopal wabbit
- Tools for analyzing topic models
- No supervised learning
- Uses list-of-tuples instead of sparse matrices to
store documents.
???
- A python library specific for topic models
- all about document retrieval and feature extraction
- Different data representation than numpy/sklearn!
- Make sure to convert to use with sklearn
---
# Introduction to gensim

.smallest[
```python
docs = ["What is my purpose", "You bring butter"]
texts = [[token for token in doc.lower().split()] for doc in docs]
print(texts)
```
```
[['what', 'is', 'my', 'purpose'], ['you', 'bring', 'butter']]
```
```python
from gensim import corpora
dictionary = corpora.Dictionary(texts)
print(dictionary)
```
```
Dictionary(7 unique tokens: ['butter', 'you', 'is', 'purpose', 'my']...)
```
```python
new_doc = "what butter"
dictionary.doc2bow(new_doc.lower().split())
```
```
[(3, 1), (5, 1)]
```
```python
corpus = [dictionary.doc2bow(text) for text in texts]
corpus
```
```
[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1)]]
```
]

???
- Doesn't do much tokenization (doesn't handle punctuation!), so I do a basic one here
- Use NLTK or spacy or a regexp for tokenization in real applications. 
- In gensim, a document is represented as list of tuples (index, frequency), 
corpus is a list of these.

---

# Converting to/from sparse matrix

.smallest[
```python
import gensim
corpus
```
```
[[(0, 1), (1, 1), (2, 1), (3, 1)], [(4, 1), (5, 1), (6, 1)]]
```
```python
gensim.matutils.corpus2csc(corpus)
```
```
<7x2 sparse matrix of type '<class 'numpy.float64'>'
  with 7 stored elements in Compressed Sparse Column format>
```
```python
X = CountVectorizer().fit_transform(docs)
X
```
```python
sparse_corpus = gensim.matutils.Sparse2Corpus(X.T)
print(sparse_corpus)
print(list(sparse_corpus))
```
```
<gensim.matutils.Sparse2Corpus object at 0x7fd6d776b438>
[[(4, 1), (3, 1), (2, 1), (5, 1)], [(1, 1), (0, 1), (6, 1)]]
```
]

???
- most transformations is gensim are lazy:
- They yield a generator (like Sparse2Corpus) which can be converted to a list.
- Note that the CSC is transposed to what we would expect in sklearn.
- Compressed Sparse Column
- Also go the other way to first vectorize then use this
- Note use of X.T to do transpose
---
# Corpus Transformations

.smaller[
```python
tfidf = gensim.models.TfidfModel(corpus)
tfidf[corpus[0]]
```
```
[(0, 0.5), (1, 0.5), (2, 0.5), (3, 0.5)]
```
```python
print(tfidf[corpus])
print(list(tfidf[corpus]))
```
```
<gensim.interfaces.TransformedCorpus object at 0x7fd6d776b2b0>
[[(0, 0.5), (1, 0.5), (2, 0.5), (3, 0.5)], [(4, 0.577), (5, 0.577), (6, 0.577)]]
```
]

???
- No fit/transform
- Build on corpus: TfidfModel is basically fit
- fit is basically square brackets
- again lazy with generators, doesn't actually do it until it needs to
---

# Word2Vec in Gensim

```python
from gensim import models
w = models.KeyedVectors.load_word2vec_format(
    '../GoogleNews-vectors-negative300.bin', binary=True)
w['queen'].shape
```
```
(300,)
```
```python
w.vectors.shape
```
```
(3000000, 300)
```
???
- Google provides a model trained on google news
- 300 dimensions here
- Many others prebuilt models you can use
- Nice for us to avoid long training, nice for companies b/c they don't need to provide the corpus
- Vectors gives info on the vocab; here we have 3M words
- Doesn't work well if we have lots of words not in the vocab, but 3M is big enough
- To think about whether this representation is good, we need a way to measure similarity
---
# Cosine Similarity
.padding-top[

`$$\Large \text{similarity}(v, w)=\cos(\theta) = \frac{v^Tw}{\lVert v\rVert \lVert w \rVert}$$`
]
???
- Common way to measure similarity, related to euclidean distance
- Normlizes away the length
- Cosine of the angle between two vectors
---
class: smaller
# Inspecting Semantics
```python
w.most_similar(positive=['movie'], topn=5)
```

```
[('film', 0.8676770925521851),
 ('movies', 0.8013108968734741),
 ('films', 0.7363011837005615),
 ('moive', 0.6830361485481262),
 ('Movie', 0.6693680286407471)]
```
???
- Start with 'movie'. Find 5 most similar to movie.
- Computes distance to all 3M other words!
- Google had some reason to include capitalization, don't know why
--
```python
w.most_similar(positive=['good'], topn=5)
```
--
```
[('great', 0.7291510105133057),
 ('bad', 0.7190051078796387),
 ('terrific', 0.6889115571975708),
 ('decent', 0.6837348341941833),
 ('nice', 0.6836092472076416)]
 ```
???
- good AND bad? B/c they often appear in similar context. Can replace good with bad and sentence would still make sense, just changes meaning.
--
```python
w.most_similar(positive=['cute', 'dog'], topn=5)
```
```
[('puppy', 0.7645270228385925),
 ('chihuahua', 0.7209305763244629),
 ('adorable_puppy', 0.7108923196792603),
 ('yorkie', 0.7013816833496094),
 ('Shitzu', 0.7004074454307556)]
 ```
???
- Take average of multiple vectors then find closest
---

# Prepare document (IMDB)

.smallest[
```python
print(text_train_sub[0].decode())
```
```
Maybe it's just because I have an intense fear of hospitals and medical stuff, but this one got under my skin
(pardon the pun). This piece is brave, not afraid to go over the top and as satisfying as they come in terms
of revenge movies. Not only did I find myself feeling lots of hatred for the screwer and lots of sympathy 
towards the "screwee", I felt myself cringe and feel pangs of disgust at certain junctures which is really a
rare and delightful thing for a somewhat jaded horror viewer like myself. Some parts are very reminiscant of
"Hellraiser", but come off as tribute rather than imitation. It's a heavy handed piece that does not offer 
the viewer much to consider, but I enjoy being assaulted by a film once and awhile. This piece brings it and 
doesn't appologize.
I liked this one a lot. Do NOT watch whilst eating pudding.
```

```python
vect_w2v = CountVectorizer(vocabulary=w.index2word)
vect_w2v.fit(text_train_sub)
docs = vect_w2v.inverse_transform(vect_w2v.transform(text_train_sub))
docs[0]
```
```
array(['in', 'for', 'that', 'is', 'the', 'at', 'not', 'as', 'it', 'by',
       'are', 'have', 'an', 'this', 'they', 'but', 'one', 'which', 'do',
       'than', 'over', 'just', 'some', 'like', 'only', 'did', 'because',
       'off', 'being', 'my', 'very', 'much', 'go', 'under', 'does', 'got',
       'top', 'come', 'really', 'lot', 'find', 'thing', 'once', 'offer',
       'feel', 'film', 'medical', 'terms', 'rather', 'certain', 'felt',
       'consider', 'watch', 'parts', 'heavy', 'towards', 'enjoy',
       'feeling', 'maybe', 'piece', 'fear', 'myself', 'stuff', 'handed',
       'brings', 'movies', 'hospitals', 'rare', 'lots', 'skin', 'eating',
       'intense', 'somewhat', 'liked', 'afraid', 'tribute', 'horror',
       'revenge', 'brave', 'whilst', 'sympathy', 'assaulted', 'satisfying',
       'hatred', 'viewer', 'awhile', 'pardon', 'delightful', 'disgust',
       'imitation', 'pudding', 'cringe', 'jaded', 'pun', 'pangs', 'doesn',
       'junctures', 'hellraiser', 'appologize'], 
      dtype='<U98')
```
]

???
- This is a silly way to tokenize the input and using only
words that appear in the vocabulary used in the pretrained
model.
- The point is just to get back this list of tokens where each token appears in the model.
- There are other ways of doing this, like a list comprehensions.
---
 
# Represent doc by average

.smaller[
```python
X_train = np.vstack([np.mean(w[doc], axis=0) for doc in docs])
X_train.shape
```
```
(18750, 300)
```
```python
docs_val = vect_w2v.inverse_transform(vect_w2v.transform(text_val))
X_val = np.vstack([np.mean(w[doc], axis=0) for doc in docs_val])
```
```python
lr_w2v = LogisticRegression(C=100).fit(X_train, y_train_sub)
lr_w2v.score(X_train, y_train_sub)
```
```
0.867
```
```python
lr_w2v.score(X_val, y_val)
```
```
0.857
```

]

???
- Each word is now a 300 dim. vector
- Each review is the mean of the 300 dim. vectors in the review
- Hence each doc is now a 300 dim. vector
- Slightly worse than BOW but pretty close

---
# Analogues and Relationships

.center[
![:scale 100%](images/king_queen.png)
]


.smaller[
Answer “King is to Kings as Queen is to ?”:<br/>
Find closest vector to vec(“Queen”) + (vec(“Kings”) - vec(“King”))
]

.smallest[
<a href="http://www.aclweb.org/anthology/N13-1090">Mikolov et. al. Linguistic Regularities in Continuous Space Word Representations (2013)</a>]
???
- Can actually do some kind of algebra and find relationships
- distance man and woman same as dist. between king and queen
- dist. between king->kings same as queen->queens
- Can automatically find relationships, plurals, etc.

---
.center[
![:scale 80%](images/analogues.png)
]


.smallest[
<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et. al. - Distributed Representations of Words and Phrases and their Compositionality (2013)</a>]
???
- This uses 300 dim. space, but then uses PCA to project down into 2 dim.
- Distance between counties and capitals is basically the same! And parallel!
- Amazing that this discovered this in an unsupervised way
---
# Finding Relations
<br />
$$\large a : b : : c : ?$$
<br />
$$\large d = \text{arg}\max_i\frac{\left(\text{vec}(b) - \text{vec}(a) + \text{vec}(c)\right)^T vec_i}{\lVert\text{vec}(b) - \text{vec}(a) + \text{vec}(c)\rVert\lVert \text{vec}_i\rVert}$$

???
- Look for word i for which vector i has biggest cosine product
to word b - a + c
- i.e., get vector between a and b, add it to c
---
class: middle

.center[
![:scale 70%](images/table_cities_states.png)
]

.smallest[<a href="https://cs224d.stanford.edu/lecture_notes/notes2.pdf">Stanford CS 224D: Deep Learning for NLP</a>]
???
- Results for trying to find the state for each city
- Model is able to figure out this city/state relationship
- We never asked for this! Amazing result about 5 years ago.
- Some Examples:
- Q: woman+king-man A: =queen
- Q: woman+he-man A: she
- Q: Germany+pizza-Italy A: Bratwurst
---

# Examples with Gensim

.smaller[
```python
w.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)
```
```
[('queen', 0.7118192911148071),
 ('monarch', 0.6189674139022827),
 ('princess', 0.5902431607246399)]
```
```python
w.most_similar(positive=['woman', 'he'], negative=['man'], topn=3)
```
```
[('she', 0.8492251634597778),
 ('She', 0.6329933404922485),
 ('her', 0.6029669046401978)]
```
```python
w.most_similar(positive=['Germany', 'pizza'], negative=['Italy'], topn=3)
```
```
[('bratwurst', 0.5436394810676575),
 ('Domino_pizza', 0.5133179426193237),
 ('donuts', 0.5121968984603882)]
```

]

???
- Some examples
- Domino pizza?
- Note: results depend on the corpus that was used to train the word embedding model.

---
class: center

<br>
.center[
![:scale 70%](images/paper_title.png)
]

<br>

<br>

.smaller[
$\overrightarrow{man} - \overrightarrow{woman} \approx \overrightarrow{king} - \overrightarrow{queen}$

<br>
<br>

$\overrightarrow{man} - \overrightarrow{woman} \approx \overrightarrow{computer programmer} - \overrightarrow{homemaker}$


]

???
- This relationship was actually found, which we might not like
- Try to get rid of bias, gender stereotypes, etc.

---
# Going along he-she direction:

<br>
.center[
![:scale 90%](images/he_she.png)
]

???
- Some are appropriate, some not, kind of historical stereotypes
- Basically learns biases that humans have had

---
class: middle

# Paragaph Vectors
???
- Slightly smarter way of embedding documents
---

# Doc2Vec

<br>
.center[
![:scale 70%](images/doc2vec.png)
]
.smallest[
<a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Le, Mikolov: Distributed Representations of Sentences and Documents (2014)</a>
]
???
- Most common approach, 2014
- based on cbow, not skip-grams. Trying to predict word based on context
- Add a vector for each paragraph / document, also randomly initialized.
- Like adding a word unique to this paragraph
- Learn the embedding for each paragraph; gets some notion of context
- Not super intuitive but actually works well
-To infer for new paragraph: keep weights fixed, do
stochastic gradient descent on the representation D,
sampling context examples from this paragraph.

- There is no gradient descent at encoding time for word2vec!


---
# Doc2Vec with gensim

.smaller[
```python
def read_corpus(text, tokens_only=False):
    for i, line in enumerate(text):
        if tokens_only:
            yield gensim.utils.simple_preprocess(line)
        else:
            # For training data, add tags
            yield gensim.models.doc2vec.TaggedDocument(
                gensim.utils.simple_preprocess(line), [i])

train_corpus = list(read_corpus(text_train_sub))
test_corpus = list(read_corpus(text_val, tokens_only=True))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2)
model.build_vocab(train_corpus)

model.train(train_corpus, total_examples=model.corpus_count, epochs=55)
```
]


.smallest[
https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb
]
???
- Actually I think there are ways to do this built-in to the libraries now
- yield makes a generator
- add tags so they are numbered
- actually each review becomes a "paragraph" here
---
# Validation of Word Vectors
```python
model.wv.most_similar("movie")
```
```
[('film', 0.9482318758964539),
 ('flick', 0.822228193283081),
 ('series', 0.715380072593689),
 ('programme', 0.7032747268676758),
 ('sequel', 0.6939107179641724),
 ('story', 0.6771408319473267),
 ('show', 0.6559576392173767),
 ('documentary', 0.6537493467330933),
 ('picture', 0.6427854299545288),
 ('thriller', 0.6300673484802246)]
 ```
???
- Got pretty good similar word embeddings (though easy example)
---
# Encoding using doc2vec

.smaller[
```python
vectors = [model.infer_vector(train_corpus[doc_id].words)
          for doc_id in range(len(train_corpus))]    

X_train = np.vstack(vectors)

X_train.shape
```
```
(18759, 50)
```
```python
test_vectors = [model.infer_vector(test_corpus[doc_id])
                for doc_id in range(len(test_corpus))]   
X_test = np.vstack(test_vectors)
```

]
???
- Do this for each review now
- Run logreg to do classification
---

```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=100).fit(X_train, y_train_sub)

lr.score(X_train, y_train_sub)
```
```
0.817
```
```python
lr.score(X_val, y_val)
```
```
0.803
```

???
- Not working well here. Either not enough training data,
sgd didn’t converge yet, or too low dimension.
- In the paper: they actually get 0.926% with 400 dimensions (And using unsupervised data)
- Same dataset
- Though they also trained a Neural Network, not logreg
---


# GloVe: Global Vectors for Word Representation


$X_{ij}$ = How often does work j appear in context of word i

`$$ J = \sum\limits_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - \log X_{ij})^2$$`


`$$ 
f(x) = 
\begin{cases}
  (x/x_\text{max})^\alpha & \text{if  } x < x_\text{max}  \\
  1 & \text{otherwise} \\
\end{cases} 
$$`

.smaller[
https://nlp.stanford.edu/projects/glove/
]


???
- Previously inspired by neural networks (but linear)
- Slightly different approach to word vectors
- Here X_ij is the co-ocurrence count matrix, counting
how often word i and j appear in the same context –-
say a 5 word window.
- We are learning a factorization of log(X_ij) into w_i and
~w_j, where w_i are the word-vectors we want to
extract.
- b_i and ~b_j are biases for each word
- Very infrequent word-pairs are less important to get
right, and are down-weighted using f(X_ij).
- alpha is 3/4 empirically. x_max is 100.
- Requires computing the big matrix up front, then gradient descent
---
class: center, spacious
# GloVe  Weighting function f

![:scale 90%](images/glove_weighting.png)
???
- All words that appear at least 100 times (when x_max=100) get weight 1
- All less than that get some smaller weight
---
# Word analogies

.center[
![:scale 45%](images/word_analogies.png)
]

???
- Comparison of CBOW, SGD, skip-gram and Glove on
the semantic (ie. state→capital) and syntactic (ie
singular→ plural or propositions) word analogy tasks

---
class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
	remark.macros['scale'] = function (percentage) {
		//var url = this;
		var base = "https://amueller.github.io/COMS4995-s18/slides/aml-21-041118-word-embeddings/";
		var url = base + this;
		return '<img src="' + url + '" style="width: ' + percentage + '" />';
	};
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
