<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-org5fe4eb9">
<h2 id="org5fe4eb9">Support Vector Machines</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
You're going to spend a long time just <b>looking</b> at the data for
hwk3. There's a lot of data there. Look at the columns, drop them if
they have a ton of missing values, decide which ones are
reasonable. But don't drop too many. You can to spend some time
looking at them, probably like between 25-50. Draw some plots, look at
their distributions. You might want to start with just the number
features, and even assume that integers are continuous (rather than
discrete). Speaking of which, it's fine to make assumptions like that
as long as you make a note of it. Then later you can go back and
either treat some int features as discrete or also look at the
categorial variables.
</p>

<p>
Also see sklearn.feature_selection.VarianceThreshold, removes all features with a low variance
</p>

</aside>
</section>
<section id="slide-orgcba625e">
<h3 id="orgcba625e">Motivation</h3>
<ul>
<li>Go from linear models to more powerful nonlinear ones.</li>
<li>Keep convexity (ease of optimization).</li>
<li>Generalize the concept of feature engineering.</li>

</ul>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgd2294b3">
<h3 id="orgd2294b3">Reminder on Linear SVM</h3>
<p>
\[ \min_{w \in \mathbb{R}^p} C \sum_{i=1}^n \max(0, 1 - y_i w^T\mathbf{x}) + ||w||^2_2 \]
\[ \hat{y} = \text{sign}(w^T \mathbf{x})  \]
</p>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgea5c088">
<h3 id="orgea5c088">Max-Margin and Support Vectors</h3>

<div class="figure">
<p><img src="./assets/max_margin.png" alt="max_margin.png" height="500px" />
</p>
</div>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orga42b737">
<h3 id="orga42b737">Max-Margin and Support Vectors</h3>
<p>
\[ \min_{w \in \mathbb{R}^p} C \sum_{i=1}^n \max(0, 1 - y_i w^T\mathbf{x}) + ||w||^2_2 \]
\[\text{Within margin} \Leftrightarrow y_iw^T x &lt; 1\]
Smaller \(w \Rightarrow\) larger margin
</p>
<aside class="notes">
<p>
Just another way of saying that \(w^T x < 1\) and \(> -1\)
</p>

</aside>
</section>
<section id="slide-org9e68f6e">
<h3 id="org9e68f6e">Max-Margin and Support Vectors</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/max_margin_C_0.1.png" alt="max_margin_C_0.1.png" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/max_margin_C_1.png" alt="max_margin_C_1.png" />
</p>
</div>
</div>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgf3f4d02">
<h3 id="orgf3f4d02">Reformulate Linear Models</h3>
<ul>
<li>Optimization Theory</li>

</ul>

<p>
\[ w = \sum_{i=1}^n \alpha_i \mathbf{x}_i \]
</p>

<p>
(alpha are dual coefficients. Non-zero for support vectors only)
</p>

<p>
\[ \hat{y} = \text{sign}(w^T \mathbf{x})  \Longrightarrow   \hat{y} = \text{sign}\left(\sum_i^{n}\alpha_i (\mathbf{x}_i^T  \mathbf{x})\right) \]
</p>

<p>
\[ \alpha_i \le C\]
</p>
<aside class="notes">
<p>
Remember last time I talked about expanding to n features and we could learn anything. Turns out we can approximate that by using kernel functions. First:
</p>
<ul>
<li>Can formulate learning as optimization over alpha, only involves x
via dot-products</li>
<li>Regularization parameter C is limit on alphas!</li>

</ul>

</aside>
</section>
</section>
<section>
<section id="slide-org789fc90">
<h2 id="org789fc90">Kernels</h2>
<div class="outline-text-2" id="text-org789fc90">
</div>
</section>
<section id="slide-orgb2ea2b5">
<h3 id="orgb2ea2b5">Introducing Kernels</h3>
<p>
\[\hat{y} = \text{sign}\left(\sum_i^{n}\alpha_i (\mathbf{x}_i^T  \mathbf{x'})\right) \longrightarrow \\
\hat{y} = \text{sign}\left(\sum_i^{n}\alpha_i (\phi(\mathbf{x}_i)^T  \phi(\mathbf{x'}))\right) \]
</p>

<p>
\[ \phi(\mathbf{x}_i)^T \phi( \mathbf{x'}) \longrightarrow k(\mathbf{x}_i,  \mathbf{x'}) \]
</p>

<aside class="notes">
<ul>
<li>Dimensionality of \(\phi\) “doesn’t matter”. We only ever need to compute dot-product!</li>
<li>Can now design k instead of \(\phi\) – which might be easier, more flexible!</li>
<li>Can be done for any linear model – not only SVM! (svm has some alpha
zero). Kernel Logistic Regression, Kernel PCA, Kernel Kmeans….</li>
<li>k positive definite, symmetric \(\Rightarrow\) there exists a \(\phi\)! (possilby $&infin;$-dim)</li>

</ul>

</aside>
</section>
<section id="slide-orgb8db098">
<h3 id="orgb8db098">Examples of Kernels</h3>
<p>
\[k_\text{linear}(\mathbf{x}, \mathbf{x}') = \mathbf{x}^T\mathbf{x}'\]
\[k_\text{poly}(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^T\mathbf{x}' + c) ^ d\]
\[k_\text{rbf}(\mathbf{x}, \mathbf{x}') = \exp(\gamma||\mathbf{x} -\mathbf{x}'||^2)\]
\[k_\text{sigmoid}(\mathbf{x}, \mathbf{x}') = \tanh\left(\gamma \mathbf{x}^T\mathbf{x}'  + r\right)\]
\[k_\cap(\mathbf{x}, \mathbf{x}')= \sum_{i=1}^p \min(x_i, x'_i)\]
</p>

<ul>
<li>If \(k\) and \(k'\) are kernels, so are $k + k', kk', ck', &#x2026;$</li>

</ul>

<aside class="notes">
<p>

</p>

</aside>
</section>
<section id="slide-orgdf7e208">
<h3 id="orgdf7e208">Polynomial Kernel vs Features</h3>
<p>
\[ k_\text{poly}(\mathbf{x}, \mathbf{x}') = (\mathbf{x}^T\mathbf{x}' + c) ^ d \]
</p>

<ul>
<li>Primal vs Dual Optimization</li>
<li>Explicit polynomials \(\rightarrow\) compute on n_samples * n_features ** d</li>
<li>Kernel trick \(\rightarrow\) compute on kernel matrix of shape n_samples * n_samples</li>

<li>For a single feature:</li>

</ul>
<p>
\[ (x^2, \sqrt{2}x, 1)^T (x'^2, \sqrt{2}x', 1) = x^2x'^2 + 2xx' + 1 = (xx' + 1)^2 \]
</p>
<aside class="notes">
<p>
SVM with poly kernel similar (but not exactly same) to linear SVM with
polynomial features!
</p>

</aside>
</section>
<section id="slide-org9823ee1">
<h3 id="org9823ee1">Poly kernels with <code>sklearn</code></h3>
<div class="org-src-container">

<pre><code class="python" >poly = PolynomialFeatures(include_bias=False)
X_poly = poly.fit_transform(X)
print(X.shape, X_poly.shape)
print(poly.get_feature_names())
# ((100, 2), (100, 5))
# ['x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/poly_kernel_features.png" alt="poly_kernel_features.png" />
</p>
</div>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-org8f53984">
<h3 id="org8f53984">Understanding Dual Coefficients</h3>
<div class="org-src-container">

<pre><code class="python" >linear_svm.coef_
#array([[0.139, 0.06, -0.201, 0.048, 0.019]])
</code></pre>
</div>
<p>
\[ y = \text{sign}(0.139 x_0 + 0.06 x_1 - 0.201 x_0^2 + 0.048 x_0 x_1 + 0.019 x_1^2) \]
</p>

<div class="org-src-container">

<pre><code class="python" >linear_svm.dual_coef_
#array([[-0.03, -0.003, 0.003, 0.03]])
linear_svm.support_
#array([1,26,42,62], dtype=int32)
</code></pre>
</div>
<p>
\[ y = \text{sign}(-0.03 \phi(\mathbf{x}_1)^T \phi(x) - 0.003 \phi(\mathbf{x}_{26})^T \phi(\mathbf{x})  \\ +0.003 \phi(\mathbf{x}_{42})^T \phi(\mathbf{x}) + 0.03 \phi(\mathbf{x}_{62})^T \phi(\mathbf{x})) \]
</p>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgc4df659">
<h3 id="orgc4df659">With Kernel</h3>
<p>
\[y = \text{sign}\left(\sum_i^{n}\alpha_i k(\mathbf{x}_i,  \mathbf{x})\right) \]
</p>
<div class="org-src-container">

<pre><code class="python" >poly_svm.dual_coef_
# array([[-0.057, -0. , -0.012, 0.008, 0.062]])
poly_svm.support_
# array([1,26,41,42,62], dtype=int32)
</code></pre>
</div>
<p>
\[ y = \text{sign}(-0.057 (\mathbf{x}_1^T\mathbf{x} + 1)^2
-0.012 (\mathbf{x}_{41}^T \mathbf{x} + 1)^2 \\
+0.008 (\mathbf{x}_{42}^T \mathbf{x} + 1)^2 + 0.062 (\mathbf{x}_{62}^T \mathbf{x} + 1)^2
\]
</p>
<aside class="notes">
<p>
y = sign(-0.03 np.inner(poly(X[1]), poly(x)) – 0.003 np.inner(poly(X[26]), poly(x)) +0.003 np.inner(poly(X[42]), poly(x)) + 0.03 np.inner(poly(X[63]), poly(x))
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org88d40c8">
<h2 id="org88d40c8">Practical Considerations</h2>
<div class="outline-text-2" id="text-org88d40c8">
</div>
</section>
<section id="slide-orga5f6d86">
<h3 id="orga5f6d86">Runtime Considerations</h3>

<div class="figure">
<p><img src="./assets/kernel_runtime.png" alt="kernel_runtime.png" />
</p>
</div>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgb8f1703">
<h3 id="orgb8f1703">Kernels in Practice</h3>
<ul>
<li>Dual coefficients less interpretable</li>
<li>Long runtime for "large" datasets (100k samples)</li>
<li>Real power in infinite-dimensional spaces: rbf!</li>
<li>Rbf is “universal kernel” - can learn (aka overfit) anything.</li>

</ul>
<aside class="notes">
<p>

</p>

</aside>
</section>
<section id="slide-org9bb7179">
<h3 id="org9bb7179">Preprocessing</h3>
<ul>
<li>Kernel use inner products or distances.</li>
<li>StandardScaler or MinMaxScaler ftw</li>
<li>Gamma parameter in RBF directly relates to scaling of data – default only works with zero-mean, unit variance.</li>

</ul>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgca5711e">
<h3 id="orgca5711e">Parameters for RBF Kernels</h3>
<ul>
<li>Regularization parameter C is limit on alphas (for any kernel)</li>
<li>Gamma is bandwidth: \(k_\text{rbf}(\mathbf{x}, \mathbf{x}') = \exp(\gamma||\mathbf{x}-\mathbf{x}'||^2)\)</li>

</ul>


<div class="figure">
<p><img src="./assets/rbf_gamma.png" alt="rbf_gamma.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
C param limits influence of any data point.
Smaller gamma -&gt; wider kernel -&gt; simpler model -&gt; smoother boundary
Larger gamma -&gt; narrow kernel -&gt; more local influence -&gt; more like NN and more complex
</p>

</aside>
</section>
<section id="slide-org0891d49">
<h3 id="org0891d49"></h3>

<div class="figure">
<p><img src="./assets/svm_grid.png" alt="svm_grid.png" />
</p>
</div>
<aside class="notes">
<p>
Illustrating the two params. Vertical = C, horizontal = gamma.
Support vectors circled.
Simplest has smallest gamma and C.
</p>

<p>
There's a tradeoff between the two; multiple combos that will give you
similarly good results.
Contours are the results before you take the "sign" to actually make a decision
</p>

</aside>
</section>
<section id="slide-org9fde0b1">
<h3 id="org9fde0b1"></h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.datasets import load_digits
digits = load_digits()
</code></pre>
</div>

<div class="figure">
<p><img src="./assets/digits.png" alt="digits.png" height="450px" />
</p>
</div>
<aside class="notes">
<p>
Slightly more interesting dataset.
</p>

</aside>
</section>
<section id="slide-org5448e86">
<h3 id="org5448e86">Scaling and Default Params</h3>
<pre class="example">
gamma : float, optional (default = "auto")
  Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.
  If gamma is 'auto' then 1/n_features will be used
</pre>
<div class="org-src-container">

<pre><code class="python" >scaled_svc = make_pipeline(StandardScaler(), SVC())
print(np.mean(cross_val_score(SVC(), X_train, y_train, cv=10)))
print(np.mean(cross_val_score(scaled_svc, \
                              X_train, y_train, cv=10)))
# 0.578
# 0.978
</code></pre>
</div>



<div class="org-src-container">

<pre><code class="python" >gamma = (1. / (X_train.shape[1] * X_train.std()))
print(np.mean(cross_val_score(SVC(gamma=gamma), \
                              X_train, y_train, cv=10)))
# 0.987
</code></pre>
</div>

<aside class="notes">
<p>
Default gamma is 1/ n_features, works reasonably well when feature have std 1. (otherwise doesn't really make sense))
If features have similar std, we can also just rescale gamma by it, instead of the data.
Features having the same scale is a bit unusual though, this is a very particular dataset.
Q: How to do grid search for these params? (C and gamma))
</p>

</aside>
</section>
<section id="slide-orgeaaf47e">
<h3 id="orgeaaf47e">Grid-Searching Parameters</h3>
<div class="org-src-container">

<pre><code class="python" >param_grid = {'svc__C': np.logspace(-3, 2, 6),
              'svc__gamma': \
              np.logspace(-3, 2, 6) / X_train.shape[0]}
param_grid
</code></pre>
</div>
<pre class="example">
{'svc_C': array([   0.001,    0.01 ,    0.1  ,    1.   ,   10.   ,  100.   ]),
'svc_gamma': array([ 0.000001,  0.000007,  0.000074,  0.000742,  0.007424, 0.074239])}
</pre>
<div class="org-src-container">

<pre><code class="python" >grid = GridSearchCV(scaled_svc, param_grid=param_grid, cv=10)
grid.fit(X_train, y_train)
</code></pre>
</div>
<aside class="notes">
nil
</aside>
</section>
<section id="slide-orgf4df4ff">
<h3 id="orgf4df4ff">Grid-Searching Parameters</h3>

<div class="figure">
<p><img src="./assets/svm_grid_mat.png" alt="svm_grid_mat.png" />
</p>
</div>
<aside class="notes">
<p>
Chance performance is 10% accuracy, since there are 10 digits (random
guessing). With wrong parameters, I do no better than chance.  Here's
an example of where you might want to extend your search
space. Clearly you don't need smaller C's, but maybe try larger ones,
though as you increase C you will also increase running time.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org593bd2e">
<h2 id="org593bd2e">Regression</h2>
<div class="outline-text-2" id="text-org593bd2e">
</div>
</section>
<section id="slide-org495e645">
<h3 id="org495e645">Support Vector Regression</h3>

<div class="figure">
<p><img src="./assets/svr_math.png" alt="svr_math.png" />
</p>
</div>
<aside class="notes">
<p>
Another way to do robust (to outliers) regression. Often the way this
is formulated: Want ALL points to be within a tube of width
\(\epsilon\), kind of opposite of SVM for classification.  All points
within this tube don't contribute to the solution, only things
outside. \(\epsilon\) is a tunable parameter, typically you pick it by
domain knowledge. Don't worry about the math too much.
</p>

</aside>
</section>
<section id="slide-orgb7a5e1e">
<h3 id="orgb7a5e1e">Using SVR</h3>
<ul>
<li>Fix epsilon based on application/outliers</li>
<li>Linear kernel \(\to\) robust linear regression</li>
<li>Poly / rbf kernel \(\to\) robust non-linear regression</li>

</ul>


<div class="figure">
<p><img src="./assets/svr.png" alt="svr.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Can use kernels with this to. Intuition: you're paying most amount of
attention to things that are far away, although we're using the
absolute value actually, not squared, so it's not really that
sensitive to outliers.
In tube, no error, outside tube, linear error.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org0329ef9">
<h2 id="org0329ef9">Kernel Approximation</h2>
<div class="outline-text-2" id="text-org0329ef9">
</div>
</section>
<section id="slide-org6f0b140">
<h3 id="org6f0b140">Why undo the kernel trick?</h3>

<div class="figure">
<p><img src="./assets/lskm.png" alt="lskm.png" />
</p>
</div>
<aside class="notes">
<p>
Problem: for 100,000+ data points, these techniques just don't work!
</p>

</aside>
</section>
<section id="slide-org7e1e8ba">
<h3 id="org7e1e8ba">RKHS vs RKS</h3>
<p>
(Reproducing Kernel Hilbert-Spaces vs Random Kitchen Sinks)
</p>
<ul>
<li>Idea: ditch kernel, approximate (infinite-dimensional) feature map</li>

</ul>
<p>
\[\phi(x)^T \phi(x') = k(x, x') \approx \hat{\phi}(x)^T \hat{\phi}(x')\]
</p>
<ul>
<li>For rbf-kernel random projection followed by sin/cos , higher n_features is better</li>

</ul>

<aside class="notes">
<p>
Went full circle: create poly/other features \(\to\) more powerful
models Rather than feature space, use kernels Now go back to feature
expansions that approximate the kernel So instead for large data sets
we add features that expand to something about the same as the kernel
we want.
You can look at RKHS if you want, otherwise don't worry.
RKS: Draw a big random matrix, multiply with data, apply sin/cos.
Somehow this approximates rbf-kernel&#x2026;
</p>

</aside>
</section>
<section id="slide-org9827100">
<h3 id="org9827100">Kernel Approximation in <code>sklearn</code></h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.kernel_approximation import RBFSampler
gamma = 1. / (X.shape[1] * X.std())
approx_rbf = RBFSampler(gamma=gamma, n_components=5000)
print(X.shape)
X_rbf = approx_rbf.fit_transform(X)
print(X_rbf.shape)
# (1347, 64)
# (1347, 5000)
np.mean(cross_val_score(LinearSVC(), X, y, cv=10))
# 0.946
np.mean(cross_val_score(SVC(gamma=gamma), X, y, cv=10))
# 0.987
np.mean(cross_val_score(LinearSVC(), X_rbf, y, cv=10))
# 0.984
</code></pre>
</div>
<aside class="notes">
<p>
Need to specify gamma AND how many features I want. More features = better approximation but slower to compute.
Comparable results.
</p>

</aside>
</section>
<section id="slide-orgc2221df">
<h3 id="orgc2221df">Nyström Approximation</h3>
<ul>
<li>Use low-rank approximation of kernel matrix</li>
<li>Select some samples, compute kernel with only those, embed all the points.</li>
<li>Using all points = full rank = exact</li>
<li>For same number of components more expensive than RBFSampler, but needs less!</li>

</ul>
<div class="org-src-container">

<pre><code class="python" >from sklearn.kernel_approximation import Nystroem
nystroem = Nystroem(gamma=gamma, n_components=200)
X_train_ny = nystroem.fit_transform(X_train)
print(X_train_ny.shape)
# (1347, 200)
np.mean(cross_val_score(LinearSVC(), \
                        X_train_ny, y_train, cv=10))
# 0.974
</code></pre>
</div>
<aside class="notes">
<p>
Another strategy. The previous only works for RBF kernels. Idea is
subsample some of the points, compute the kernel have only on those
points. Then you can transform all your points into this 200
dimensional feature space.
</p>

</aside>
</section>
<section id="slide-org4177362">
<h3 id="org4177362">New techniques</h3>
<ul>
<li>Many newer / faster algorithms out there</li>
<li>Not in sklearn so far</li>
<li>FastFood one of the most prominent ones</li>
<li>Current research on selecting good points for Nystroem.</li>

</ul>
<aside class="notes">
<p>
FastFood is fairly recent, also how to select points from Nystroem.
</p>

<p>
We saw how adding features was useful, then it was convenient to use
kernel, then we went backwards. What we're really interested in is
creating the right features.
</p>

</aside>
</section>
<section id="slide-orgbaa4b92">
<h3 id="orgbaa4b92">Relation to Random Neural Nets</h3>
<ul>
<li>Why approximate kernels?</li>
<li>Just go random !</li>

</ul>
<div class="org-src-container">

<pre><code class="python" >rng = np.random.RandomState(0)
w = rng.normal(size=(X_train.shape[1], 100))
X_train_wat = np.tanh(scale(np.dot(X_train, w)))
print(X_train_wat.shape)
# (1347, 100)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >np.mean(cross_val_score(LinearSVC(), \
                        X_train_wat, y_train, cv=10))
# 0.966
</code></pre>
</div>

<aside class="notes">
<p>
Here's a technique: add random features! Like a random neural
network. Dot product of train data and random data, then tanh, this is
basically training a Neural Network (will look at later). Then train
LinearSVC on this projected data, and we have good performance!
</p>

<p>
What this is basically saying is that it doesn't really matter what
your projection is, as long as you blow up your feature space to high
dimensions. With all that freedom in higher dimensions, the randomness
effect is small and you can classify anything! Weird! Works well in practice!
</p>

</aside>
</section>
<section id="slide-orgb504e5d">
<h3 id="orgb504e5d">Extreme Learning Machine Hoax</h3>
<ul>
<li>AKA random neural networks</li>
<li>Same result published in the 90s</li>
<li>Bogus math</li>

</ul>
<aside class="notes">
<p>
Some papers published calling this "Extreme Learning". Don't use this
term. Not me, others famous in the field called them out.
</p>

</aside>
</section>
<section id="slide-orgabaa192">
<h3 id="orgabaa192">Kernel Approximation in Practice</h3>
<ul>
<li>SVM: only when n_samples \(<\) 100,000 but works for n_features large</li>
<li>RBFSampler, Nystroem can allow making anything kernelized!</li>
<li>Some kernels (like chi2 and intersection) have really fast
approximation.</li>

</ul>
<aside class="notes">
<p>
Others implemented in sklearn that can be approximated well,
especially for computer vision.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org140c4fe">
<h2 id="org140c4fe">Summary</h2>
<ul>
<li>Kernels are cool!</li>
<li>Kernels work best for "small" n_samples</li>
<li>Approximate kernels or random features for many samples</li>
<li>Could do even SGD / streaming with kernel approximations!</li>

</ul>
<aside class="notes">
<ul>
<li>Could use Logistic Regression (hurray probabilities)</li>

</ul>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
