<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\)

<section>
<section id="slide-orgb0dc669">
<h2 id="orgb0dc669">Linear Models for Regression</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
</section>
</section>
<section>
<section id="slide-orged7c373">
<h2 id="orged7c373">Computational Properties</h2>
<div class="outline-text-2" id="text-orged7c373">
</div>
</section>
<section id="slide-org858d276">
<h3 id="org858d276">Computational Properties: Centroids</h3>
<ul>
<li>fit time (for \(n\) data points): \(O(nP)\)</li>
<li>predict time (for 1 data point): \(O(CP)\)</li>
<li>memory: \(O(CP)\)</li>

</ul>

<p>
\(P = \text{# features}, C = \text{# classes}\)
</p>
<aside class="notes">
<p>
There are basically three: time to build the model, memory to store the model, and time to make a prediction. What are those? Fit time is simple, it’s computing the centroids which is n * p. Basically we need to look at each feature exactly once. Memory is storing the centroids, which is number of classes times number of features, and predict time is computing the distance to each of them, which is the same. When is this slow? Basically never - in high dim with only few important, maybe trees could be better, but we could also shrink those dimensions away.
</p>

</aside>
</section>
<section id="slide-org66a5be2">
<h3 id="org66a5be2">Computational Properties: Nearest Neighbors (Naive)</h3>
<ul>
<li>fit: no time</li>
<li>predict: \(O(nP)\)</li>
<li>memory: \(O(nP)\)</li>

</ul>
<aside class="notes">
<p>
Ok, now for the k nearest neighbors. How would you implement this? (brute force or neighbors structure) When not using a structure: fit no time, memory is the whole dataset, and prediction requires comparing against all data points. 
</p>

</aside>
</section>
<section id="slide-orgeaaa41c">
<h3 id="orgeaaa41c">Computational Properties: Nearest Neighbors (Kd-tree)</h3>
<ul>
<li>fit: \(O(Pn\log n)\)</li>
<li>predict: \(O(k\log n)\) for fixed \(P\)</li>
<li>memory: \(O(nP)\)</li>

</ul>
<aside class="notes">
<p>
If we create a kd-tree or ball tree? Fit takes O(n log n), memory is the same. Prediction might be faster in low dimensions.
</p>

<p>
What’s bad about this? memory scales with data, prediction speed scales with data :-/
</p>

<p>
Can have high complexity models though. When would you use one vs the other? Large datasets , high dimensions: use nearest centroid maybe.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org2c0a6c6">
<h2 id="org2c0a6c6">Linear Models for Regression</h2>
<div class="outline-text-2" id="text-org2c0a6c6">
</div>
</section>
<section id="slide-org662bc0e">
<h3 id="org662bc0e">Linear Models for Regression</h3>

<div class="figure">
<p><img src="./assets/linear_regression_1d.png" alt="linear_regression_1d.png" height="300px" />
</p>
</div>

<p>
\[ \hat{y} = w^T \vec{x} + b = \sum_{i=1}^p w_i x_i + b \]
</p>

<aside class="notes">
<p>
Predictions in all linear models for regression are of the form shown here: It's an inner product of the features with some coefficient or weight vector w, and some bias or intercept b. In other words, the output is a weighted sum of the inputs, possibly with a shift. here i runs over the features and x<sub>i</sub> is one feature of the data point x. These models are called linear models because they are linear in the parameters w. The way I wrote it down here they are also linear in the features x<sub>i</sub>. However, you can replace the features by any non-linear function of the inputs, and it'll still be a linear model.
</p>

<p>
There are many differnt linear models for regression, and they all share this formula for making predictions. The difference between them is in how they find w and b based on the training data.
</p>

</aside>
</section>
<section id="slide-org221c3c9">
<h3 id="org221c3c9">Ordinary Least Squares</h3>
<p>
\[ \hat{y} = w^T \vec{x} + b = \sum_{i=1}^p w_i x_i + b\]
\[ \min_{w \in \mathbb{R}^p} \sum_{i=1}^n \norm{w^T \vec{x}_i - y_i}^2 \]
</p>

<p>
Unique solution if \(\vec{X} = (\vec{x},\ldots,\vec{x}_n)^T\) has full column rank.
</p>
<aside class="notes">
<p>
The most straight-forward solution that goes back to Gauss is ordinary least squares. In ordinary least squares, find w and b such that the predictions on the training set are as accurate as possible according the the squared error. That intuitively makes sense: we want the predictions to be good on the training set. If there is more samples than features (and the samples span the whole feature space), then there is a unique solution. The problem is what's called a least squares problem, which is particularly easy to optimize and get the unique solution to.
</p>

<p>
However, if there are more features than samples, there are usually many perfect solutions that lead to 0 error on the training set. Then it's not clear which solution to pick. Even if there are more samples than features, if there are strong correlations among features the results might be unstable, and we'll see some examples of that soon.
</p>

<p>
Before we look at examples, I want to introduce a popular alternative.
</p>

</aside>
</section>
<section id="slide-org54da82c">
<h3 id="org54da82c">Ridge Regression</h3>
<p>
\[ \min_{w \in \mathbb{R}^p} \sum_{i=1}^p \norm{w^T \vec{x}_i - y_i}^2 + \alpha \ltwo{w} \]
</p>

<ul>
<li>Always has a unique solution</li>
<li>\(\alpha\) is tuning parameter.</li>

</ul>

<aside class="notes">
<p>
In Ridge regression we add another term to the optimization problem. Not only do we want to fit the training data well, we also want w to have a small squared l2 norm or squared euclidean norm. The idea here is that we're decreasing the "slope" along each of the feature by pushing the coefficients towards zero. This constraings the model to be more simple.
</p>

<p>
So there are two terms in this optimization problem, which is also called the objective function of the model: the data fitting term here that wants to be close to the training data according to the squared norm, and the prenalty or regularization term here that wants w to have small norm, and that doesn't depend on the data.
</p>

<p>
Usually these two goals are somewhat opposing. If we made w zero, the second term would be zero, but the predictions would be bad. So we need to trade off between these two. The trade off is problem specific and is specified by the user. If we set alpha to zero, we get linear regression, if we set alpha to infinity we get a constant model. Obviously usually we want something in between.
</p>

<p>
This is a very typical example of a general principle in machine learning, called regularized empirical risk minimization.
</p>

</aside>
</section>
<section id="slide-orgb958e6f">
<h3 id="orgb958e6f">Regularized Empirical Risk Minimization</h3>
<p>
\[ \min_{f \in F} \sum_{i=1}^n L(f(\vec{x}_i),y_i) + \alpha R(f) \]
</p>

<aside class="notes">
<p>
Many models in machine learning, like linear models, SVMs and neural networks follow the general framework of empirical risk minimization, which you can see here. We formulate the machine learning problem as an optimization problem over a family of functions. In our case that was the family of linear functions parametrized by w and b. The minimization problem consists of two parts, the data fitting part and the model complexity part. The data fitting part says that the predictions mad eby our functions should be accurate according to some loss L. For our regression problems that was the squared loss. The model complexity part says that we prefer simple models and penalizes complicated f. Most machine learning algorithms can be cast into this, with a particular choice of family of functions f, loss function L and regularizer R. And most of machine learning theory is build around this framework. People proof for differnt choices of F and L and R that if you minimize this, you'll be able to generalize well. And that makes intuitive sense. To do well on the test set, we definitely want to do reasonably well on the training set. We don't expect that we can do better on a test set than the training set. But we also want to minimize the performance difference between training and test set. If we restrict our model to be simple via the regularizer R, we have better chances of the model generalizing.
</p>

</aside>
</section>
<section id="slide-orgfd3ae97">
<h3 id="orgfd3ae97">Reminder on model complexity</h3>

<div class="figure">
<p><img src="./assets/overfitting_underfitting_cartoon_full.png" alt="overfitting_underfitting_cartoon_full.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
I hope this sounds familiar from what we talked about last time. This is a particular way of dealing with overfitting and underfitting. For this framework in general, or for ridge regression in particular, trading off the data fitting and the regularization changes the model complexity. If we set alpha high we restrict the model, and we will be on the left side of the graph. If we make alpha small, we allow the model to fit the data more, and we're on the right side of the graph.
</p>

</aside>
</section>
<section id="slide-orgd428695">
<h3 id="orgd428695">Boston Housing Dataset</h3>

<div class="figure">
<p><img src="./assets/boston_housing_scatter.png" alt="boston_housing_scatter.png" />
</p>
</div>
<aside class="notes">
<p>
Ok after all this pretty abstract talk, let's make this concrete. Let's do some regression on the boston housing dataset. After the last homework you're hopefully familiar with it. The idea is to predict prices of property in the boston area in different neighborhoods. This is a dataset from the 70s I think, so everything is pretty cheap. Most of the features you can see are continuous, with the exception of the charlston river variable which says whether the neighborhood is on the river.
</p>

<p>
Keep in mind that this data lives in a 13 dimensional space and these univariate plots only look at 13 different projections of the data, and can't capture any of the interactions.
</p>

<p>
But still we can see that the price clearly depends on some of these variables. It's also pretty clear that the dependency is non-linear for some of the variables. We'll still start with a linear model, because its a very simple class of models, and I'd always star approaching any model from the simplest baseline. In this case it's linear regression. We're having 506 samples and 13 features. We have much more samples than features. Linear regression should work just fine. Also it's a tiny dataset, so basically anything we'll try will run instantaneously, which is also good to keep in mind.
</p>

<p>
Another thing that you can see in this graph is that the features have
very different scales. Here's a box plot that shows that even more
clearly.
</p>

</aside>
</section>
<section id="slide-org5f9c3b2">
<h3 id="org5f9c3b2">Feature Scales</h3>
<div class="org-src-container">

<pre><code class="python" >plt.figure()
plt.boxplot(X)
plt.xticks(np.arange(1, X.shape[1] + 1), boston.feature_names, rotation=30, ha="right")
plt.ylabel("MEDV")
</code></pre>
</div>


<div class="figure">
<p><img src="assets/boxplot1.png" alt="boxplot1.png" height="350px" />
</p>
</div>

<aside class="notes">
<p>
That's something that will trip up the distance based models models we
talked about last time, as well as the linear models we're talking
about today. For the penalized models the different scales mean that
different features are penalized differently, which you usually want
to avoid. Usually there is no particular semantics attached to the
fact that one feature has different magnitutes than another. We could
measure something in inches instead of miles, and that would change
the outcome of the model. That's certainly not something we want. A
good idea is to scale the data to get rid of this effect. We'll talk
about that and other preprocessing methods in-depth on Wednesday next
week. Today I'm mostly gonna ignore this. But let's get started with
Linear Regression
</p>

</aside>
</section>
<section id="slide-orga52c5b7">
<h3 id="orga52c5b7">LR and Ridge in Scikit-Learn</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import LinearRegression, Ridge
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0)
print(np.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10)))
print(np.mean(cross_val_score(Ridge(), X_train, y_train, cv=10)))
</code></pre>
</div>

<pre class="example">
0.7172214319371177
0.715280214405421

</pre>

<aside class="notes">
<p>
Comparing the two in terms of cv score
Ridge uses default alpha of 1. The scores are&#x2026;
</p>

</aside>
</section>
<section id="slide-org3661769">
<h3 id="org3661769">Coefficient of determination</h3>
<p>
\[ R^2(y,\hat{y}) = 1 - \frac{\sum_{i=1}^{n-1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n-1} (y_i - \overline{y})^2 } \]
\[ \overline{y} = \frac1n \sum_{i=0}^{n-1} y_i \]
Can be negative for biased estimators - or for the test set!
</p>
<aside class="notes">
<p>
\(R^2\) Usually between 0 and 1. 
1 means perfect correlation, 0 means random prediction. How much of the data's variance is predicted by your model
How far is prediction from ground truth relative to mean.
How much better is our prediction that just predicting the mean
In this context it's not the square of anything (different definition)
Negative when you do a worse job at predicting that just guessing the mean.
(1) really biased model (too regularized/restricted)
(2) different test set vs. training set
negative really bad, 0 bad, 1 good
</p>

</aside>
</section>
<section id="slide-org958c878">
<h3 id="org958c878">Scaling</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.preprocessing import StandardScaler
X, y = boston.data, boston.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
ridge = Ridge().fit(X_train_scaled, y_train)
X_test_scaled = scaler.transform(X_test)
print(ridge.score(X_test_scaled, y_test))
</code></pre>
</div>

<pre class="example">
0.6344884687786743

</pre>
<aside class="notes">
<p>
If you want to scale the data, you can use StandardScaler for that. It makes the mean zero and the standard deviation one. It's an unsupervised model, so it only takes the data X<sub>train</sub> to fit. Fitting just means computing mean and standard devitation. Then, we can scale the data using the transform method. Make sure that you fit the data only on the training set, and then transform the training and the test set. If you want to use scaling inside cross-validation, it's a little bit more tricky. As I said, more details next wednesday. I'm just mentioning this here because it might come in handy for the homework.
</p>

<p>
Ok but so let's come back to the Ridge model. Above we just used the default setting for the parameter alpha, which is one. This is a reasonable default, but there is no reason why this should give us the optimum generalization performance on this problem. So it's a good idea to adjust the parameter. As we saw on Monday, we can easily do that with gridsearchCV.
</p>

</aside>
</section>
<section id="slide-orge720ccd">
<h3 id="orge720ccd">Tuning Ridge Regression</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.model_selection import GridSearchCV
param_grid = {'alpha': np.logspace(-3, 3, 13)}
print(param_grid)
grid = GridSearchCV(Ridge(), param_grid, cv=10)
grid.fit(X_train, y_train)
</code></pre>
</div>

<pre class="example">
{'alpha': array([1.00000000e-03, 3.16227766e-03, 1.00000000e-02, 3.16227766e-02,
       1.00000000e-01, 3.16227766e-01, 1.00000000e+00, 3.16227766e+00,
       1.00000000e+01, 3.16227766e+01, 1.00000000e+02, 3.16227766e+02,
       1.00000000e+03])}

</pre>

<aside class="notes">
<p>
So we want to find the best alpha, 1 probably isn't best.
Lots of uncertainty, training always better than test.
Regularization didn't seem to help!
</p>

<p>
We can modify the data so that regularization makes a difference.
</p>

</aside>
</section>
<section id="slide-orgf7981d8">
<h3 id="orgf7981d8">Tuning Ridge Regression</h3>

<div class="figure">
<p><img src="./assets/ridge_alpha_search.png" alt="ridge_alpha_search.png" height="400px" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-orgd7b64bc">
<h2 id="orgd7b64bc">Features and Coefficients</h2>
<div class="outline-text-2" id="text-orgd7b64bc">
</div>
</section>
<section id="slide-orgf3cc011">
<h3 id="orgf3cc011">Adding features</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.preprocessing import PolynomialFeatures, scale
poly = PolynomialFeatures(include_bias=False)
X_poly = poly.fit_transform(scale(X))
X_train, X_test, y_train, y_test = train_test_split(X_poly, y)
print(f'{np.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10)):.2f}')
print(f'{np.mean(cross_val_score(Ridge(), X_train, y_train, cv=10)):.2f}')
</code></pre>
</div>

<pre class="example">
0.75
0.79

</pre>
<aside class="notes">
<p>
So let's add degree 2 features - combine each with each (all pairs).
Much easier to overfit, but gives you more flexible.
Note that this is still consider linear, since it's linear in our coefficients!
So ridge is better here.
</p>

</aside>
</section>
<section id="slide-org2dbcc12">
<h3 id="org2dbcc12">Grid Search</h3>

<div class="figure">
<p><img src="./assets/ridge_alpha_search_poly.png" alt="ridge_alpha_search_poly.png" height="300px" />
</p>
</div>

<ul>
<li>Best \(\alpha\): 31.6</li>
<li>Best score: 0.83</li>

</ul>
<aside class="notes">
<p>
Regularization makes a difference here.
Note also the smaller stddev at the best alpha.
</p>

</aside>
</section>
<section id="slide-org5a4a0b9">
<h3 id="org5a4a0b9">Plotting coefficient values for LR</h3>
<div class="org-src-container">

<pre><code class="python" >plt.figure()
lr = LinearRegression().fit(X_train, y_train)
plt.scatter(range(X_poly.shape[1]),
            lr.coef_, c=np.sign(lr.coef_), cmap="bwr_r")
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/lr_coefficients_large.png" alt="lr_coefficients_large.png" height="350px" />
</p>
</div>

<aside class="notes">
<p>
Note the HUGE scale. Two of the features are just absolutely huge.
The features are probably correlated, but they also cancel each other out!
Large coefficients tend to mean overfitting, as a rule of thumb.
</p>

</aside>
</section>
<section id="slide-org8b174f9">
<h3 id="org8b174f9">Ridge Coefficients</h3>
<div class="org-src-container">

<pre><code class="python" >plt.figure()
grid = GridSearchCV(Ridge(), param_grid, cv=10)
grid.fit(X_train, y_train)
ridge = grid.best_estimator_
plt.scatter(range(X_poly.shape[1]), ridge.coef_,
			c=np.sign(ridge.coef_), cmap="bwr_r")
</code></pre>
</div>


<div class="figure">
<p><img src="assets/coeffplot2.png" alt="coeffplot2.png" height="350px" />
</p>
</div>

<aside class="notes">
<p>
Much better scale, none is huge.
</p>

</aside>
</section>
<section id="slide-org21a186c">
<h3 id="org21a186c">Ridge Coefficients</h3>
<div class="org-src-container">

<pre><code class="python" >ridge100 = Ridge(alpha=100).fit(X_train, y_train)
ridge1 = Ridge(alpha=1).fit(X_train, y_train)
plt.figure(figsize=(8, 4))
plt.plot(ridge1.coef_, 'o', label="alpha=1")
plt.plot(ridge.coef_, 'o', label="alpha=14")
plt.plot(ridge100.coef_, 'o', label="alpha=100")
plt.legend()
</code></pre>
</div>


<div class="figure">
<p><img src="assets/coeffplot3.png" alt="coeffplot3.png" height="350px" />
</p>
</div>

<aside class="notes">
<p>
Basically alpha pushes all coefficients towards zero.
</p>

</aside>
</section>
<section id="slide-org538dcbd">
<h3 id="org538dcbd">Coefficient Paths</h3>

<div class="figure">
<p><img src="./assets/ridge_coefficient_paths.png" alt="ridge_coefficient_paths.png" height="400px" />
</p>
</div>

<aside class="notes">
<p>
X axis: alpha, y-axis: coefficient magnitude
They shrink as alpha increases
</p>

</aside>
</section>
<section id="slide-org80d2ed6">
<h3 id="org80d2ed6">Learning Curves</h3>

<div class="figure">
<p><img src="./assets/ridge_learning_curve.png" alt="ridge_learning_curve.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Hold features constant, look at what happeneds when I increasing training size, for multiple settings of alpha.
With small data, regularization does help. With lots of data, the difference is smaller.
In other words, with small data sets we are prone to overfitting unless we regularize.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
