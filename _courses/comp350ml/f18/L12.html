<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-orgdb9e3ca">
<h2 id="orgdb9e3ca">Multiclass Classification</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
Ok, so I think that's enough on the two loss functions and regularization, and hopefully you have a bit of a feel for how these two classifiers work, and also an understanding that they are in fact quite similar in practice.
</p>

<p>
Next I want to look at how to go from binary classification to multi-class classification. Basically there is a simple but hacky way, and there's a slightly more complicated but theoretically sound way.
</p>

</aside>
</section>
<section id="slide-org1d86999">
<h3 id="org1d86999">Hack: Reduction to Binary Classification</h3>
<ul>
<li>One vs. Rest</li>
<li>One vs. One</li>

</ul>
<aside class="notes">
<p>
The slightly hacky way is using what's known as a reduction. We're doing a reduction like in math: reducing one problem to another. In this case we're reducing the problem of multi-class classification into several instances of the binary classification problem. And we already know how to deal with binary classification.
</p>

<p>
There are two straight-forward ways to reduce multi-class to binary classification. the first is called one vs rest, the second one is called one-vs-one.
</p>

</aside>
</section>
<section id="slide-org0614340">
<h3 id="org0614340">One vs. Rest (OVR)</h3>
<ul>
<li>For 4 classes:
<ul>
<li>A vs. {B,C,D}, B vs. {A,C,D}, C vs. {A,B,D}, D vs. {A,B,C}</li>

</ul></li>
<li>In general:
<ul>
<li>\(n\) binary classifiers, each on all data points</li>

</ul></li>

</ul>
<aside class="notes">
<p>
Let's start with One vs Rest. here, we learn one binary classifier for each class against the remaining classes. So let's say we have 4 classes, called 1 to 4. First we learn a binary classifier of the points in class 1 vs the points in the classes 2, 3 and 4. Then, we do the same for class 2, and so on. The way we end up building as many classifiers as we have classes.
</p>

</aside>
</section>
<section id="slide-orgc768cb4">
<h3 id="orgc768cb4">Prediction with OVR</h3>
<ul>
<li>Pick class with highest score: \[ \hat{y} = \text{argmax}_{i \in Y} w_i \vec{x} \]</li>
<li>Unclear why it works, but works well.</li>

</ul>
<aside class="notes">
<p>
To make a prediction, we compute the decision function of all classifiers, say 4 in the example, on a new data point. The one with the highest score for the positive class, the single class, wins, and that class is predicted.
</p>

<p>
It's a little bit unclear why this works as well as it does. Maybe there's some papers about that now, but I'm not
</p>

<p>
So in this case we have one coefficient vector w and one bias b for each class.
</p>

</aside>
</section>
<section id="slide-orgc82e7ac">
<h3 id="orgc82e7ac">OVR Prediction</h3>

<div class="figure">
<p><img src="./assets/ovr_lines.png" alt="ovr_lines.png" />
</p>
</div>
<aside class="notes">
<p>
Here is an illustration of what that looks like. Unfortunately it's a bit hard to draw 4 classes in general position in 2 dimensions, so I only used 3 classes here. So each class has an associated coefficient vector and bias, corresponding to a line. The line tries to separate this class from the other two classes.
</p>

</aside>
</section>
<section id="slide-orgfaa07e4">
<h3 id="orgfaa07e4">OVR Prediction Boundaries</h3>

<div class="figure">
<p><img src="./assets/ovr_boundaries.png" alt="ovr_boundaries.png" />
</p>
</div>
<aside class="notes">
<p>
Here are the decision boundaries resulting from the these three binary classifiers. Basically what they say is that the line that is closest decides the class. What you can not see here is that each of the lines also have a magnitude associated with them. It's not only the direction of the normal vector that matters, but also the length. You can think of that as some form of uncertainty attached to the line.
</p>

</aside>
</section>
<section id="slide-org94daefa">
<h3 id="org94daefa">One Vs. One (OVO)</h3>
<ul>
<li>A vs. B, A vs. C, A vs. D, B vs. C, B vs. D, C vs. D</li>
<li>\({n\choose{2}} = \frac{n(n-1)}{2}\) binary classifiers
<ul>
<li>each trained on a fraction of the data</li>

</ul></li>
<li>Vote for highest positives
<ul>
<li>Classify on all classifiers</li>
<li>Count how often each class was predicted</li>
<li>Return most commonly predicted class</li>

</ul></li>
<li>Works well, but again a heuristic</li>

</ul>
<aside class="notes">
<p>
The other method of reduction is called one vs one. In one vs one, we build one binary model for each pair of classes. In the example of having four classes that is one for 1 vs 2, one for 1v3 and so on. So we end up with n * (n - 1) /2 binary classifiers. And each is trained only on the subset of the data that belongs to these classes.
</p>

<p>
To make a prediction, we again apply all of the classifiers. For each class we count how often one of the classifiers predicted that class, and we predict the class with the most votes.
</p>

<p>
Again, this is just a heuristic and there's not really a good theoretical explanation why this should work.
</p>

</aside>
</section>
<section id="slide-org018f4b5">
<h3 id="org018f4b5">OVO Prediction</h3>

<div class="figure">
<p><img src="./assets/ovo_lines.png" alt="ovo_lines.png" />
</p>
</div>
<aside class="notes">
<p>
Here is an example for predicting on three classes in 2d using the one-vs-one heuristic. In the case of three classes, there's also three pairs. Three is a bit of a special case, with any more classes there would be more classifiers than classes.
</p>

<p>
The dashed lines are colored according to the pair of classes they separate. So the green and blue line separates the green and blue classes. The data points belonging to the grey class were not used in building this model at all.
</p>

</aside>
</section>
<section id="slide-orgbfbbddc">
<h3 id="orgbfbbddc">OVO Prediction Boundaries</h3>

<div class="figure">
<p><img src="./assets/ovo_boundaries.png" alt="ovo_boundaries.png" />
</p>
</div>
<aside class="notes">
<p>
Looking at the predictions made by the one vs one classifier the correspondence to the binary decision boundaries is a bit more clear than for the one vs rest heuristic, because it only takes the actual boundaries into account, not the length of the normal vectors. That makes it easier to visualize the geometry, but it's also a bit of a downside of the method because it means it discards any notion of uncertainty that was present in the binary classifiers. The decision boundary for each class is given by the two lines that this class is involved in. So the grey class is bounded by the green and grey line and the blue and grey line.
</p>

<p>
There is a triangle in the center in which there is one vote for each of the classes. In this implemenatation the tie is broken to just always predict the first class, which is the green one. That might not be the best tie breaking strategy, but this is a relatively rare case, in particular if there's more than three classes.
</p>

<p>
OVR and OVO are general heuristics not restricted to linear models. They can be used whenever a binary model for classification needs to be extended to the multi-class case. For logistic regression, there is actually a natural extension of the formulation, and we don't have to resort to these hacks.
</p>

</aside>
</section>
<section id="slide-orga3ccc2c">
<h3 id="orga3ccc2c">OVR-OVO Comparison</h3>
<div class="column" style="float:left; width: 50%">
<p>
OVR:
</p>
<ul>
<li>\(n\) classifiers</li>
<li>trained on imbalanced datasets of original size</li>
<li>Retains some uncertainty estimates</li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">
<p>
OVO:
</p>
<ul>
<li>\(n(n-1)/2\) classifiers</li>
<li>trained on balanced subsets</li>
<li>No uncertainty propagated</li>

</ul>
</div>
<aside class="notes">
<p>
If original problem was balanced, that is&#x2026;
</p>

<p>
OVR you are still returning a prediction probability, whereas OVO is just returning the winner of a vote.
</p>

</aside>
</section>
<section id="slide-orgd163da1">
<h3 id="orgd163da1">Multinomial Logistic Regression</h3>
<p>
\[ p(y=i | x) = \frac{\e^{w_{i}^T \vec{x}}}{\sum_j \e^{w_{j}^T \vec{x}}} \]
\[ \minw \sumn \log(p(y=y_i | x_i)) \]
\[ \hat{y} = \text{argmax}_{i \in Y} w_i \vec{x} \]
</p>
<ul>
<li>Same prediction rule as OVR.</li>

</ul>
<aside class="notes">
<p>
The binary logistic regression case can be generalized to multinomial logistic regression, in which we model the probability that i is one of the classes using this formula, which is also known as softmax. The probability is proportional to e to the minus wtx which is the same as in the binary case. But now we need to normalize it so that the sum over all classes is one. So we just divide it by this sum.
</p>

</aside>
</section>
<section id="slide-org3d6f273">
<h3 id="org3d6f273">In scikit-learn</h3>
<ul>
<li>OVO: Only SVC</li>
<li>OVR: default for all linear models, even <code>LogisticRegression</code></li>
<li><code>LogisticRegression(multi_class="multinomial")</code></li>
<li><code>clf.decision_function</code> \(=w^T x\)</li>
<li><code>logreg.predict_proba</code> gives probabilities for each class</li>
<li><code>SVC(probability=True)</code> not great</li>

</ul>
<aside class="notes">
<p>
All models in sklearn have multiclass built in
Logistic Regression still uses ovr, even though multinomial is better, so you have to set it
SVC(prob=True) does OVO SVM, then builds second model on top, really slow and probably not what you want.
</p>

</aside>
</section>
<section id="slide-orgafe992a">
<h3 id="orgafe992a">Multiclass in Practice</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.datasets import load_iris
iris = load_iris()
X,y = iris.data, iris.target
print(np.bincount(y))

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

logreg = LogisticRegression(multi_class="ovr",
                            random_state=0,
                            solver="lbfgs").fit(X,y)
linearsvm = LinearSVC().fit(X,y)
print(logreg.coef_.shape)
print(linearsvm.coef_.shape)
</code></pre>
</div>

<pre class="example">
[50 50 50]
(3, 4)
(3, 4)

</pre>

<aside class="notes">
<p>
OVR and multinomial logreg produce one coef per class. SVC would product same shape, but different semantics.
Any thing ending with "_" means it was learned from data.
</p>

</aside>
</section>
<section id="slide-org71f94dc">
<h3 id="org71f94dc"></h3>

<div class="figure">
<p><img src="./assets/logistic_coefs.png" alt="logistic_coefs.png" />
</p>
</div>
<div class="org-src-container">

<pre><code class="python" >logreg.coef_
</code></pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">-0.44524582</td>
<td class="org-right">0.89469401</td>
<td class="org-right">-2.32542777</td>
<td class="org-right">-0.97869151</td>
</tr>

<tr>
<td class="org-right">-0.18587061</td>
<td class="org-right">-2.11489439</td>
<td class="org-right">0.69770617</td>
<td class="org-right">-1.25139648</td>
</tr>

<tr>
<td class="org-right">-0.39444575</td>
<td class="org-right">-0.5132796</td>
<td class="org-right">2.93082545</td>
<td class="org-right">2.41710589</td>
</tr>
</tbody>
</table>

<aside class="notes">
<p>
Interpreting the feature coefficients
For each class we have a coefficient
Tells you what the classifier has learned
It's pretty interpretable, which is nice&#x2026;
after centering data, without intercept
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
