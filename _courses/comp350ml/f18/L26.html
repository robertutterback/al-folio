<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)
<section>
<section id="slide-orgaf42d8f">
<h2 id="orgaf42d8f">Calibration</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Müller
</p>
</section>
</section>
<section>
<section id="slide-org37f501e">
<h2 id="org37f501e">Calibration</h2>
<ul>
<li>Probabilities can be much more informative than labels:</li>
<li>"The model predicted you don’t have cancer" vs "The model predicted you’re 40% likely to have cancer"</li>

</ul>
<aside class="notes">
<p>
Also builds on model on others, but the goal is to get accurate
probability estimates - not just predictions. All models have some
uncertainty about how well it will perform in the real world. It's
important that this uncertainty estimate is accurate.
</p>

<p>
Calibration lets us get probability estimates form any model. Remember
SVM wasn't good at that. Even if your model does have estimates, you
can use calibration to make sure these are actually good estimates
(they might be off since the models are focused on prediction
accuracy, not getting accurate prob. estimates).
</p>

</aside>
</section>
<section id="slide-orgc58ea05">
<h3 id="orgc58ea05">Calibration curve (Reliability diagram)</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/prob_table.png" alt="prob_table.png" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/calib_curve.png" alt="calib_curve.png" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>How to measure calibration (even without ground truth!)</li>
<li>For binary classification only (much simpler)</li>
<li>you can be calibrated and inaccurate!</li>
<li>want to make sure you have a model that provides reasonable probabilities</li>
<li>Given a predicted ranking or probability from a supervised classifier, bin predictions.</li>
<li>Plot fraction of data that's positive in each bin.</li>
<li>X axis here is not great, it's index of bin</li>
<li>The bins here are 0.33, 0.66, 1</li>
<li>Prevalence in the, say, .9 bin should be .9.</li>
<li>i.e., what we want is a diagonal line</li>
<li>So this is a pretty bad classifier</li>
<li>Does NOT imply that a model is accurate</li>
<li>For a binary model with a balanced dataset: always say .5 for all
data points. Perfectly calibrated! But doesn't tell you anything
about the data points!</li>

</ul>

</aside>
</section>
<section id="slide-orge45aba2">
<h3 id="orge45aba2"><code>calibration_curve</code> with sklearn</h3>
<p>
Using subsample of covertype dataset
</p>
<div class="column" style="float:left; width: 50%">
<div class="org-src-container">

<pre id="tinycode"><code class="python" >from sklearn.linear_model import LogisticRegressionCV
print(X_train.shape)
print(np.bincount(y_train))
lr = LogisticRegressionCV().fit(X_train, y_train)
(52292, 54)
[19036 33256]
</code></pre>
</div>
<div class="org-src-container">

<pre id="tinycode"><code class="python" >lr.C_
array([ 2.783])
</code></pre>
</div>
<div class="org-src-container">

<pre id="tinycode"><code class="python" >print(lr.predict_proba(X_test)[:10])
print(y_test[:10])
# [[ 0.681  0.319]
#  [ 0.049  0.951]
#  [ 0.706  0.294]
#  [ 0.537  0.463]
#  [ 0.819  0.181]
#  [ 0.     1.   ]
#  [ 0.794  0.206]
#  [ 0.676  0.324]
#  [ 0.727  0.273]
#  [ 0.597  0.403]]
# [0 1 0 1 1 1 0 0 0 1]
</code></pre>
</div>
</div>
<div class="column" style="float:left; width: 50%">
<div class="org-src-container">

<pre id="tinycode"><code class="python" >from sklearn.calibration import calibration_curve
probs = lr.predict_proba(X_test)[:, 1]
prob_true, prob_pred = calibration_curve(y_test, probs, n_bins=5)
print(prob_true)
print(prob_pred)
# [ 0.2    0.303  0.458  0.709  0.934]
# [ 0.138  0.306  0.498  0.701  0.926]
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/predprob_positive.png" alt="predprob_positive.png" height="300px" />
</p>
</div>
</div>
<aside class="notes">
<ul>
<li>Logreg tends to give good prob. estimates, i.e., be well calibrated</li>
<li>Note: do this on test set, since of course we're good on training set</li>
<li>Take probabilities for first class</li>
<li>Using 5 bins, first bin is 0-0.2, would expect 0.1 positive samples,
actually 0.2, so slightly miscalibrated, and so on</li>

</ul>

</aside>
</section>
<section id="slide-orgc68d09f">
<h3 id="orgc68d09f">Influence of number of bins</h3>

<div class="figure">
<p><img src="./assets/influence_bins.png" alt="influence_bins.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>More bins gives you more resolution, but gives you more noise</li>
<li>logreg is sometimes too sure, which you can see at the far right</li>
<li>Works here because dataset is big</li>
<li>Might become very noisy for larger datasets</li>

</ul>

</aside>
</section>
<section id="slide-org32644c3">
<h3 id="org32644c3">Comparing Models</h3>

<div class="figure">
<p><img src="./assets/calib_curve_models.png" alt="calib_curve_models.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>No pruning on Decision tree, not great</li>
<li>Interesting sigmoid shape for RF&#x2026;</li>
<li>Want to fix this, i.e. trust the probs</li>
<li>But first let's measure them</li>
<li>Graph is nice, but we want an objective, numerical comparison</li>

</ul>

</aside>
</section>
<section id="slide-org5951a49">
<h3 id="org5951a49">Brier Score (for binary classification)</h3>
<ul>
<li>"mean squared error of probability estimate"</li>

</ul>
<p>
\[ BS = \frac{\sum_{i=1}^{n} (\widehat{p} (y_i)-y_i)^{2}}{n}\]
</p>

<div class="figure">
<p><img src="./assets/models_bscore.png" alt="models_bscore.png" height="250px" />
</p>
</div>
<aside class="notes">
<ul>
<li>Could also use log-loss</li>
<li>y_i is 1 or 0</li>
<li>\(\hat{p}\) is probability estimate</li>
<li>Very bad if you predicted 0 when it's actually 1</li>
<li>Smaller is better</li>
<li>Shown here above the plots</li>
<li>So RF is actually best here, b/c this is a measure of both accuracy and calibration</li>

</ul>

</aside>
</section>
<section id="slide-orga8ff481">
<h3 id="orga8ff481">Fixing it: Calibrating a classifier</h3>
<ul>
<li>Build another model, mapping classifier probabilities to better probabilities!</li>
<li>1d model! (or more for multi-class)</li>

</ul>

<p>
\[ f_{callib}(s(x)) \approx p(y)\]
</p>

<font size=6>
<ul>
<li>s(x) is score given by model, usually</li>
<li>Can also work with models that don’t even provide probabilities!
Need model for \(f_{callib}\), need to decide what data to train it
on.</li>
<li>Can train on training set \(\to\) Overfit</li>
<li>Can train using cross-validation \(\to\) use data, slower</li>

</ul>
</font>
<aside class="notes">
<ul>
<li>Similar to stacking! &#x2013; Build another model on top of prob. estimates</li>
<li>1D function: maps this probability to something more accurate</li>
<li>Two main methods</li>
<li>Basically we are going to try to find some function that pulls the
previous model to the diagonal in the plots we've seen</li>

</ul>

</aside>
</section>
<section id="slide-org828b44b">
<h3 id="org828b44b">Platt Scaling</h3>
<ul>
<li>Use a logistic sigmoid for \(f_{callib}\)</li>
<li>Basically learning a 1d logistic regression</li>
<li>(+ some tricks)</li>
<li>Works well for SVMs</li>

</ul>

<p>
\[f_{platt} = \frac{1}{1 + exp(-ws(x))}\]
</p>
<aside class="notes">
<ul>
<li>Let's you fix kind of a sigmoid shape, but not a lot to tune (just scalar w)</li>
<li>In other words, we're generating a single new feature s(x)</li>

</ul>

</aside>
</section>
<section id="slide-org60ee219">
<h3 id="org60ee219">Isotonic Regression</h3>
<font size=6>
<ul>
<li>Very flexible way to specify \(f_{callib}\)</li>
<li>Learns arbitrary monotonically increasing step-functions in 1d.</li>
<li>Groups data into constant parts, steps in between.</li>
<li>Optimum monotone function on training data (wrt mse).</li>

</ul>
</font>

<div class="figure">
<p><img src="./assets/isotonic_regression.png" alt="isotonic_regression.png" height="400px" />
</p>
</div>

<aside class="notes">
<ul>
<li>non-parametric mapping</li>
<li>Fits the monotone function that minimizes the squared error</li>
<li>Find the piecewise constant function that is monotonous and minimizes error</li>

</ul>

</aside>
</section>
<section id="slide-org7b1e356">
<h3 id="org7b1e356">Building the model</h3>
<ul>
<li>Using the training set is bad</li>
<li>Either use hold-out set or cross-validation</li>
<li>Cross-validation can be use as in stacking to make unbiased probability predictions, use that as training set.</li>

</ul>
<aside class="notes">
<p>

</p>

</aside>
</section>
<section id="slide-orgf874bf9">
<h3 id="orgf874bf9">CalibratedClassifierCV</h3>
<div class="org-src-container">

<pre id="smallcode"><code class="python" >from sklearn.calibration import CalibratedClassifierCV
X_train_sub, X_val, y_train_sub, y_val = \
    train_test_split(X_train, y_train,
                     stratify=y_train, random_state=0)
rf = RandomForestClassifier(n_estimators=100).fit(X_train_sub, y_train_sub)
scores = rf.predict_proba(X_test)[:, 1]
plot_calibration_curve(y_test, scores, n_bins=20)
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/random_forest.png" alt="random_forest.png" height="350px" />
</p>
</div>
<aside class="notes">
<ul>
<li>Will use X_val to calibrate next</li>

</ul>

</aside>
</section>
<section id="slide-org3ce0d4b">
<h3 id="org3ce0d4b">Calibration on Random Forest</h3>
<div class="org-src-container">

<pre id="smallcode"><code class="python" >cal_rf = CalibratedClassifierCV(rf, cv="prefit",
                                method='sigmoid')
cal_rf.fit(X_val, y_val)
scores_sigm = cal_rf.predict_proba(X_test)[:, 1]

cal_rf_iso = CalibratedClassifierCV(rf, cv="prefit",
                                    method='isotonic')
cal_rf_iso.fit(X_val, y_val)
scores_iso = cal_rf_iso.predict_proba(X_test)[:, 1]
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/types_callib.png" alt="types_callib.png" />
</p>
</div>

<aside class="notes">
<p>
Not actually doing CV here, using "prefit"
</p>

</aside>

</section>
<section id="slide-org5fff9c5">
<h3 id="org5fff9c5">Cross-validated Calibration</h3>
<div class="org-src-container">

<pre><code class="python" >cal_rf_iso_cv = CalibratedClassifierCV(rf, method='isotonic')
cal_rf_iso_cv.fit(X_train, y_train)
scores_iso_cv = cal_rf_iso_cv.predict_proba(X_test)[:, 1]
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/types_callib_cv.png" alt="types_callib_cv.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>Actually does cross-validation here</li>
<li>For each fold, train separate model. Then use all the models and average them.</li>
<li>kinda cheating, we have more trees now lol</li>
<li>we use all the data, get good probabilities. just
time-consuming. Ends up building more trees.</li>

</ul>

</aside>
</section>
<section id="slide-orgdfbed4e">
<h3 id="orgdfbed4e">Multi-Class Calibration</h3>

<div class="figure">
<p><img src="./assets/multi_class_calibration.png" alt="multi_class_calibration.png" />
</p>
</div>
<aside class="notes">
<ul>
<li>per-class calibration</li>
<li>Do for each class individually, then renormalize</li>

</ul>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
