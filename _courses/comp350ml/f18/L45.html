<!DOCTYPE html>
<html>
  <head>
    <title>More Neural Networks</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Garamond);
      @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      body {
        font-family: 'Muli';
        font-size: 140%;
      }
      h1, h2 {
        font-family: 'Garamond';
        font-weight: normal;
        margin-top: 10px;
        margin-bottom: 10px;
      }
      .remark-slide-content h1 {
        font-size: 70px;
        text-align: center;
      }
      .remark-slide-content p, .remark-slide-content li {
        font-size:30px;
        line-height: 1.4;
      }
      .remark-code {
        font-size:30px;
      }
      .remark-slide-content p {
          margin: 5px;
      }
      .remark-slide-container .spacious p,
      .remark-slide-container .spacious li{
          margin-bottom: 50px;
          margin-top: 50px;
      }
      .remark-slide-container .spacious h1{
          margin-bottom: 50px;
      }
      .remark-slide-container .some-space p,
      .remark-slide-container .some-space li,
      .remark-slide-container .some-space h1{
          margin-bottom: 30px;
      }
      .reset-column {
          overflow: auto;
          width: 100%;
      }
      .remark-slide-content .compact p, .remark-slide-content .compact li, .remark-slide-content .compact pre, .remark-slide-content .compact .MathJax_Display{
          font-size: 30px;
          line-height: 1.1;
          display: block;
          margin: 2px 0;
      }
      .padding-top {
          padding-top: 100px;
      }
      .remark-slide-content .smaller p, .remark-slide-content .smaller li,
      .remark-slide-content .smaller .remark-code, .remark-slide-content .smaller a{
          font-size: 25px;
      }
      .remark-slide-container .smallest p, .remark-slide-container .smallest li,
      .remark-slide-container .smallest .remark-code, .remark-slide-container .smallest a{
          font-size: 15px;
      }
      .normal {
          font-size: 30px;
      }
      .quote_author {
          display: block;
          text-align: right;
          margin-top: 20px;
          font-size: 30px;
          font-family: 'Garamond';
      }
      .larger, .larger .remark-code {
          font-size: 40px;
      }
      .largest, .largest .remark-code {
          font-size: 50px;
      }
      .left-column, .right-column {
          width: 48%;
      }
      .right-column{
          float: right;
      }
      .left-column{
          float: left;
      }
      .narrow-right-column {
          float: right;
          width: 32%
      }
      .wide-left-column {
          float: left;
          width: 65%
      }
      .invisible {
          visibility: hidden
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

# Neural Networks in Practice

Based on slides by Andreas Müller

???

---

class:center,middle
# Introduction to Keras
???
---
# Keras Sequential

.smaller[
```python
from keras.models import Sequential
from keras.layers import Dense, Activation
```
```
Using TensorFlow backend.
```
```python
model = Sequential([
    Dense(32, input_shape=(784,)),
    Activation('relu'),
    Dense(10),
    Activation('softmax')])

# or
model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation('relu'))

# or
model = Sequential([
    Dense(32, input_shape=(784,), activation='relu'),
    Dense(10, activation='softmax')])
```
]


???
- There are two interfaces to keras, sequential and the
functional, but we’ll only discuss sequential.
- Sequential is for feed-forward neural networks where one layer
follows the other. You specify the layers as a list, similar to a
sklearn pipeline.
- Dense layers are just matrix multiplications. Here we have a neural
net with 32 hidden units for the mnist dataset with 10 outputs. The
hidden layer nonlinearity is relu, the output if softmax for
multi-class classification.
- You can also instantiate an empty sequential model
and then add steps to it.
- For the first layer we need to specify the input shape so the model
knows the sizes of all the matrices. The following layers can infer
the sizes from the previous layers.
- softmax is for multiclass classification; num. nodes in last layer must match num. classes
- For most layers library will figure out required shape, except first layer, must tell it input
---

.smaller[
```python
model.summary()
```
]

.center[
![:scale 100%](images/model_summary.png)
]

???
- Tells me about all the layers I have
- First: (* 32 784)25088 plus biases (32)
- Second: (* 10 32)320 plus biases (10)

---
# Setting Optimizer

.center[
![:scale 90%](images/optimizer.png)
]

.smaller[
```python
model.compile("adam", "categorical_crossentropy", metrics=['accuracy'])
```
]

???
- Compile method picks optimization procedure and loss
- Usually ignore sample_weight_mode
- categorical crossentropy is basically multinomial logistic regression
---

# Training the model

.center[
![:scale 100%](images/training_model.png)
]

???
- Fit gets many parameters, unlike sklearn.
- epochs: num. iterations over whole dataset
- validation {data,split} can decide to stop early

---
# Preparing MNIST data
.smaller[
```python
from keras.datasets import mnist
import keras
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

num_classes = 10
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
```
```
60000 train samples
10000 test samples
```
]


???
- Classic digit dataset
- Convert to floats, scale manually (previously all grayscale between 0-255)
- Q: Now what's the range? A: between 0-1
- Need to convert labels to one hot encoding
---
# Fit Model
```python
model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)
```


.center[
![:scale 80%](images/model_fit.png)
]

???
- For each epoch tells loss and accuracy
---
# Fit with Validation

```python
model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, 
          validation_split=.1)
```


.center[
![:scale 100%](images/validation_fit.png)
]


???
- Here we get validation loss/accuracy too
---
# Evaluating on Test Set

```python
score = model.evaluate(X_test, y_test, verbose=0)
print("Test loss: {:.3f}".format(score[0]))
print("Test Accuracy: {:.3f}".format(score[1]))
```

```
Test loss: 0.120
Test Accuracy: 0.966
```


???
- Instead of .score use .evaluate
---

# Loggers and Callbacks

.smaller[
```python
history_callback = model.fit(X_train, y_train, batch_size=128,
                             epochs=100, verbose=1, validation_split=.1)
pd.DataFrame(history_callback.history).plot()
```
]

.center[
![:scale 70%](images/logger_callback_plot.png)
]

???
- This is not really being used as a callback here, but still useful
- Am I overfitting? How long does it take to train? Should I train longer?
- Probably trained too much here
---
# Wrappers for sklearn

.smaller[
```python
from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor
from sklearn.model_selection import GridSearchCV

def make_model(optimizer="adam", hidden_size=32):
    model = Sequential([
        Dense(hidden_size, input_shape=(784,)),
        Activation('relu'),
        Dense(10),
        Activation('softmax'),
    ])
    model.compile(optimizer=optimizer,loss="categorical_crossentropy",   
                  metrics=['accuracy'])
    return model

clf = KerasClassifier(make_model)
param_grid = {'epochs': [1, 5, 10],  # epochs is fit parameter, not in make_model!
              'hidden_size': [32, 64, 256]}
grid = GridSearchCV(clf, param_grid=param_grid, cv=5)
grid.fit(X_train, y_train)
```
]

- See https://keras.io/scikit-learn-api/

???
- Useful for grid-search.
- You need to define a callable that returns a compiled model.
- You can search parameters that in Keras would be passed to “fit”
  like the number of epochs.
- Searching over epochs in this way is not necessarily a good idea,
  though.
---
.smaller[
```python
res = pd.DataFrame(grid.cv_results_)
res.pivot_table(index=["param_epochs", "param_hidden_size"],
                values=['mean_train_score', "mean_test_score"])
```
]

.center[
![:scale 70%](images/keras_api_results.png)
]



???
- Training longer overfits more and more units overfit more, but both
  also lead to better results.
- We should probably train much longer actually.
- Setting the number of epochs via cross-validation is a bit silly
since it means starting from scratch again each time. Using early
stopping would be better.

---
class: middle
# Drop-out

???
- Relatively new technique for regularization
---
# Drop-out Regularization

.center[
![:scale 65%](images/dropout_reg.png)
]

--

- https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf

- Rate often as high as .5, i.e. 50% of units set to zero!

- Predictions: use all weights, down-weight by rate

???

- Randomly set activations to zero.
- Drop out is a very successful regularization technique developed
in 2014. It is an extreme case of adding noise to the input, a
previously established method to avoid overfitting.
- Instead of adding noise, we actually set given inputs to
0. And not only on the input layer, also the intermediate layer.
- For each sample, and each iteration we pick different
nodes. Randomization avoids overfitting to particular examples.

---
class:spacious
# Ensemble Interpretation

- Every possible configuration represents different
network.

- With p=.5 we jointly learn `$\binom{n}{n/2}$` networks

- Networks share weights

- For last layer dropout: prediction is approximate
geometric mean of predictions of sub-networks.
???
---
# Implementing Drop-Out

.smaller[
```python
from keras.layers import Dropout

model_dropout = Sequential([
    Dense(1024, input_shape=(784,), activation='relu'),
    Dropout(.5),
    Dense(1024, activation='relu'),
    Dropout(.5),
    Dense(10, activation='softmax'),
])
model_dropout.compile("adam", "categorical_crossentropy", metrics=['accuracy'])
history_dropout = model_dropout.fit(X_train, y_train, batch_size=128,
                            epochs=20, verbose=1, validation_split=.1)
```
]

???
- Basically just another 'layer'
- Made much bigger network here but regularized by dropout to avoid overfitting
---
class:spacious
# When to use drop-out

- Avoids overfitting

- Allows using much deeper and larger models

- Slows down training somewhat

- Should be possible to get even better results on MNIST with GPU

???
- Very popular technique/trick these days
---
class:center,middle
# Convolutional neural networks

???
- Augmenting (or adding noise to) training data turns out to be helpful
- Example: flip image on axis, still same image, but gives another example
- Doubles size of data set!
- Or: crop the images in different ways
- Correlated but works pretty well
- Similarly just add a little bit of noise increasing data set size
- This is similar to what drop-out is doing, except it is often more
  helpful to do it with the input IF you have domain knowledge
- Getting data is easy, getting labelled images can be hard, so this works well
- Convolutional NN is a way of doing this
---
class:spacious
# Idea

- Translation invariance

- Weight sharing

???

- "Translating" the image a little shouldn't really change my interpretation
- e.g., finding a face; doesn't matter where in the image it is
- So can move an image around to get training images where faces are all over

---
# Definition of Convolution


`$$ (f*g)[n] = \sum\limits_{m=-\infty}^\infty f[m]g[n-m] $$`

`$$ = \sum\limits_{m=-\infty}^\infty f[n-m]g[m] $$`

.center[
![:scale 80%](images/convolution.png)
]

???
- Definition of convolution of integers from wikipedia. Not very natural...
- The definition is symmetric in f, but usually one is the input
signal, say f, and g is a fixed “filter” that is applied to it.
- You can imagine the convolution as g sliding over f.
- If the support of g is smaller than the support of f (it’s a shorter
non-zero sequence) then you can think of it as each entry in f * g
depending on all entries of g multiplied with a local window in f.
- Example: f data is time series
- Want to apply filter g: -1, 2, -1
- This convolution: dot product with window
- Not that the output is shorter than the input by half the size of
g (is that right?). This is called a valid convolution.
- We could also extend f with zeros, and get a result that is larger
than f by half the size of g, that’s called a full convolution. We can
also just pad a little bit and get something that is of the same size
as f.
- Also not that the filter g is flipped as it’s indexed with m

---
# 1D example: Gaussian smoothing

.center[
![:scale 80%](images/Gaussian_Smoothing.png)
]

???
- Top left some complicated data, maybe stock prices
- Filter with Gaussian
- Get a smoother dataset
- Each point in new series is Gaussian-weighted average of surrounding points
- Smoothing is a simple thing you can do with convolutions
---
# 2D Smoothing
.center[
![:scale 80%](images/2dsmoothing.png)
]

???
- Same in 2D
- Basically taking a 14x14 matrix that is Gaussian
- Convolve the image with it, basically get a blurred image
- Just a toy example, not that interesting
- But also can use it to compute gradients

---
# 2D Image Gradients
.center[
![:scale 80%](images/2dgradient.png)
]

???
- This filter actually computes an image gradient
- An image gradient is a directional change in the intensity or color
  in an image, related to calculus gradient
- Left side is negative, right side is positive
- Basically what you get is basically vertical edges
- In an CNN, each layer applies convolutional filters
- The network learns the weights of these filters
- B/c the same filters get applied everywhere, you get translation
invariance; you learn the patterns no matter where they are in the
image

---
# Convolution Neural Networks

.center[
![:scale 100%](images/CNET1.png)
]

- Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.

- Gradient-based learning applied to document recognition

???
- Here is the architecture of an early convolutional net
from 1998. The basic architecture in current networks is still the
same.
- Applied 20 years ago, first applied to MNIST I think
- You can have multiple layers of convolutions and resampling
operations. You start convolving the image, which extracts local
features.
- Each convolutions creates new “feature maps” that serve as input to
later convolutions. First layer is 6 different 28x28 feature map
- To allow more global operations, after the convolutions the image
resolution is changed. Back then it was subsampling, today it is
max-pooling.
- So you end up with more and more feature maps with lower and lower
resolution.
- At the end, you have some fully connected layers to do the
classification.


---
.center[
![:scale 60%](images/cnn_digits.png)
]
???
- Big pic is input (plus noise added?) (that might fool simpler models!)
- Columns are first layer activation, subsampling, next convolution, etc.
- Above big pic is original input, then output by model
---
# Deconvolution
.center[
![:scale 100%](images/deconvolution_1.png)
]
https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf
???
- Applied to natural images, this is what you get
- This is one of the standard NN trained on ImageNet dataset (a famous image dataset)
- Top left shows the 9 filters in the first layer
- Below that shows which image parts activated that particular filter the most
- You can see upper left corner finds NW/SE edges, Middle bottom finds green, etc.
- Big layer 2 shows next layer's filters, though it's hard to visualize exactly what they do
- The filters act on output from the previous layer, not input, so you
  have to project what the filters do to the input space

---
class: center, middle

![:scale 100%](images/deconvolution_2.png)

???
- Another layer up. The hope is that each layer up it learns more
  abstract concepts about the images.
- Convolutions used to be used without NN, but then experts had to figure out which filters to use.
- BTW this dataset is a 1000-class classification task! Pretty hard!
- Notice humans. This dataset didn't ask to detect humans! Probably
  helped the network find other things when humans were present.
---
class: center, middle


![:scale 100%](images/deconvolution_3.png)

???

---

# Max Pooling

.center[
![:scale 100%](images/maxpool.png)
]

???
- Instead of subsampling most now use max pooling
- Subsampling is just averaging
- Max pooling just takes maximum of a particular group
- Again not differentiable → subgradient descent
- Need to remember position of maximum for back-propagation.

---

.center[
![:scale 80%](images/other_architectures.png)
]
???
- Here are two more recent architectures, AlexNet from 2012 and VGG
net from 2015.
- AlexNet blew everything out of the water at the time
- It was as big as you could fit on two GPUs at the time
- These nets are typically very deep, but often have very small
convolutions. In VGG there are 3x3 convolutions and even 1x1
convolutions which serve to summarize multiple feature maps into one.
- This means that most of the parameters are in the dense layers at
  the end. In the convolutional layers there are lots of activations
  but not that many parameters/weights to learn.
- There is often multiple convolutions without pooling in between but
pooling is definitely essential.
- More recent ones might be even bigger, e.g., InceptionNet, GoogleNet

---
class:center,middle
# Conv-nets with keras

???
---
# Preparing Data

.smaller[
```python
batch_size = 128
num_classes = 10
epochs = 12

# input image dimensions
img_rows, img_cols = 28, 28

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()


X_train_images = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
X_test_images = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
```
]


???
- For convolutional nets the data is n_samples, width, height,
channels.
- MNIST has one channel because it’s grayscale. Often you have RGB
channels or possibly Lab.
- First need to reshape - using 3 dimensional data structure here!
- Input is a row for each image, but reshape to make into actual image
- The position of the channels is configurable, using the
“channels_first” and “channels_last” options – but you shouldn’t have
to worry about that.

---
# Create Tiny Network

```python
from keras.layers import Conv2D, MaxPooling2D, Flatten

num_classes = 10
cnn = Sequential()
cnn.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Conv2D(32, (3, 3), activation='relu'))
cnn.add(MaxPooling2D(pool_size=(2, 2)))
cnn.add(Flatten())
cnn.add(Dense(64, activation='relu'))
cnn.add(Dense(num_classes, activation='softmax'))
```



???
- For convolutional nets we need 3 new layer types:
- Conv2d for 2d convolutions, MaxPooling2d for max pooling and Flatten
go reshape the input for a dense layer.
- There are many other options but these are the most commonly used
ones.
- 32 different filters for Conv2D

---
# Number of Parameters

.left-column[
Convolutional Network for MNIST
![:scale 100%](images/cnn_params_mnist.png)
]
.right-column[
Dense Network for MNIST
![:scale 100%](images/dense_params_mnist.png)
]

???
- Comparing num. parameters

---
# Train and Evaluate

.smaller[
```python
cnn.compile("adam", "categorical_crossentropy", metrics=['accuracy'])
history_cnn = cnn.fit(X_train_images, y_train,
                      batch_size=128, epochs=20, verbose=1, validation_split=.1)
cnn.evaluate(X_test_images, y_test)
```
```
 9952/10000 [============================>.] - ETA: 0s
 [0.089020583277629253, 0.98429999999999995]
```
]
.center[
![:scale 50%](images/train_evaluate.png)
]
???
- Some overfitting but generally pretty decent
---
# Visualize Filters
.smaller[
```python
weights, biases = cnn_small.layers[0].get_weights()
weights2, biases2 = cnn_small.layers[2].get_weights()
print(weights.shape)
print(weights2.shape)
```
```
(3,3,1,8)
(3,3,8,8)
```
]
.center[![:scale 40%](images/visualize_filters.png)]
???
- Can then look at the filters it learned
---
.center[![:scale 80%](images/digits.png)]
???
- Can look at the activation
---
class:center,middle
# Batch Normalization
???
- A heuristic that speeds up learning and often gets better results
- Idea is like trying to scale hidden units and 0 mean per batch
---
# Batch Normalization

.center[
![:scale 80%](images/batch_norm.png)
]

.smallest[
[Ioffe, Szegedy: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
]

???
- Another relatively recent advance in neural networks is batch
normalization.
- The idea is that neural networks learn best when the input is zero
mean and unit variance. We can scale the data to get that.
- But each layer inside a neural network is itself a neural network
with inputs given by the previous layer. And that output might have
much larger or smaller scale (depending on the activation function).
- Batch normalization re-normalizes the activations for a layer for
each batch during training (as the distribution over activation
changes). The avoids saturation when using saturating functions.
- To keep the expressive power of the model, additional scale and
shift parameters are learned that are applied after the per-batch
normalization. (turns out some scale is helpful)

---
# Convnet with Batch Normalization
.smaller[```python
from keras.layers import BatchNormalization

num_class = 10
cnn_small_bn = Sequential()
cnn_small_bn.add(Conv2D(8, kernel_size=(3, 3),
                 input_shape=input_shape))
cnn_small_bn.add(Activation("relu"))
cnn_small_bn.add(BatchNormalization())
cnn_small_bn.add(MaxPooling2D(pool_size=(2, 2)))
cnn_small_bn.add(Conv2D(8, (3, 3)))
cnn_small_bn.add(Activation("relu"))
cnn_small_bn.add(BatchNormalization())
cnn_small_bn.add(MaxPooling2D(pool_size=(2, 2)))
cnn_small_bn.add(Flatten())
cnn_small_bn.add(Dense(64, activation='relu'))
cnn_small_bn.add(Dense(num_classes, activation='softmax'))
```]
???
- Usually done *after* activations
---
# Learning speed and accuracy
.center[![:scale 80%](images/learning_speed.png)
]
???
- Solid lines with batch normalizations
- Gets to top much faster
---
# For larger net (64 filters)
.center[![:scale 80%](images/learning_speed_larger.png)
]
???
- Learns even faster, learns to overfit really well.
---

class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
      // Config Remark
      remark.macros['scale'] = function (percentage) {
	  var base = "https://amueller.github.io/COMS4995-s18/slides/aml-23-041818-convolutional-nets/";
	  var url = base + this;
	  return '<img src="' + url + '" style="width: ' + percentage + '" />';
      };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
