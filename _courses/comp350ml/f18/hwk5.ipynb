{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 -- Name(s) here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due Friday, November 30 by 2:00 PM**. You may submit this assignment in groups of 2. Be sure to put your names above. Also, don't forget to submit your data file(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is based on an assignment by Andreas MÃ¼ller.\n",
    "\n",
    "In this homework, we'll be looking at an imbalanced classification task with text data. Links to data: [test](https://www.dropbox.com/s/6ot9w3on66gp129/hw5_data_test.csv?dl=1) [train](https://www.dropbox.com/s/71q00c3r3vmsz9j/hw5_data_train.csv?dl=1)\n",
    "\n",
    "The dataset consists of women's fashion online shopping reviews. The reviews have a title, review text, and a yes or no on whether the review author would recommend the product. We are trying to determine this yes/no answer based on the review title and text. IN a real application we could find out what is good or bad about certain products or decide which reviews to feature on a product landing page, e.g., picking a typical critical review and a typical positive review.\n",
    "\n",
    "Be sure to use a metric that's appropriate for imbalanced classification, e.g., AUC or average precision (or something else). Inspect all models by visualizing the coefficients.\n",
    "\n",
    "For each task select a single, best model, explain your choice, and evaluate this model using the test set. Since you'll also need to use some data to select this best model, be sure to use cross-validation for evaluation, then only use the test set after you've selected the best model.\n",
    "\n",
    "We will only try out some combination of preprocessing methods and models. While that doesn't guarantee the best possible model, we try to get a reasonable trade-off of results and model development time.\n",
    "\n",
    "Note: using the `memory` option of `Pipeline` is likely to decrease running times for your grid-searches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any necessary setup code that all the below blocks will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Title and Body (30 points)\n",
    "\n",
    "We will look at four ways to use the data:\n",
    "\n",
    "1. Use the title only\n",
    "2. Use the review text only\n",
    "3. Concatenate the title and review body to a single text and analyze that (discarding the information about which words were in the title and which in body)\n",
    "4. Vectoring title and review individually and concatenating the vector representations.\n",
    "\n",
    "Use `CountVectorizer` with the default settings and train a linear classifier. Visualize the 20 most important features in the linear model. Tune the regularization parameter of the classifier, and visualize the 20 most important features after regularization.\n",
    "\n",
    "Do this for all four options shown above. Which one works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For simplicity, in the remaining tasks we will work only with option 3, concatenating the review title and body.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature Tuning (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "Try using `TfidfVectorizer` instead of `CountVectorizer`. Does it change the score? Does it change the important coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "Remember that `TfidfVectorizer` uses normalization by default. Does using a `Normalizer (sklearn.preprocessing.Normalizer)` with `CountVectorizer` change the outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "Try using stop-words. Do the standard English stop-words help? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "Limit the vocabulary using `min_df` or `max_df`. How do these impact the number of features, and how do they impact the scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: N-grams (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "\n",
    "Using your current best model, try changing from unigrams to n-grams of varying length. What provides the best performance? Visualize the coefficients. Try visualizing only the higher-order n-grams that are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "\n",
    "Try using character n-grams. Visualize the coefficients. Can we learn something from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3\n",
    "\n",
    "Investigate how `min_df` and the use of stop-words changes the number of features when using word n-grams, and how they change the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Model Tuning (again) (10 points)\n",
    "\n",
    "Revisit your choice of model. Compare different linear models with L1 and L2 penalty ont he best performing features from Task 3.\n",
    "\n",
    "Are there any other obvious features to try, or combinations to try out? (Don't perform them, just list them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
