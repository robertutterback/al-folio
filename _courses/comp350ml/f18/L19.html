<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-org78e6f61">
<h2 id="org78e6f61">Automatic Feature Selection</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
\(2+2\)
</p>

</aside>
</section>
<section id="slide-orga6759fd">
<h3 id="orga6759fd">Why Select Features?</h3>
<ul>
<li>Avoid overfitting</li>
<li>Faster prediction and training</li>
<li>Less storage for model and dataset</li>
<li>More interpretable model</li>

</ul>
<aside class="notes">
<p>
Can avoid overfitting, although in practice this doesn't help that much (other methods, e.g. penalization)
But the others are important, especially the last.
</p>

</aside>
</section>
<section id="slide-org70ce875">
<h3 id="org70ce875">Types of Feature Selection</h3>
<ul>
<li>Unsupervised vs Supervised</li>
<li>Univariate vs Multivariate</li>
<li>Model based or not</li>

</ul>
<aside class="notes">
<p>
Supervised will consider the target.
Univariate &#x2013; looks at each feature by itself, multivariate looks at interactions.
Model will build an ML model.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org84bb9f0">
<h2 id="org84bb9f0">Unsupervised Feature Selection</h2>
<div class="outline-text-2" id="text-org84bb9f0">
</div>
</section>
<section id="slide-orgc78ddd8">
<h3 id="orgc78ddd8">Unsupervised Feature Selection</h3>
<ul>
<li>May discard important information</li>
<li>Variance-based: 0 variance or few unique values</li>
<li>Covariance-based: remove correlated features</li>
<li>PCA: remove linear subspaces</li>

</ul>
<aside class="notes">
<p>
Small variance? Don't remove! Probably data is just on a small scale.
</p>

<p>
Covariance-based: maybe the diff. between two correlated features was
the only way to distinguish between two classes! In general a problem
with unsupervised; it's not considering the results.
</p>

<p>
PCA derived "linearly independent" features from original
features. Principal Components Analysis. 
</p>

</aside>
</section>
<section id="slide-orgbd1aa3c">
<h3 id="orgbd1aa3c">Covariance</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.preprocessing import scale
boston = load_boston()
X, y = boston.data, boston.target
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, random_state=0)
X_train_scaled = scale(X_train)
cov = np.cov(X_train_scaled, rowvar=False)
</code></pre>
</div>

<div class="figure">
<p><img src="./assets/l18_17.png" alt="l18_17.png" height="350px" />
</p>
</div>
<aside class="notes">
<p>
Look at which features are most correlated with others, and drop
those.  Here the lighter values are positively correlated. But this
plot is hard to read.
</p>

</aside>
</section>
<section id="slide-org08ced72">
<h3 id="org08ced72"></h3>
<div class="org-src-container">

<pre><code class="python" >from scipy.cluster import hierarchy
order = np.array(hierarchy.dendrogram(
    hierarchy.ward(cov),no_plot=True)['ivl'], dtype="int")
</code></pre>
</div>

<div class="figure">
<p><img src="./assets/l18_19.png" alt="l18_19.png" />
</p>
</div>
<aside class="notes">
<p>
Here I've reordered columns and rows to create "blocks" that are
correlated or uncorrelated. Can look at this manually or you can
create algorithms that look at these values and based on some
threshold can remove some features.
</p>

<p>
Note that that code doesn't create this nice picture. Look at
matplotlib's <code>pcolor</code> (mentionted before), <code>imshow</code>, or <code>matshow</code>
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orge9a81eb">
<h2 id="orge9a81eb">Supervised Feature Selection</h2>
<div class="outline-text-2" id="text-orge9a81eb">
</div>
</section>
<section id="slide-org46be286">
<h3 id="org46be286">Univariate Statistics</h3>
<ul>
<li>Pick statistic, check p-values !</li>
<li>f_regression, f_classsif, chi2 in scikit-learn</li>

</ul>
<div class="org-src-container">

<pre><code class="python" >from sklearn.feature_selection import f_regression
f_values, p_values = f_regression(X, y)
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/l18_20.png" alt="l18_20.png" height="375px" />
</p>
</div>
<aside class="notes">
<p>
Use a statistical test, look at each feature and see if it's
significantly related to the target. This is using boston again. Ex:
rooms and lstat are most important here. CHAS is a binary model and
this is using a regression model, and linear models aren't very good
at using binary features.
</p>

</aside>
</section>
<section id="slide-org926d20f">
<h3 id="org926d20f"></h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.feature_selection import \
    SelectKBest, SelectPercentile, SelectFpr
from sklearn.linear_model import RidgeCV
select = SelectKBest(k=2, score_func=f_regression)
select.fit(X_train, y_train)
print(X_train.shape)
print(select.transform(X_train).shape)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >(379, 13)
(379, 2)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >all_features = make_pipeline(StandardScaler(), RidgeCV())
np.mean(cross_val_score(all_features, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.718
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >select_2 = make_pipeline(StandardScaler(),
                         SelectKBest(k=2,
                                     score_func=f_regression),
                         RidgeCV())
np.mean(cross_val_score(select_2, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.624
</code></pre>
</div>

<aside class="notes">
<p>
Select k best features, percentile, Fpr controls for false positive
rate of thinking features are significantly important. Need to set
some score function, regression or classification. First try selects
just 2/13 features.
</p>

<p>
Second part is just with standard scaler.
</p>

<p>
Third: Fitting a model with only 2 best features. It overselected &#x2013;
two is not enough features, it got worse.
</p>

</aside>
</section>
<section id="slide-org77bf004">
<h3 id="org77bf004">Mutual Information</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.feature_selection import mutual_info_regression
scores = mutual_info_regression(X_train, y_train,
                                discrete_features=[3])
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/l18_22.png" alt="l18_22.png" />
</p>
</div>
<aside class="notes">
<p>
A little more complicated method that uses a non-parametric
model. Works on discrete and continuous, but must tell it which are
discrete.
</p>

<p>
Plot is comparing mutual info scores vs. F values. Kind of similar but
there are differences. If you think there are non-linear interactions
between features, this might catch that, although this takes
a lot longer to run. Still univariate though.
</p>

</aside>
</section>
<section id="slide-org0f31f38">
<h3 id="org0f31f38">Model-Based Feature Selection</h3>
<ul>
<li>Get best fit for a particular model</li>
<li>Ideally: exhaustive search over all possible combinations</li>
<li>Exhaustive is infeasible (and has multiple testing issues)</li>
<li>Use heuristics in practice.</li>

</ul>
<aside class="notes">
<p>
Find the subset of features on which this model performs best, i.e., a
search algorithm. Exhaustive search is too long and also might overfit
(pick up on noise).
</p>

</aside>
</section>
<section id="slide-org509901a">
<h3 id="org509901a">Model based (single fit)</h3>
<ul>
<li>Build a model, select features important to model</li>
<li>Lasso, other linear models, tree-based Models</li>
<li>Multivariate - linear models assume linear relation</li>

</ul>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import LassoCV
X_train_scaled = scale(X_train)
lasso = LassoCV().fit(X_train_scaled, y_train)
print(lasso.coef_)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >[-0.881 0.951 -0.082 0.59 -1.69 2.639 -0.146 -2.796 1.695 -1.614 -2.133 0.729 -3.615]
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/l18_23.png" alt="l18_23.png" height="200px" />
</p>
</div>
<aside class="notes">
<p>
Fit a model, discard the features it doesn't think are important.
</p>

</aside>
</section>
<section id="slide-orgced6425">
<h3 id="orgced6425">Changing Lasso alpha</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import Lasso
X_train_scaled = scale(X_train)
lasso = Lasso().fit(X_train_scaled, y_train)
print(lasso.coef_)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >[-0. 0. -0. 0. -0. 2.529 -0. -0. -0. -0.228 -1.701 0.132 -3.606]
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/l18_24.png" alt="l18_24.png" />
</p>
</div>
<aside class="notes">
<p>
Different results if you have a different alpha. (this one is penalizing more)
</p>

</aside>
</section>
<section id="slide-orge28c7d4">
<h3 id="orge28c7d4"><code>SelectFromModel</code></h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.feature_selection import SelectFromModel
select_lassocv = SelectFromModel(LassoCV(), threshold=1e-5)
select_lassocv.fit(X_train, y_train)
print(select_lassocv.transform(X_train).shape)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >(379,11)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >pipe_lassocv = make_pipeline(StandardScaler(),
                             select_lassocv, RidgeCV())
np.mean(cross_val_score(pipe_lassocv, X_train, y_train, cv=10))
np.mean(cross_val_score(all_features, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.717
0.718
</code></pre>
</div>

<div class="org-src-container">

<pre id="smallcode"><code class="python" ># could grid-search alpha in lasso
select_lasso = SelectFromModel(Lasso())
pipe_lasso = make_pipeline(StandardScaler(), select_lasso, RidgeCV())
np.mean(cross_val_score(pipe_lasso, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.671
</code></pre>
</div>

<aside class="notes">
<p>
Takes any model that gives you feature importances, and uses it to do
feature selection. Notice the threshold value. 
</p>

<p>
Can then put this into a pipeline. Performance is about the same actually.
</p>

</aside>
</section>
<section id="slide-org7cac783">
<h3 id="org7cac783">Iterative Model-Based Selection</h3>
<ul>
<li>Fit model, find least important feature, remove, iterate.</li>
<li>Or: Start with single feature, find most important feature, add, iterate.</li>

</ul>
<aside class="notes">
<p>
Importance of features might change after we remove a feature! So we
should do this iteratively! Start with removing features&#x2026;
</p>

</aside>
</section>
<section id="slide-org7855a00">
<h3 id="org7855a00">Recursive Feature Elimination</h3>
<ul>
<li>Uses feature importances / coefficients, similar to “SelectFromModel”</li>
<li>Iteratively removes features (one by one or in groups)</li>
<li>Runtime: (n_features - n_feature_to_keep) / stepsize</li>

</ul>
<aside class="notes">
<p>
Fit model, discard least important feature(s), repeat until you have
as many features as you want. Can use any models which give you a
measure of feature importance. (hard to get with SVMs).
</p>

<p>
Expensive b/c we must train a model many times.
</p>

</aside>
</section>
<section id="slide-org377b74e">
<h3 id="org377b74e"></h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
# create ranking among all features by selecting only one
rfe = RFE(LinearRegression(), n_features_to_select=1)
rfe.fit(X_train_scaled, y_train)
rfe.ranking_
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >array([ 9, 8, 13, 11, 5, 2, 12, 4, 7, 6, 3, 10, 1])
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/l18_27.png" alt="l18_27.png" />
</p>
</div>
<aside class="notes">
<p>
Implemented in sklearn. using LR to decide which features are
important. Notice using RFE ranking is slightly different than LR by
itself. The red dots are LR coefficients for one run of the model. But
RFE is doing this iteratively and then ranking which coefficients are
important. In this case the difference in not huge, but YMMV.
</p>

<p>
This is selecting only one feature at a time, so along the way it's
training models with n, n-1, n-2, &#x2026;, 5, 4, 3, 2, 1 models. So if you
wanted to grid search the best number of features to use it would be
wasting a lot of effort if you passed in RFE to it!
</p>

</aside>

</section>
<section id="slide-org4a9cb2e">
<h3 id="org4a9cb2e">RFECV</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFECV
rfe = RFECV(LinearRegression(), cv=10)
rfe.fit(X_train_scaled, y_train)
print(rfe.support_)
print(boston.feature_names[rfe.support_])
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >[ True  True False  True  True  True False  True  True  True  True  True True]
['CRIM' 'ZN' 'CHAS' 'NOX' 'RM' 'DIS' 'RAD' 'TAX' 'PTRATIO' 'B' 'LSTAT']
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >pipe_rfe_ridgecv = make_pipeline(StandardScaler(),
                                 RFECV(LinearRegression(),
                                       cv=10),
                                 RidgeCV())
np.mean(cross_val_score(pipe_rfe_ridgecv, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.710
</code></pre>
</div>

<aside class="notes">
<p>
Efficient CV for n_features_to_keep. This is RFE <b>inside</b>
cross-validation. Going from all features to just one feature, uses
crossval to select best num. In this case it only dropped 2 features,
the eleven it kept are shown.
</p>

<p>
If we want to predict with the same model as used for selection, RFECV
can be used as the prediction step. Could also use RFECV as
transformer and use any other model!
</p>

</aside>
</section>
<section id="slide-org31dd955">
<h3 id="org31dd955"></h3>
<div class="org-src-container">

<pre><code class="python" >
pipe_rfe_ridgecv = make_pipeline(StandardScaler(),
                                 RFECV(LinearRegression(),
                                       cv=10),
                                 RidgeCV())
np.mean(cross_val_score(pipe_rfe_ridgecv, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.710
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >from sklearn.preprocessing import PolynomialFeatures
pipe_rfe_ridgecv = make_pipeline(StandardScaler(),
                                 PolynomialFeatures(),
                                 RFECV(LinearRegression(),
                                       cv=10),
                                 RidgeCV())
np.mean(cross_val_score(pipe_rfe_ridgecv, X_train, y_train, cv=10))
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.820
</code></pre>
</div>

<aside class="notes">
<p>
If you do feature selection this way, your model for feature selection
could be different than model for prediction. Just an additional
degree of freedom. So I might do something very complicated for
feature selection, but use a more interpretable model for prediction
so I can explain it to my boss.
</p>

</aside>
</section>
<section id="slide-org4ca46c0">
<h3 id="org4ca46c0">Wrapper methods</h3>
<ul>
<li>Can be applied for ANY model!</li>
<li>Shrink / grow feature set by greedy search</li>
<li>Called Forward or Backward selection</li>
<li>Run CV / train-val split per feature</li>
<li>Complexity: n_features * (n_features + 1) / 2</li>
<li>Implemented in <code>mlxtend</code></li>

</ul>
<aside class="notes">
<p>
Expensive, but can be applied to any model. Basically just do a greedy
search over features, either adding most important or removing least
important feature at each step. 
</p>

<p>
Ex: Start with all features. Leave out first feature, build model,
then do crossval to get an accuracy feature. Do this for <b>each</b>
feature, leaving it out and building a model. After this, drop the
feature that gave you the highest accuracy/\(R^2\). Then repeat!
</p>

<p>
Each iteration requires training an crossval on many models. VERY slow.
</p>

<p>
Not in sklearn, but in <code>mlxtend</code>, but has same interface as sklearn,
so you can use these in a pipeline.
</p>

</aside>
</section>
<section id="slide-orgd73b6cd">
<h3 id="orgd73b6cd"><code>SequentialFeatureSelector</code></h3>
<div class="org-src-container">

<pre><code class="python" >from mlxtend.feature_selection import \
    SequentialFeatureSelector
sfs = SequentialFeatureSelector(LinearRegression(),
                                forward=False, k_features=7)
sfs.fit(X_train_scaled, y_train)
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >Features: 7/7
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >print(sfs.k_feature_idx_)
print(boston.feature_names[np.array(sfs.k_feature_idx_)])
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >(1, 4, 5, 7, 9, 10, 12)
['ZN' 'NOX' 'RM' 'DIS' 'TAX' 'PTRATIO' 'LSTAT']
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="python" >sfs.k_score_
</code></pre>
</div>

<div class="org-src-container">

<pre><code class="text" >0.725
</code></pre>
</div>

<aside class="notes">
<p>
Forward: start with 0, add the feature that gives the best 1-feature model, and continue from there.
Backward: start with all features and remove 1 at a time. <code>k_features</code> is how many to keep.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
