<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-orgf489b6e">
<h2 id="orgf489b6e">Random Forests and Ensembles</h2>
<p>
Robert Utterback
</p>

<p>
Based on slides by Andreas Muller
</p>
<aside class="notes">
<p>
\(2+2\)
</p>

</aside>
</section>
</section>
<section>
<section id="slide-orgcdc09d3">
<h2 id="orgcdc09d3">Random Forests</h2>
<div class="outline-text-2" id="text-orgcdc09d3">
</div>
</section>
<section id="slide-org7dca13f">
<h3 id="org7dca13f">Forests</h3>
<ul>
<li>Build different trees</li>
<li>Average their results (or vote)</li>

</ul>
</section>
<section id="slide-org5b86246">
<h3 id="org5b86246">Random Forests</h3>

<div class="figure">
<p><img src="./assets/forest01.png" alt="forest01.png" />
</p>
</div>
<aside class="notes">
<p>
Individually not very good, but work well together.
</p>

</aside>

</section>
<section id="slide-org04bd5cf">
<h3 id="org04bd5cf">Tree Randomization</h3>
<div class="column" style="float:left; width: 50%">
<ul>
<li>For each tree:
<ul>
<li>Pick bootstrap sample of data</li>

</ul></li>

<li>For each split:
<ul>
<li>Pick random sample of features</li>

</ul></li>
<li>More trees are always better</li>

</ul>
</div>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/tree_datasplit.png" alt="tree_datasplit.png" />
</p>
</div>


<div class="figure">
<p><img src="./assets/tree_featuresplit.png" alt="tree_featuresplit.png" />
</p>
</div>
</div>
<aside class="notes">
<p>
bootstrap agg: sampling with replacement
</p>

</aside>
</section>
<section id="slide-org16fcee3">
<h3 id="org16fcee3">Tuning Random Forests</h3>
<ul>
<li>Main hyperparameter: max_features
<ul>
<li>around sqrt(n_features) for classification</li>
<li>Around n_features for regression</li>

</ul></li>
<li>n_estimators &gt; 100</li>
<li>Prepruning might help, definitely helps with model size!</li>
<li>max_depth, max_leaf_nodes, min_samples_split again</li>

</ul>
<aside class="notes">
<p>
Num features to look at for each split. Controls variance.  
</p>

<p>
Q: Which (high,low) would lead to higher variance? A: Lower values ==
higher variance.
</p>

<p>
sklearn default num trees (estimators) is typically too low, use 100+.
</p>

<p>
Generally prepruning won't help accuracy as much as
num trees, though it makes your models smaller so decreases RAM and
prediction time.
</p>

</aside>
</section>
<section id="slide-orgb67b81d">
<h3 id="orgb67b81d">Extremely Randomized Trees</h3>
<ul>
<li>More randomness!</li>
<li>Randomly draw threshold for each feature</li>
<li>Doesnâ€™t use bootstrap</li>
<li>Faster because no sorting / searching</li>
<li>Can have smoother boundaries</li>

</ul>
<aside class="notes">
<p>
Even more random! Probably creates deeper trees since you can't really
make good decisions. Not as commonly used as regular random trees.
Like random forests, might not be best model, but they don't require
processing and they tend to just <b>work</b>.
</p>

</aside>
</section>
<section id="slide-orgb4f004d">
<h3 id="orgb4f004d">Warm-Starts</h3>
<div class="org-src-container">

<pre><code class="python" >train_scores = []
test_scores = []

rf = RandomForestClassifier(warm_start=True)
estimator_range = range(1, 100, 5)
for n_estimators in estimator_range:
    rf.n_estimators = n_estimators
    rf.fit(X_train, y_train)
    train_scores.append(rf.score(X_train, y_train))
    test_scores.append(rf.score(X_test, y_test))
</code></pre>
</div>

<div class="figure">
<p><img src="./assets/forest_warmstart.png" alt="forest_warmstart.png" height="275px" />
</p>
</div>
<aside class="notes">
<p>
Don't grid search num trees b/c higher is better. But eventually you
reach a point where it's not helping that much.  So "warm-start" grid
search: keep adding more and more trees until you don't increase
performance very much.
</p>

<p>
RFC will keep those old trees around so you don't retrain them.
</p>

<p>
Or just pick a high number&#x2026;downside: wasting time/space.
</p>

</aside>
</section>
<section id="slide-org6565db2">
<h3 id="org6565db2">Out-of-bag estimates</h3>
<ul>
<li>Each tree only uses ~66% of data</li>
<li>Can evaluate it on the rest!</li>
<li>Make predictions for out-of-bag, average, score.</li>
<li>Each prediction is an average over different subset of trees</li>

</ul>
<div class="org-src-container">

<pre id="smallcode"><code class="python" >train_scores, test_scores, oob_scores = [], [], []

feature_range = range(1, 64, 5)
for max_features in feature_range:
    rf = RandomForestClassifier(max_features=max_features, oob_score=True,
                                n_estimators=200, random_state=0)
    rf.fit(X_train, y_train)
    train_scores.append(rf.score(X_train, y_train))
    test_scores.append(rf.score(X_test, y_test))
    oob_scores.append(rf.oob_score_)
</code></pre>
</div>
<aside class="notes">
<p>
We drew with replacements, so we have some data we didn't select. That
makes a great test set <b>for that particular tree</b>. So for each tree
predict on the points that were out-of-bag. Then for each data point,
look at all trees that held out that data point, let them vote.
</p>

<p>
So only a subset of trees are used for each data point, but if you
have enough trees it will be fine.
</p>

<p>
Can use oob estimate instead of explicit test set.
</p>

</aside>
</section>
<section id="slide-org83dedff">
<h3 id="org83dedff"></h3>

<div class="figure">
<p><img src="./assets/tree_oob.png" alt="tree_oob.png" />
</p>
</div>
<aside class="notes">
<p>
train: overfit. oob and test at the same magnitude, slightly worse. But it comes for free. Theoretically it's an unbiased estimate.
</p>

</aside>
</section>
<section id="slide-org67181d7">
<h3 id="org67181d7">Variable Importance</h3>
<div class="org-src-container">

<pre><code class="python" >X_train, X_test, y_train, y_test = \
    train_test_split(iris.data, iris.target,
                     stratify=iris.target, random_state=1)
rf = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)
rf.feature_importances_
plt.barh(range(4), rf.feature_importances_)
plt.yticks(range(4), iris.feature_names);
# array([ 0.126,  0.033,  0.445,  0.396])
</code></pre>
</div>

<div class="figure">
<p><img src="./assets/tree_feat_importance.png" alt="tree_feat_importance.png" height="300px" />
</p>
</div>
<aside class="notes">
<p>
More robust feature importances than for a single tree. If you have
two correlated features, as an average of all trees, they are likely
to have similar importance, unlike a similar. So "smoother" estimates.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
