<!DOCTYPE html>
<html lang="python">
<head>
<meta charset="utf-8"/>
<title>Applied Machine Learning</title>
<meta name="author" content="(Robert Utterback)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/reveal.css"/>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/css/theme/white.css" id="theme"/>

<link rel="stylesheet" href="./notes.css"/>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'https://cdn.jsdelivr.net/reveal.js/3.0.0/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
\(
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\E}{\mathop{\boldsymbol{E}}}
\newcommand{\var}{\boldsymbol{Var}}
\newcommand{\norm}[1]{\lvert\lvert#1\rvert\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\ltwo}[1]{\norm{#1}_2}
\newcommand{\lone}[1]{\norm{#1}_1}
\newcommand{\sgn}[1]{\text{sign}\left( #1 \right)}
\newcommand{\e}{\mathrm{e}}
\newcommand{\minw}{\min_{w \in \mathbb{R}^p}}
\newcommand{\sumn}{\sum_{i=1}^n}
\newcommand{\logloss}{\log{(\exp{(-y_iw^T\vec{x}_i)} + 1)}}
\)

<section>
<section id="slide-orgdef060e">
<h2 id="orgdef060e">Lasso Regression</h2>
<div class="outline-text-2" id="text-orgdef060e">
</div>
</section>
<section id="slide-orgb7b4e6e">
<h3 id="orgb7b4e6e">Lasso Regression</h3>
<p>
\[ \min_{w \in \mathbb{R}^p} \sum_{i=1}^p \norm{w^T \vec{x}_i - y_i}^2 + \alpha \lone{w} \]
</p>

<ul>
<li>Shrinks \(w\) towards zero like Ridge</li>
<li>Sets some \(w\) exactly to zero</li>
<li>Automatic feature selection!</li>

</ul>
<aside class="notes">
<p>
Use l1 norm instead of l2.
l1 is sum of absolute values, l2 is sum of squares
l2 penalizes very large, l1 penalizes all equally
</p>

</aside>
</section>
<section id="slide-org8df0b39">
<h3 id="org8df0b39">Grid-Search for Lasso</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import Lasso
param_grid = {'alpha': np.logspace(-3, 0, 13)}
print(param_grid)
grid = GridSearchCV(Lasso(), param_grid, cv=10)
grid.fit(X_train, y_train)

print(grid.best_params_)
print(grid.best_score_)
</code></pre>
</div>

<pre class="example">
{'alpha': array([0.001     , 0.00177828, 0.00316228, 0.00562341, 0.01      ,
       0.01778279, 0.03162278, 0.05623413, 0.1       , 0.17782794,
       0.31622777, 0.56234133, 1.        ])}
{'alpha': 0.001}
0.7165187254446156

</pre>

</section>
<section id="slide-org3dde8b1">
<h3 id="org3dde8b1">Grid-Search for Lasso</h3>

<div class="figure">
<p><img src="./assets/lasso_alpha_search.png" alt="lasso_alpha_search.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Q: Which side corresponds to overfitting? A: left!
Right side is biased, underfitting!
</p>

</aside>
</section>
<section id="slide-org0fd88c6">
<h3 id="org0fd88c6">Coefficients for Lasso</h3>

<div class="figure">
<p><img src="./assets/lasso_coefficients.png" alt="lasso_coefficients.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Only selects 64 features here! Out of over 100!
But if two features are very correlated (e.g. duplicates), lasso will randomly select between them. Makes interpretation harder.
</p>

</aside>
</section>
<section id="slide-org76066bc">
<h3 id="org76066bc">Coefficient Path for Lasso</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import lars_path
plt.figure()
# lars_path computes the exact regularization path which is piecewise linear.
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
alphas, active, coefs = lars_path(X_train, y_train, eps=0.00001, method="lasso")
plt.plot(alphas, coefs.T, alpha=.5)
plt.xscale("log")
</code></pre>
</div>


<div class="figure">
<p><img src="assets/larspath.png" alt="larspath.png" height="350px" />
</p>
</div>
<aside class="notes">
<p>
Lasso is really only going to help if you are pretty sure the model is
sparse.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org400c7ee">
<h2 id="org400c7ee">Understanding Penalties</h2>
<div class="outline-text-2" id="text-org400c7ee">
</div>
</section>
<section id="slide-orgad9e59a">
<h3 id="orgad9e59a">Understanding L1 and L2 Penalties</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/l2_l1_l0.png" alt="l2_l1_l0.png" width="400px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">
<p>
\(\ell_2(w) = \sum_i \sqrt{w_i^2}\)
</p>

<p>
\(\ell_1(w) = \sum_i \abs{w_i}\)
</p>

<p>
\(\ell_0(w) = \sum_i \mathbf{1}\{w_i \ne 0\}\)
</p>
</div>
<aside class="notes">
<p>
If you really wanted a model to select features, you tell it to try to penalize setting any feature to anyting other than 0. That's the l0 norm. However, if you plot this function you'll see how nasty it looks, it's very hard to optimize.
We actually want continuous functions to differentiate and optimize. But we can do pretty well with the l1, even though it's not differentiable at 0.
</p>

</aside>
</section>
<section id="slide-org094bff0">
<h3 id="org094bff0">Understanding L1 and L2 Penalties</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/l1_kink.png" alt="l1_kink.png" width1="400px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">
<p>
\[ f(x) = (2x-1)^2 \]
</p>

<p>
\[ f(x) + L2 = (2x-1)^2 + \alpha x^2 \]
</p>

<p>
\[ f(x) + L1 = (2x-1)^2 + \alpha \abs{x} \]
</p>
</div>
<aside class="notes">
<p>
These are the f plus loss functions plotted.
Very vaguely, you can see the lines getting squished towards 0 on the x axis.
</p>

</aside>
</section>
<section id="slide-orgccc13f5">
<h3 id="orgccc13f5">Understanding L1 and L2 Penalties</h3>

<div class="figure">
<p><img src="./assets/l1l2ball.png" alt="l1l2ball.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Let's fix a norm of 1 instead of minimizing. These are plotted here. Diamond is l1.
If we plot a quadratic we intersect the circle, and that's our solution.
For the diamond, the geometry of this makes it likely to hit a corner, setting the parameter to 0.
</p>

</aside>
</section>
<section id="slide-orgac555d7">
<h3 id="orgac555d7">Understanding L1 and L2 Penalties</h3>

<div class="figure">
<p><img src="./assets/l1l2ball_intersect.png" alt="l1l2ball_intersect.png" height="400px" />
</p>
</div>
</section>
</section>
<section>
<section id="slide-org97e3a60">
<h2 id="org97e3a60">Elastic Net</h2>
<div class="outline-text-2" id="text-org97e3a60">
</div>
</section>
<section id="slide-orga569638">
<h3 id="orga569638">Elastic Net</h3>
<p>
\[ \min_{w \in \mathbb{R}^p} \sum_{i=1}^p + \norm{w^T \vec{x}_i - y_i}^2 + \alpha_1 \lone{w} + \alpha_2 \ltwo{w}^2 \]
</p>

<ul>
<li>Combines benefits for ridge and lasso</li>
<li>Must tune two parameters</li>

</ul>
<aside class="notes">
<p>
It works best in practice to combine them both!
</p>

</aside>
</section>
<section id="slide-org41bbe11">
<h3 id="org41bbe11">Comparing Unit balls</h3>

<div class="figure">
<p><img src="./assets/l1l2_elasticnet.png" alt="l1l2_elasticnet.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Elastic net is inbetween, is round but also has corner
</p>

</aside>
</section>
<section id="slide-org28a02f6">
<h3 id="org28a02f6">Parameterization in scikit-learn</h3>
<p>
\[ \min_{w \in \mathbb{R}^p} \sum_{i=1}^p + \norm{w^T \vec{x}_i - y_i}^2 + \alpha\eta \lone{w} + \alpha(1-\eta) \ltwo{w}^2 \]
</p>

<p>
Where \(\eta\) is the relative amount of L1 penalty.
</p>
<aside class="notes">
<p>
Still has alpha, but also l1 ratio: how much should be l1 and how much l2.
If you use alpha=0 or eta=0 or 1, it will be inefficient, just use one of the above
</p>

</aside>
</section>
<section id="slide-orgfa6a6df">
<h3 id="orgfa6a6df">Grid-Search for Elastic Net</h3>
<div class="org-src-container">

<pre><code class="python" >from sklearn.linear_model import ElasticNet
param_grid = {'alpha': np.logspace(-4, -1, 10),
              'l1_ratio': [0.01, .1, .5, .9, .98, 1]}
grid = GridSearchCV(ElasticNet(), param_grid, cv=10)
grid.fit(X_train, y_train)
print(grid.best_params_)
print(grid.best_score_)
</code></pre>
</div>

<pre class="example">
{'alpha': 0.00021544346900318845, 'l1_ratio': 0.01}
0.7165639198575516

</pre>

</section>
<section id="slide-org5a87c3a">
<h3 id="org5a87c3a">Analyzing 2D Grid Search</h3>
<div class="org-src-container">

<pre><code class="python" >import pandas as pd
res = pd.pivot_table(pd.DataFrame(grid.cv_results_),
    values='mean_test_score', index='param_alpha', columns='param_l1_ratio')
</code></pre>
</div>


<div class="figure">
<p><img src="./assets/elasticnet_search.png" alt="elasticnet_search.png" height="350px" />
</p>
</div>

<aside class="notes">
<p>
Not a nice curve, but there's this thing called a pivot table.
Alpha vs. l1 ratio
Of course, this hasn't been validated. When you do 2+ more parameters, you're more likely to have some noise that got picked up on, so these are not unbiased estimators of actual performance.
This just tells us whether the parameters matter and what's the best.
</p>

</aside>
</section>
</section>
<section>
<section id="slide-org69f5020">
<h2 id="org69f5020">Robust Regression</h2>
<div class="outline-text-2" id="text-org69f5020">
</div>
</section>
<section id="slide-org31a8aa2">
<h3 id="org31a8aa2">Robust Regression</h3>

<div class="figure">
<p><img src="./assets/robust_regression.png" alt="robust_regression.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Looks kind of clear to us: there are outliers.
But it fools the standard linear models.
</p>

</aside>
</section>
<section id="slide-org913b333">
<h3 id="org913b333">Least Squares Fit to Outlier Data</h3>

<div class="figure">
<p><img src="./assets/outliers_least_squares.png" alt="outliers_least_squares.png" height="400pxa" />
</p>
</div>
</section>
<section id="slide-org5a74fc7">
<h3 id="org5a74fc7">Robust Fit</h3>

<div class="figure">
<p><img src="./assets/outliers_robust_fit.png" alt="outliers_robust_fit.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Here's what we'd like. Very helpful if you know there are outliers. Easy to see now, but actually hard in many dimensions.
</p>

</aside>
</section>
<section id="slide-org6325ea1">
<h3 id="org6325ea1">Huber Loss</h3>
<div class="column" style="float:left; width: 50%">

<div class="figure">
<p><img src="./assets/huber_loss.png" alt="huber_loss.png" width="300px" />
</p>
</div>
</div>
<div class="column" style="float:left; width: 50%">
<p>
\[ \min_{w,\sigma} \sum_{i=1}^n \left( \sigma + H\left( \frac{X_i w_i - y_i}{\sigma} \right) \sigma \right) \\+ \alpha \ltwo{w}^2 \]
</p>

</div>
<aside class="notes">
<p>
Can't figure out how to get org-mode to export this to html
</p>

<div>
\begin{equation} 
H(z) = 
\begin{cases} 
z^2, & \text{\qquad if $|z| < \epilson}\\ 
2\epsilon \abs{z} & \text{\qquad else} 
\end{cases}
\end{equation}

</div>

<p>
Changes loss to "Huber loss". Details are not important. Sigma learns some scaling of the data. Around zero it's quadratic, outside that it's linear.
So outlier won't be penalized too much. But being quadratic around zero makes it differentiable everywhere.
</p>

</aside>
</section>
<section id="slide-org775ad71">
<h3 id="org775ad71">RANSAC</h3>

<div class="figure">
<p><img src="./assets/ransac_algorithm.png" alt="ransac_algorithm.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Doesn't actually fit our loss/risk minimization model.
</p>

<p>
Subsamples a few points to fit a linear model.
Repeat this many times.
</p>

<p>
Now evaluate them:
How well do they predict particular fragments of the data.
</p>

</aside>
</section>
<section id="slide-org3a6e789">
<h3 id="org3a6e789">RANSAC</h3>

<div class="figure">
<p><img src="./assets/ransac_algorithm2.png" alt="ransac_algorithm2.png" height="400px" />
</p>
</div>
<aside class="notes">
<p>
Calculates what the "inliers" (well predicted by the model)
The biggest set of inliers is the model we choose.
</p>

</aside>
</section>
</section>
</div>
</div>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/head.min.js"></script>
<script src="https://cdn.jsdelivr.net/reveal.js/3.0.0/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c/t',
rollingLinks: false,
keyboard: true,
overview: true,
margin: 0.05,
minScale: 0.20,
maxScale: 15.00,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: '0.0',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'https://cdn.jsdelivr.net/reveal.js/3.0.0/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
,pdfSeparateFragments: false});
</script>
</body>
</html>
